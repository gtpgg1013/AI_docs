{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyScaler(data): #MinMaxScaling\n",
    "    #print(np.min(data)) #전체 data 최소값\n",
    "    de=np.max(data,axis=0)-np.min(data,axis=0) #열 단위 최소값\n",
    "    num=data-np.min(data,axis=0)\n",
    "    return num/de   \n",
    "    #print(np.min(data,axis=1)) #행 단위 최소값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881783 0.83755792]\n",
      " [0.54412549 0.50274824 0.57608696 0.60646801 0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.4258239  0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.4258239  0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "xy=MyScaler(xy)\n",
    "print(xy)\n",
    "#xy.shape #(8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata=xy[:, 0:-1]\n",
    "xdata.shape#(8, 4)\n",
    "ydata=xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y=tf.placeholder(tf.float32, shape=[None, 1])\n",
    "w=tf.Variable(tf.random_normal([4,1]))\n",
    "b=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.matmul(x,w)+b\n",
    "cost=tf.reduce_mean(tf.square(hf-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(1e-6).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 1.282203 hf: [[1.3040681]\n",
      " [2.3772767]\n",
      " [1.8603342]\n",
      " [1.2555195]\n",
      " [1.5521595]\n",
      " [1.4844656]\n",
      " [1.3042667]\n",
      " [1.4782021]]\n",
      "1 cost: 1.282195 hf: [[1.3040633]\n",
      " [2.3772717]\n",
      " [1.8603301]\n",
      " [1.2555163]\n",
      " [1.5521557]\n",
      " [1.4844618]\n",
      " [1.3042641]\n",
      " [1.4781992]]\n",
      "2 cost: 1.2821871 hf: [[1.3040586]\n",
      " [2.3772666]\n",
      " [1.8603259]\n",
      " [1.255513 ]\n",
      " [1.5521519]\n",
      " [1.4844582]\n",
      " [1.3042614]\n",
      " [1.4781966]]\n",
      "3 cost: 1.2821791 hf: [[1.3040538]\n",
      " [2.3772616]\n",
      " [1.8603218]\n",
      " [1.2555097]\n",
      " [1.5521481]\n",
      " [1.4844544]\n",
      " [1.3042587]\n",
      " [1.4781939]]\n",
      "4 cost: 1.2821712 hf: [[1.304049 ]\n",
      " [2.3772566]\n",
      " [1.8603175]\n",
      " [1.2555065]\n",
      " [1.5521443]\n",
      " [1.4844508]\n",
      " [1.3042561]\n",
      " [1.4781911]]\n",
      "5 cost: 1.2821634 hf: [[1.3040442]\n",
      " [2.3772519]\n",
      " [1.8603134]\n",
      " [1.2555033]\n",
      " [1.5521406]\n",
      " [1.484447 ]\n",
      " [1.3042533]\n",
      " [1.4781884]]\n",
      "6 cost: 1.2821555 hf: [[1.3040395]\n",
      " [2.3772469]\n",
      " [1.8603094]\n",
      " [1.2555001]\n",
      " [1.5521369]\n",
      " [1.4844434]\n",
      " [1.3042507]\n",
      " [1.4781857]]\n",
      "7 cost: 1.2821476 hf: [[1.3040347]\n",
      " [2.3772418]\n",
      " [1.8603051]\n",
      " [1.2554967]\n",
      " [1.5521331]\n",
      " [1.4844397]\n",
      " [1.3042481]\n",
      " [1.478183 ]]\n",
      "8 cost: 1.2821397 hf: [[1.30403  ]\n",
      " [2.3772368]\n",
      " [1.860301 ]\n",
      " [1.2554936]\n",
      " [1.5521293]\n",
      " [1.484436 ]\n",
      " [1.3042455]\n",
      " [1.4781803]]\n",
      "9 cost: 1.2821317 hf: [[1.304025 ]\n",
      " [2.3772318]\n",
      " [1.8602967]\n",
      " [1.2554903]\n",
      " [1.5521255]\n",
      " [1.4844323]\n",
      " [1.3042428]\n",
      " [1.4781775]]\n",
      "10 cost: 1.2821238 hf: [[1.3040203]\n",
      " [2.3772268]\n",
      " [1.8602927]\n",
      " [1.2554871]\n",
      " [1.5521216]\n",
      " [1.4844286]\n",
      " [1.3042402]\n",
      " [1.4781749]]\n",
      "11 cost: 1.2821159 hf: [[1.3040155]\n",
      " [2.377222 ]\n",
      " [1.8602884]\n",
      " [1.2554839]\n",
      " [1.5521178]\n",
      " [1.4844251]\n",
      " [1.3042375]\n",
      " [1.4781721]]\n",
      "12 cost: 1.2821081 hf: [[1.3040107]\n",
      " [2.377217 ]\n",
      " [1.8602843]\n",
      " [1.2554806]\n",
      " [1.552114 ]\n",
      " [1.4844213]\n",
      " [1.3042349]\n",
      " [1.4781694]]\n",
      "13 cost: 1.2821002 hf: [[1.304006 ]\n",
      " [2.377212 ]\n",
      " [1.8602802]\n",
      " [1.2554774]\n",
      " [1.5521103]\n",
      " [1.4844177]\n",
      " [1.3042321]\n",
      " [1.4781667]]\n",
      "14 cost: 1.2820921 hf: [[1.3040012]\n",
      " [2.377207 ]\n",
      " [1.860276 ]\n",
      " [1.2554741]\n",
      " [1.5521065]\n",
      " [1.4844139]\n",
      " [1.3042295]\n",
      " [1.478164 ]]\n",
      "15 cost: 1.2820843 hf: [[1.3039963]\n",
      " [2.377202 ]\n",
      " [1.8602719]\n",
      " [1.255471 ]\n",
      " [1.5521027]\n",
      " [1.4844103]\n",
      " [1.3042269]\n",
      " [1.4781613]]\n",
      "16 cost: 1.2820764 hf: [[1.3039916]\n",
      " [2.377197 ]\n",
      " [1.8602676]\n",
      " [1.2554677]\n",
      " [1.552099 ]\n",
      " [1.4844065]\n",
      " [1.3042243]\n",
      " [1.4781586]]\n",
      "17 cost: 1.2820684 hf: [[1.3039868]\n",
      " [2.377192 ]\n",
      " [1.8602636]\n",
      " [1.2554644]\n",
      " [1.5520952]\n",
      " [1.4844029]\n",
      " [1.3042216]\n",
      " [1.4781559]]\n",
      "18 cost: 1.2820606 hf: [[1.303982 ]\n",
      " [2.3771873]\n",
      " [1.8602593]\n",
      " [1.2554612]\n",
      " [1.5520914]\n",
      " [1.4843991]\n",
      " [1.3042189]\n",
      " [1.4781532]]\n",
      "19 cost: 1.2820525 hf: [[1.3039773]\n",
      " [2.3771822]\n",
      " [1.8602552]\n",
      " [1.255458 ]\n",
      " [1.5520875]\n",
      " [1.4843955]\n",
      " [1.3042163]\n",
      " [1.4781504]]\n",
      "20 cost: 1.2820448 hf: [[1.3039725]\n",
      " [2.3771772]\n",
      " [1.8602512]\n",
      " [1.2554548]\n",
      " [1.5520837]\n",
      " [1.4843919]\n",
      " [1.3042136]\n",
      " [1.4781477]]\n",
      "21 cost: 1.2820368 hf: [[1.3039677]\n",
      " [2.3771722]\n",
      " [1.8602469]\n",
      " [1.2554514]\n",
      " [1.5520799]\n",
      " [1.4843881]\n",
      " [1.3042109]\n",
      " [1.478145 ]]\n",
      "22 cost: 1.2820289 hf: [[1.303963 ]\n",
      " [2.3771672]\n",
      " [1.8602427]\n",
      " [1.2554482]\n",
      " [1.5520761]\n",
      " [1.4843845]\n",
      " [1.3042083]\n",
      " [1.4781423]]\n",
      "23 cost: 1.2820208 hf: [[1.3039582]\n",
      " [2.3771622]\n",
      " [1.8602386]\n",
      " [1.255445 ]\n",
      " [1.5520723]\n",
      " [1.4843807]\n",
      " [1.3042057]\n",
      " [1.4781395]]\n",
      "24 cost: 1.2820132 hf: [[1.3039534]\n",
      " [2.3771572]\n",
      " [1.8602345]\n",
      " [1.2554418]\n",
      " [1.5520687]\n",
      " [1.4843771]\n",
      " [1.304203 ]\n",
      " [1.4781368]]\n",
      "25 cost: 1.2820052 hf: [[1.3039485]\n",
      " [2.3771524]\n",
      " [1.8602303]\n",
      " [1.2554386]\n",
      " [1.5520649]\n",
      " [1.4843733]\n",
      " [1.3042004]\n",
      " [1.4781342]]\n",
      "26 cost: 1.2819972 hf: [[1.3039438]\n",
      " [2.3771474]\n",
      " [1.8602262]\n",
      " [1.2554352]\n",
      " [1.5520611]\n",
      " [1.4843698]\n",
      " [1.3041978]\n",
      " [1.4781314]]\n",
      "27 cost: 1.2819893 hf: [[1.303939 ]\n",
      " [2.3771424]\n",
      " [1.8602219]\n",
      " [1.255432 ]\n",
      " [1.5520573]\n",
      " [1.4843659]\n",
      " [1.304195 ]\n",
      " [1.4781287]]\n",
      "28 cost: 1.2819813 hf: [[1.3039342]\n",
      " [2.3771374]\n",
      " [1.8602178]\n",
      " [1.2554288]\n",
      " [1.5520535]\n",
      " [1.4843624]\n",
      " [1.3041923]\n",
      " [1.4781259]]\n",
      "29 cost: 1.2819734 hf: [[1.3039294]\n",
      " [2.3771324]\n",
      " [1.8602136]\n",
      " [1.2554256]\n",
      " [1.5520496]\n",
      " [1.4843587]\n",
      " [1.3041897]\n",
      " [1.4781232]]\n",
      "30 cost: 1.2819656 hf: [[1.3039247]\n",
      " [2.3771274]\n",
      " [1.8602095]\n",
      " [1.2554224]\n",
      " [1.5520458]\n",
      " [1.484355 ]\n",
      " [1.304187 ]\n",
      " [1.4781206]]\n",
      "31 cost: 1.2819576 hf: [[1.3039198]\n",
      " [2.3771224]\n",
      " [1.8602054]\n",
      " [1.255419 ]\n",
      " [1.552042 ]\n",
      " [1.4843513]\n",
      " [1.3041844]\n",
      " [1.4781178]]\n",
      "32 cost: 1.2819498 hf: [[1.303915 ]\n",
      " [2.3771174]\n",
      " [1.8602011]\n",
      " [1.2554159]\n",
      " [1.5520383]\n",
      " [1.4843476]\n",
      " [1.3041818]\n",
      " [1.4781151]]\n",
      "33 cost: 1.2819419 hf: [[1.3039103]\n",
      " [2.3771126]\n",
      " [1.8601971]\n",
      " [1.2554126]\n",
      " [1.5520345]\n",
      " [1.484344 ]\n",
      " [1.3041792]\n",
      " [1.4781125]]\n",
      "34 cost: 1.281934 hf: [[1.3039055]\n",
      " [2.3771076]\n",
      " [1.860193 ]\n",
      " [1.2554094]\n",
      " [1.5520308]\n",
      " [1.4843402]\n",
      " [1.3041766]\n",
      " [1.4781096]]\n",
      "35 cost: 1.281926 hf: [[1.3039007]\n",
      " [2.3771026]\n",
      " [1.8601887]\n",
      " [1.2554061]\n",
      " [1.552027 ]\n",
      " [1.4843366]\n",
      " [1.3041738]\n",
      " [1.478107 ]]\n",
      "36 cost: 1.281918 hf: [[1.303896 ]\n",
      " [2.3770976]\n",
      " [1.8601846]\n",
      " [1.2554029]\n",
      " [1.5520232]\n",
      " [1.4843328]\n",
      " [1.3041711]\n",
      " [1.4781042]]\n",
      "37 cost: 1.2819102 hf: [[1.3038912]\n",
      " [2.3770926]\n",
      " [1.8601804]\n",
      " [1.2553997]\n",
      " [1.5520194]\n",
      " [1.4843292]\n",
      " [1.3041685]\n",
      " [1.4781015]]\n",
      "38 cost: 1.2819023 hf: [[1.3038864]\n",
      " [2.3770876]\n",
      " [1.8601763]\n",
      " [1.2553964]\n",
      " [1.5520155]\n",
      " [1.4843254]\n",
      " [1.3041658]\n",
      " [1.4780989]]\n",
      "39 cost: 1.2818944 hf: [[1.3038816]\n",
      " [2.3770828]\n",
      " [1.860172 ]\n",
      " [1.2553931]\n",
      " [1.5520117]\n",
      " [1.4843218]\n",
      " [1.3041632]\n",
      " [1.4780961]]\n",
      "40 cost: 1.2818866 hf: [[1.3038769]\n",
      " [2.3770778]\n",
      " [1.860168 ]\n",
      " [1.2553899]\n",
      " [1.5520079]\n",
      " [1.4843181]\n",
      " [1.3041606]\n",
      " [1.4780934]]\n",
      "41 cost: 1.2818785 hf: [[1.303872 ]\n",
      " [2.3770728]\n",
      " [1.8601637]\n",
      " [1.2553867]\n",
      " [1.5520041]\n",
      " [1.4843144]\n",
      " [1.3041579]\n",
      " [1.4780908]]\n",
      "42 cost: 1.2818706 hf: [[1.3038672]\n",
      " [2.3770678]\n",
      " [1.8601596]\n",
      " [1.2553835]\n",
      " [1.5520004]\n",
      " [1.4843109]\n",
      " [1.3041552]\n",
      " [1.4780879]]\n",
      "43 cost: 1.2818627 hf: [[1.3038625]\n",
      " [2.3770628]\n",
      " [1.8601555]\n",
      " [1.2553803]\n",
      " [1.5519967]\n",
      " [1.484307 ]\n",
      " [1.3041526]\n",
      " [1.4780853]]\n",
      "44 cost: 1.2818549 hf: [[1.3038577]\n",
      " [2.3770578]\n",
      " [1.8601513]\n",
      " [1.255377 ]\n",
      " [1.5519929]\n",
      " [1.4843035]\n",
      " [1.30415  ]\n",
      " [1.4780824]]\n",
      "45 cost: 1.281847 hf: [[1.3038529]\n",
      " [2.377053 ]\n",
      " [1.8601472]\n",
      " [1.2553737]\n",
      " [1.5519891]\n",
      " [1.4842997]\n",
      " [1.3041472]\n",
      " [1.4780798]]\n",
      "46 cost: 1.281839 hf: [[1.3038481]\n",
      " [2.377048 ]\n",
      " [1.860143 ]\n",
      " [1.2553705]\n",
      " [1.5519853]\n",
      " [1.4842961]\n",
      " [1.3041446]\n",
      " [1.478077 ]]\n",
      "47 cost: 1.281831 hf: [[1.3038433]\n",
      " [2.377043 ]\n",
      " [1.8601389]\n",
      " [1.2553673]\n",
      " [1.5519814]\n",
      " [1.4842924]\n",
      " [1.304142 ]\n",
      " [1.4780743]]\n",
      "48 cost: 1.2818233 hf: [[1.3038385]\n",
      " [2.377038 ]\n",
      " [1.8601346]\n",
      " [1.2553641]\n",
      " [1.5519776]\n",
      " [1.4842887]\n",
      " [1.3041394]\n",
      " [1.4780717]]\n",
      "49 cost: 1.2818153 hf: [[1.3038337]\n",
      " [2.377033 ]\n",
      " [1.8601305]\n",
      " [1.2553608]\n",
      " [1.5519738]\n",
      " [1.484285 ]\n",
      " [1.3041368]\n",
      " [1.478069 ]]\n",
      "50 cost: 1.2818074 hf: [[1.303829 ]\n",
      " [2.377028 ]\n",
      " [1.8601264]\n",
      " [1.2553575]\n",
      " [1.55197  ]\n",
      " [1.4842813]\n",
      " [1.304134 ]\n",
      " [1.4780662]]\n",
      "51 cost: 1.2817994 hf: [[1.3038242]\n",
      " [2.377023 ]\n",
      " [1.8601222]\n",
      " [1.2553544]\n",
      " [1.5519663]\n",
      " [1.4842776]\n",
      " [1.3041314]\n",
      " [1.4780635]]\n",
      "52 cost: 1.2817916 hf: [[1.3038194]\n",
      " [2.377018 ]\n",
      " [1.8601182]\n",
      " [1.2553511]\n",
      " [1.5519626]\n",
      " [1.4842739]\n",
      " [1.3041286]\n",
      " [1.4780607]]\n",
      "53 cost: 1.2817837 hf: [[1.3038146]\n",
      " [2.3770132]\n",
      " [1.8601139]\n",
      " [1.2553478]\n",
      " [1.5519588]\n",
      " [1.4842703]\n",
      " [1.304126 ]\n",
      " [1.4780581]]\n",
      "54 cost: 1.2817758 hf: [[1.3038099]\n",
      " [2.3770082]\n",
      " [1.8601098]\n",
      " [1.2553446]\n",
      " [1.551955 ]\n",
      " [1.4842665]\n",
      " [1.3041234]\n",
      " [1.4780554]]\n",
      "55 cost: 1.2817678 hf: [[1.3038051]\n",
      " [2.3770032]\n",
      " [1.8601055]\n",
      " [1.2553414]\n",
      " [1.5519512]\n",
      " [1.484263 ]\n",
      " [1.3041208]\n",
      " [1.4780526]]\n",
      "56 cost: 1.28176 hf: [[1.3038003]\n",
      " [2.3769982]\n",
      " [1.8601015]\n",
      " [1.2553382]\n",
      " [1.5519474]\n",
      " [1.4842591]\n",
      " [1.3041182]\n",
      " [1.47805  ]]\n",
      "57 cost: 1.281752 hf: [[1.3037955]\n",
      " [2.3769932]\n",
      " [1.8600974]\n",
      " [1.2553349]\n",
      " [1.5519435]\n",
      " [1.4842556]\n",
      " [1.3041155]\n",
      " [1.4780471]]\n",
      "58 cost: 1.281744 hf: [[1.3037907]\n",
      " [2.3769882]\n",
      " [1.8600931]\n",
      " [1.2553316]\n",
      " [1.5519397]\n",
      " [1.4842519]\n",
      " [1.3041128]\n",
      " [1.4780445]]\n",
      "59 cost: 1.2817361 hf: [[1.3037859]\n",
      " [2.3769834]\n",
      " [1.8600891]\n",
      " [1.2553284]\n",
      " [1.5519359]\n",
      " [1.4842482]\n",
      " [1.3041102]\n",
      " [1.4780418]]\n",
      "60 cost: 1.2817283 hf: [[1.3037812]\n",
      " [2.3769784]\n",
      " [1.8600848]\n",
      " [1.2553252]\n",
      " [1.5519322]\n",
      " [1.4842445]\n",
      " [1.3041074]\n",
      " [1.478039 ]]\n",
      "61 cost: 1.2817204 hf: [[1.3037764]\n",
      " [2.3769734]\n",
      " [1.8600807]\n",
      " [1.255322 ]\n",
      " [1.5519284]\n",
      " [1.4842408]\n",
      " [1.3041048]\n",
      " [1.4780364]]\n",
      "62 cost: 1.2817125 hf: [[1.3037716]\n",
      " [2.3769684]\n",
      " [1.8600765]\n",
      " [1.2553186]\n",
      " [1.5519247]\n",
      " [1.4842372]\n",
      " [1.3041022]\n",
      " [1.4780337]]\n",
      "63 cost: 1.2817047 hf: [[1.3037667]\n",
      " [2.3769634]\n",
      " [1.8600724]\n",
      " [1.2553154]\n",
      " [1.5519209]\n",
      " [1.4842334]\n",
      " [1.3040996]\n",
      " [1.4780309]]\n",
      "64 cost: 1.2816968 hf: [[1.303762 ]\n",
      " [2.3769584]\n",
      " [1.8600681]\n",
      " [1.2553122]\n",
      " [1.5519171]\n",
      " [1.4842298]\n",
      " [1.3040969]\n",
      " [1.4780283]]\n",
      "65 cost: 1.2816887 hf: [[1.3037572]\n",
      " [2.3769534]\n",
      " [1.860064 ]\n",
      " [1.255309 ]\n",
      " [1.5519133]\n",
      " [1.484226 ]\n",
      " [1.3040942]\n",
      " [1.4780254]]\n",
      "66 cost: 1.281681 hf: [[1.3037524]\n",
      " [2.3769486]\n",
      " [1.86006  ]\n",
      " [1.2553058]\n",
      " [1.5519094]\n",
      " [1.4842224]\n",
      " [1.3040916]\n",
      " [1.4780228]]\n",
      "67 cost: 1.2816728 hf: [[1.3037477]\n",
      " [2.3769436]\n",
      " [1.8600557]\n",
      " [1.2553024]\n",
      " [1.5519056]\n",
      " [1.4842187]\n",
      " [1.304089 ]\n",
      " [1.47802  ]]\n",
      "68 cost: 1.2816651 hf: [[1.3037429]\n",
      " [2.3769386]\n",
      " [1.8600516]\n",
      " [1.2552993]\n",
      " [1.5519018]\n",
      " [1.484215 ]\n",
      " [1.3040862]\n",
      " [1.4780173]]\n",
      "69 cost: 1.2816571 hf: [[1.3037381]\n",
      " [2.3769336]\n",
      " [1.8600473]\n",
      " [1.255296 ]\n",
      " [1.551898 ]\n",
      " [1.4842113]\n",
      " [1.3040836]\n",
      " [1.4780146]]\n",
      "70 cost: 1.2816492 hf: [[1.3037333]\n",
      " [2.3769286]\n",
      " [1.8600433]\n",
      " [1.2552928]\n",
      " [1.5518944]\n",
      " [1.4842076]\n",
      " [1.304081 ]\n",
      " [1.4780118]]\n",
      "71 cost: 1.2816414 hf: [[1.3037286]\n",
      " [2.3769236]\n",
      " [1.8600392]\n",
      " [1.2552896]\n",
      " [1.5518906]\n",
      " [1.4842039]\n",
      " [1.3040783]\n",
      " [1.4780092]]\n",
      "72 cost: 1.2816334 hf: [[1.3037238]\n",
      " [2.3769186]\n",
      " [1.860035 ]\n",
      " [1.2552863]\n",
      " [1.5518868]\n",
      " [1.4842002]\n",
      " [1.3040757]\n",
      " [1.4780065]]\n",
      "73 cost: 1.2816255 hf: [[1.3037189]\n",
      " [2.3769138]\n",
      " [1.8600308]\n",
      " [1.2552831]\n",
      " [1.551883 ]\n",
      " [1.4841967]\n",
      " [1.304073 ]\n",
      " [1.4780037]]\n",
      "74 cost: 1.2816175 hf: [[1.3037142]\n",
      " [2.3769088]\n",
      " [1.8600266]\n",
      " [1.2552798]\n",
      " [1.5518792]\n",
      " [1.4841928]\n",
      " [1.3040704]\n",
      " [1.478001 ]]\n",
      "75 cost: 1.2816097 hf: [[1.3037094]\n",
      " [2.3769038]\n",
      " [1.8600225]\n",
      " [1.2552767]\n",
      " [1.5518754]\n",
      " [1.4841893]\n",
      " [1.3040676]\n",
      " [1.4779983]]\n",
      "76 cost: 1.2816017 hf: [[1.3037046]\n",
      " [2.3768988]\n",
      " [1.8600184]\n",
      " [1.2552733]\n",
      " [1.5518715]\n",
      " [1.4841855]\n",
      " [1.304065 ]\n",
      " [1.4779956]]\n",
      "77 cost: 1.2815938 hf: [[1.3036999]\n",
      " [2.3768938]\n",
      " [1.8600142]\n",
      " [1.2552701]\n",
      " [1.5518677]\n",
      " [1.4841819]\n",
      " [1.3040624]\n",
      " [1.4779929]]\n",
      "78 cost: 1.2815859 hf: [[1.3036951]\n",
      " [2.3768888]\n",
      " [1.8600099]\n",
      " [1.2552669]\n",
      " [1.551864 ]\n",
      " [1.4841782]\n",
      " [1.3040597]\n",
      " [1.4779902]]\n",
      "79 cost: 1.2815781 hf: [[1.3036902]\n",
      " [2.3768837]\n",
      " [1.8600059]\n",
      " [1.2552637]\n",
      " [1.5518602]\n",
      " [1.4841745]\n",
      " [1.3040571]\n",
      " [1.4779875]]\n",
      "80 cost: 1.2815702 hf: [[1.3036854]\n",
      " [2.376879 ]\n",
      " [1.8600017]\n",
      " [1.2552605]\n",
      " [1.5518565]\n",
      " [1.4841708]\n",
      " [1.3040545]\n",
      " [1.4779847]]\n",
      "81 cost: 1.2815622 hf: [[1.3036807]\n",
      " [2.376874 ]\n",
      " [1.8599975]\n",
      " [1.2552571]\n",
      " [1.5518527]\n",
      " [1.4841671]\n",
      " [1.3040518]\n",
      " [1.477982 ]]\n",
      "82 cost: 1.2815543 hf: [[1.3036759]\n",
      " [2.376869 ]\n",
      " [1.8599935]\n",
      " [1.255254 ]\n",
      " [1.5518489]\n",
      " [1.4841635]\n",
      " [1.3040491]\n",
      " [1.4779793]]\n",
      "83 cost: 1.2815464 hf: [[1.3036711]\n",
      " [2.376864 ]\n",
      " [1.8599892]\n",
      " [1.2552507]\n",
      " [1.5518451]\n",
      " [1.4841597]\n",
      " [1.3040464]\n",
      " [1.4779766]]\n",
      "84 cost: 1.2815385 hf: [[1.3036664]\n",
      " [2.376859 ]\n",
      " [1.8599851]\n",
      " [1.2552475]\n",
      " [1.5518413]\n",
      " [1.4841561]\n",
      " [1.3040438]\n",
      " [1.4779739]]\n",
      "85 cost: 1.2815305 hf: [[1.3036616]\n",
      " [2.376854 ]\n",
      " [1.8599808]\n",
      " [1.2552443]\n",
      " [1.5518374]\n",
      " [1.4841523]\n",
      " [1.3040411]\n",
      " [1.4779712]]\n",
      "86 cost: 1.2815226 hf: [[1.3036568]\n",
      " [2.376849 ]\n",
      " [1.8599768]\n",
      " [1.2552409]\n",
      " [1.5518336]\n",
      " [1.4841487]\n",
      " [1.3040385]\n",
      " [1.4779685]]\n",
      "87 cost: 1.2815149 hf: [[1.303652 ]\n",
      " [2.3768442]\n",
      " [1.8599725]\n",
      " [1.2552378]\n",
      " [1.5518298]\n",
      " [1.484145 ]\n",
      " [1.3040359]\n",
      " [1.4779658]]\n",
      "88 cost: 1.2815068 hf: [[1.3036473]\n",
      " [2.3768392]\n",
      " [1.8599684]\n",
      " [1.2552345]\n",
      " [1.5518261]\n",
      " [1.4841413]\n",
      " [1.3040332]\n",
      " [1.477963 ]]\n",
      "89 cost: 1.281499 hf: [[1.3036424]\n",
      " [2.3768342]\n",
      " [1.8599644]\n",
      " [1.2552313]\n",
      " [1.5518224]\n",
      " [1.4841377]\n",
      " [1.3040305]\n",
      " [1.4779603]]\n",
      "90 cost: 1.281491 hf: [[1.3036376]\n",
      " [2.3768291]\n",
      " [1.8599601]\n",
      " [1.255228 ]\n",
      " [1.5518186]\n",
      " [1.484134 ]\n",
      " [1.3040279]\n",
      " [1.4779575]]\n",
      "91 cost: 1.2814832 hf: [[1.3036329]\n",
      " [2.3768241]\n",
      " [1.859956 ]\n",
      " [1.2552247]\n",
      " [1.5518148]\n",
      " [1.4841304]\n",
      " [1.3040252]\n",
      " [1.4779549]]\n",
      "92 cost: 1.2814751 hf: [[1.3036281]\n",
      " [2.3768191]\n",
      " [1.8599517]\n",
      " [1.2552216]\n",
      " [1.551811 ]\n",
      " [1.4841266]\n",
      " [1.3040226]\n",
      " [1.4779521]]\n",
      "93 cost: 1.2814673 hf: [[1.3036233]\n",
      " [2.3768144]\n",
      " [1.8599477]\n",
      " [1.2552183]\n",
      " [1.5518072]\n",
      " [1.484123 ]\n",
      " [1.3040199]\n",
      " [1.4779494]]\n",
      "94 cost: 1.2814595 hf: [[1.3036186]\n",
      " [2.3768094]\n",
      " [1.8599436]\n",
      " [1.255215 ]\n",
      " [1.5518034]\n",
      " [1.4841192]\n",
      " [1.3040173]\n",
      " [1.4779468]]\n",
      "95 cost: 1.2814516 hf: [[1.3036137]\n",
      " [2.3768044]\n",
      " [1.8599393]\n",
      " [1.2552118]\n",
      " [1.5517995]\n",
      " [1.4841156]\n",
      " [1.3040147]\n",
      " [1.477944 ]]\n",
      "96 cost: 1.2814436 hf: [[1.3036089]\n",
      " [2.3767993]\n",
      " [1.8599352]\n",
      " [1.2552086]\n",
      " [1.5517957]\n",
      " [1.4841118]\n",
      " [1.304012 ]\n",
      " [1.4779413]]\n",
      "97 cost: 1.2814356 hf: [[1.3036041]\n",
      " [2.3767943]\n",
      " [1.859931 ]\n",
      " [1.2552054]\n",
      " [1.551792 ]\n",
      " [1.4841082]\n",
      " [1.3040093]\n",
      " [1.4779385]]\n",
      "98 cost: 1.2814277 hf: [[1.3035994]\n",
      " [2.3767893]\n",
      " [1.8599269]\n",
      " [1.255202 ]\n",
      " [1.5517882]\n",
      " [1.4841044]\n",
      " [1.3040067]\n",
      " [1.4779358]]\n",
      "99 cost: 1.2814198 hf: [[1.3035946]\n",
      " [2.3767843]\n",
      " [1.8599226]\n",
      " [1.2551988]\n",
      " [1.5517845]\n",
      " [1.4841008]\n",
      " [1.304004 ]\n",
      " [1.4779332]]\n",
      "100 cost: 1.2814119 hf: [[1.3035898]\n",
      " [2.3767793]\n",
      " [1.8599186]\n",
      " [1.2551956]\n",
      " [1.5517807]\n",
      " [1.4840971]\n",
      " [1.3040013]\n",
      " [1.4779304]]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(101):\n",
    "    cv,hv,_=sess.run([cost, hf, train], feed_dict={x:xdata, y:ydata})\n",
    "    print(step, \"cost:\", cv, \"hf:\", hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "#y값은 one hot 인코딩 방식으로 초기화 함...(a,b,c)중 하나를 hot하게 함...\n",
    "y_data = [[0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [1, 0, 0],#0\n",
    "          [1, 0, 0]]#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=3\n",
    "x=tf.placeholder(\"float\", [None,4])\n",
    "y=tf.placeholder(\"float\", [None,num_classes])\n",
    "w=tf.Variable(tf.random_normal([4,num_classes]))\n",
    "b=tf.Variable(tf.random_normal([num_classes]))\n",
    "#hf=x*w+b\n",
    "#[None,4],[4,3]+[3]="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.nn.softmax(tf.matmul(x,w)+b)\n",
    "cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf),axis=1))\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.506994\n",
      "200 0.68822074\n",
      "400 0.56815\n",
      "600 0.47372437\n",
      "800 0.384216\n",
      "1000 0.29525173\n",
      "1200 0.23215695\n",
      "1400 0.21086007\n",
      "1600 0.1930259\n",
      "1800 0.17785762\n",
      "2000 0.16481116\n",
      "==================================================\n",
      "[1]\n",
      "==================================================\n",
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(train, feed_dict={x:x_data, y:y_data})\n",
    "        if step % 200==0:\n",
    "            print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}))\n",
    "    print(\"=\"*50)\n",
    "#xdata= 1, 11, 7, 9   => y?\n",
    "    res=sess.run(hf, feed_dict={x:[[1, 11, 7, 9 ]]})\n",
    "    print(sess.run(tf.argmax(res, axis=1)))\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    res2=sess.run(hf, feed_dict={x:[[1, 11, 7, 9 ],\n",
    "                                   [1,3,4,3],\n",
    "                                   [1,1,0,1]]})\n",
    "    print(sess.run(tf.argmax(res2, axis=1)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0774668e-02, 9.8921645e-01, 8.8615579e-06]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy=np.loadtxt('zoo.csv', delimiter=',', dtype=np.float32)\n",
    "xy.shape #101,17\n",
    "xdata=xy[:, 0:-1]\n",
    "ydata=xy[:,[-1]]\n",
    "xdata.shape\n",
    "ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_9:0\", shape=(?, 1, 7), dtype=float32)\n",
      "one_hot after reshape Tensor(\"Reshape_9:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(tf.float32, [None, 16])\n",
    "y=tf.placeholder(tf.int32, [None, 1])\n",
    "y_one_hot=tf.one_hot(y,7) # 3(y) -> 0001000(y_one_hot)\n",
    "print(\"one_hot\", y_one_hot)\n",
    "y_one_hot=tf.reshape(y_one_hot, [-1,7])\n",
    "print(\"one_hot after reshape\", y_one_hot)\n",
    "\n",
    "#one hot함수는 한 차원 높게 변환됨\n",
    "#y: [[0], [3], ..., [5]]=> [None,1] 의 shape\n",
    "#one_hot =>\n",
    "#[[[1000000]], [[0001000]], ..., [[00000100]]] => [None, 1, 7]\n",
    "#우리가 바라는 출력 결과의 모습은 [None,7] 임.\n",
    "#reshape을 하면 됨. tf.shape(y_one_hot, [-1, 7])코딩하면\n",
    "#[None,7]로 reshape 됨.\n",
    "#[[[1000000]], [[0001000]]...] => [[1000000], [0001000]...] \n",
    "\n",
    "\n",
    "#0~6 :7가지 종류의 동물 -> 분류기 7개\n",
    "#w1x1+w2x2+...w7x7+b => softmax => 0번 종류의 동물에 대한 확률\n",
    "#w1x1+w2x2+...w7x7+b => softmax => 1번 종류의 동물에 대한 확률\n",
    "#..\n",
    "##w1x1+w2x2+...w7x7+b => softmax => 6번 종류의 동물에 대한 확률\n",
    "\n",
    "#1000000, 0100000... => [None,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.random_normal([16, 7]))\n",
    "b=tf.Variable(tf.random_normal([7]))\n",
    "#x:[None,16], w:[16,7]\n",
    "logit= tf.matmul(x,w)+b\n",
    "hf=tf.nn.softmax(logit)\n",
    "cost=tf.nn.softmax_cross_entropy_with_logits(logits=logit,\n",
    "                                             labels=y_one_hot)\n",
    "cost2=tf.reduce_mean(cost)\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost2)\n",
    "prediction=tf.argmax(hf,1)\n",
    "correct_prediction= tf.equal(prediction,tf.argmax(y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 cost: 6.4049406 acc: 0.22772278\n",
      "step: 100 cost: 0.6056478 acc: 0.8118812\n",
      "step: 200 cost: 0.40867412 acc: 0.8910891\n",
      "step: 300 cost: 0.30601144 acc: 0.9207921\n",
      "step: 400 cost: 0.24107844 acc: 0.9405941\n",
      "step: 500 cost: 0.19698489 acc: 0.97029704\n",
      "step: 600 cost: 0.16565429 acc: 0.97029704\n",
      "step: 700 cost: 0.14258347 acc: 0.990099\n",
      "step: 800 cost: 0.12506601 acc: 0.990099\n",
      "step: 900 cost: 0.111400634 acc: 0.990099\n",
      "step: 1000 cost: 0.10048217 acc: 0.990099\n",
      "step: 1100 cost: 0.09157333 acc: 1.0\n",
      "step: 1200 cost: 0.084169984 acc: 1.0\n",
      "step: 1300 cost: 0.07791944 acc: 1.0\n",
      "step: 1400 cost: 0.072569855 acc: 1.0\n",
      "step: 1500 cost: 0.06793719 acc: 1.0\n",
      "step: 1600 cost: 0.063884206 acc: 1.0\n",
      "step: 1700 cost: 0.060306814 acc: 1.0\n",
      "step: 1800 cost: 0.057124615 acc: 1.0\n",
      "step: 1900 cost: 0.05427452 acc: 1.0\n",
      "step: 2000 cost: 0.051706262 acc: 1.0\n",
      "step: 2100 cost: 0.049379364 acc: 1.0\n",
      "step: 2200 cost: 0.04726085 acc: 1.0\n",
      "step: 2300 cost: 0.045323424 acc: 1.0\n",
      "step: 2400 cost: 0.043544617 acc: 1.0\n",
      "step: 2500 cost: 0.041905336 acc: 1.0\n",
      "step: 2600 cost: 0.04038963 acc: 1.0\n",
      "step: 2700 cost: 0.038983826 acc: 1.0\n",
      "step: 2800 cost: 0.03767617 acc: 1.0\n",
      "step: 2900 cost: 0.036456633 acc: 1.0\n",
      "step: 3000 cost: 0.035316475 acc: 1.0\n",
      "step: 3100 cost: 0.034248084 acc: 1.0\n",
      "step: 3200 cost: 0.033244792 acc: 1.0\n",
      "step: 3300 cost: 0.032300744 acc: 1.0\n",
      "step: 3400 cost: 0.03141077 acc: 1.0\n",
      "step: 3500 cost: 0.030570287 acc: 1.0\n",
      "step: 3600 cost: 0.02977519 acc: 1.0\n",
      "step: 3700 cost: 0.029021917 acc: 1.0\n",
      "step: 3800 cost: 0.028307172 acc: 1.0\n",
      "step: 3900 cost: 0.027628021 acc: 1.0\n",
      "step: 4000 cost: 0.026981814 acc: 1.0\n",
      "step: 4100 cost: 0.026366245 acc: 1.0\n",
      "step: 4200 cost: 0.025779054 acc: 1.0\n",
      "step: 4300 cost: 0.02521839 acc: 1.0\n",
      "step: 4400 cost: 0.024682427 acc: 1.0\n",
      "step: 4500 cost: 0.024169534 acc: 1.0\n",
      "step: 4600 cost: 0.023678252 acc: 1.0\n",
      "step: 4700 cost: 0.023207232 acc: 1.0\n",
      "step: 4800 cost: 0.022755183 acc: 1.0\n",
      "step: 4900 cost: 0.022321032 acc: 1.0\n",
      "step: 5000 cost: 0.021903653 acc: 1.0\n",
      "step: 5100 cost: 0.0215021 acc: 1.0\n",
      "step: 5200 cost: 0.021115525 acc: 1.0\n",
      "step: 5300 cost: 0.020743018 acc: 1.0\n",
      "step: 5400 cost: 0.020383896 acc: 1.0\n",
      "step: 5500 cost: 0.020037327 acc: 1.0\n",
      "step: 5600 cost: 0.019702762 acc: 1.0\n",
      "step: 5700 cost: 0.019379556 acc: 1.0\n",
      "step: 5800 cost: 0.019067118 acc: 1.0\n",
      "step: 5900 cost: 0.018764867 acc: 1.0\n",
      "step: 6000 cost: 0.018472347 acc: 1.0\n",
      "step: 6100 cost: 0.018189099 acc: 1.0\n",
      "step: 6200 cost: 0.017914683 acc: 1.0\n",
      "step: 6300 cost: 0.017648647 acc: 1.0\n",
      "step: 6400 cost: 0.017390648 acc: 1.0\n",
      "step: 6500 cost: 0.017140342 acc: 1.0\n",
      "step: 6600 cost: 0.016897313 acc: 1.0\n",
      "step: 6700 cost: 0.016661301 acc: 1.0\n",
      "step: 6800 cost: 0.016431956 acc: 1.0\n",
      "step: 6900 cost: 0.016209057 acc: 1.0\n",
      "step: 7000 cost: 0.01599231 acc: 1.0\n",
      "step: 7100 cost: 0.015781404 acc: 1.0\n",
      "step: 7200 cost: 0.015576181 acc: 1.0\n",
      "step: 7300 cost: 0.015376347 acc: 1.0\n",
      "step: 7400 cost: 0.015181772 acc: 1.0\n",
      "step: 7500 cost: 0.0149921365 acc: 1.0\n",
      "step: 7600 cost: 0.014807343 acc: 1.0\n",
      "step: 7700 cost: 0.014627169 acc: 1.0\n",
      "step: 7800 cost: 0.014451471 acc: 1.0\n",
      "step: 7900 cost: 0.014280029 acc: 1.0\n",
      "step: 8000 cost: 0.014112759 acc: 1.0\n",
      "step: 8100 cost: 0.01394942 acc: 1.0\n",
      "step: 8200 cost: 0.013789969 acc: 1.0\n",
      "step: 8300 cost: 0.013634183 acc: 1.0\n",
      "step: 8400 cost: 0.013481991 acc: 1.0\n",
      "step: 8500 cost: 0.01333322 acc: 1.0\n",
      "step: 8600 cost: 0.0131878005 acc: 1.0\n",
      "step: 8700 cost: 0.013045607 acc: 1.0\n",
      "step: 8800 cost: 0.01290655 acc: 1.0\n",
      "step: 8900 cost: 0.012770475 acc: 1.0\n",
      "step: 9000 cost: 0.012637307 acc: 1.0\n",
      "step: 9100 cost: 0.01250695 acc: 1.0\n",
      "step: 9200 cost: 0.012379337 acc: 1.0\n",
      "step: 9300 cost: 0.012254366 acc: 1.0\n",
      "step: 9400 cost: 0.012131982 acc: 1.0\n",
      "step: 9500 cost: 0.012012036 acc: 1.0\n",
      "step: 9600 cost: 0.011894538 acc: 1.0\n",
      "step: 9700 cost: 0.011779366 acc: 1.0\n",
      "step: 9800 cost: 0.011666465 acc: 1.0\n",
      "step: 9900 cost: 0.0115557145 acc: 1.0\n",
      "step: 10000 cost: 0.011447146 acc: 1.0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 1 실제 y: 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x:xdata, y:ydata})\n",
    "        if step % 100==0:\n",
    "            cv, av=sess.run([cost2, accuracy], feed_dict={x:xdata, y:ydata})\n",
    "            print(\"step:\", step, \"cost:\",cv, \"acc:\", av)\n",
    "#전체 데이터로 트레이닝 수행하여 모델 생성\n",
    "#모델에 전체 데이터를 집어 넣어서 정확도 출력\n",
    "#데이터를 분할하지 않았음\n",
    "    pred=sess.run(prediction, feed_dict={x:xdata})\n",
    "    for p, y in zip(pred, ydata.flatten()):\n",
    "        print(\"[{}] 예측: {} 실제 y: {}\".\n",
    "              format(p==int(y), p, int(y)))\n",
    "    #print(sess.run(prediction, feed_dict={x:[[...]]}))\n",
    "        \n",
    "    #[[0],[3],...] => [0, 3, ...]\n",
    "    #print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten\n",
    "#[[1],[0]] -> [1,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. mushroom데이터 분류모델\n",
    "# 2. animal데이터를 7:3로 분할 -> 모델, 테스트\n",
    "# 3. breast cancer(유방암)\n",
    "# 4. wine quality(10단계)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
