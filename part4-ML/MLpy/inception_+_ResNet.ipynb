{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inception + ResNet",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPBlN93TOPtb",
        "colab_type": "code",
        "outputId": "c0d120d1-2b04-42c4-f680-4ac8eb97f62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poX7_GniOypD",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 로드 & 함수 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWeR-AWtOZrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "630c72e5-64e7-4845-b140-6f64aaeb2874"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "data = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = data.load_data()\n",
        "\n",
        "test_labels = test_labels.reshape(-1,1)\n",
        "train_labels = train_labels.reshape(-1,1)\n",
        "\n",
        "train_images = train_images / 255\n",
        "test_images = test_images /255\n",
        "\n",
        "def make_conv_twice(input_layer, num_filter, keep_prob, layer_num,  batch_prob, filter_size=3, strides=1):\n",
        "#   w1 = tf.Variable(tf.random_normal([filter_size,filter_size,1,num_filter]) )\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding='SAME')\n",
        "  L1 = tf.layers.batch_normalization(L1, center=True, scale=True, training=batch_prob)\n",
        "  L1 = tf.nn.relu(L1)\n",
        "#   w1_1 = tf.Variable(tf.random_normal([filter_size,filter_size,num_filter,num_filter]))\n",
        "  w1_1 = tf.get_variable(\"w2_\"+str(layer_num), shape=[filter_size,filter_size,num_filter,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1_1 = tf.nn.conv2d(L1, w1_1, strides=[1,strides,strides,1], padding='SAME') \n",
        "  L1_1 = tf.nn.relu(L1_1)\n",
        "  L1_1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "  res = L1_1\n",
        "#   res = tf.nn.dropout(L1_1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once(input_layer, num_filter, keep_prob, layer_num, batch_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding='SAME')\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once_not_same(input_layer, num_filter, keep_prob, layer_num, batch_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  w1 = tf.Variable(tf.random_normal([filter_size,filter_size,1,num_filter]))\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding=\"VALID\")\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once_not_same_wo_pooling(input_layer, num_filter, keep_prob, layer_num, batch_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding=\"VALID\")\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once_wo_pooling(input_layer, num_filter, keep_prob, layer_num, batch_prob, padding=\"SAME\", filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding=padding)\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def my_inception_model(input_layer, keep_prob, batch_prob, filter_num_list):\n",
        "  w1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[0]]))\n",
        "  l1 = tf.nn.conv2d(input_layer, w1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  w2_1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[1]]))\n",
        "  l2_1 = tf.nn.conv2d(input_layer, w2_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w2_2 = tf.Variable(tf.random_normal([3, 3, filter_num_list[1], filter_num_list[2]]))\n",
        "  l2_2 = tf.nn.conv2d(l2_1, w2_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  w3_1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[3]]))\n",
        "  l3_1 = tf.nn.conv2d(input_layer, w3_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w3_2 = tf.Variable(tf.random_normal([5, 5, filter_num_list[3], filter_num_list[4]]))\n",
        "  l3_2 = tf.nn.conv2d(l3_1, w3_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  l4_1 = tf.nn.max_pool(input_layer, ksize=[1,3,3,1], strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w4_1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[5]]))\n",
        "  l4_2 = tf.nn.conv2d(l4_1, w4_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  res = tf.concat([l1, l2_2, l3_2, l4_2], 3) # input의 shape가 [-1,28,28,1] 이라고 가정\n",
        "  \n",
        "  return res # 다시 이 return된 layer를 inception 모델에 돌리기!\n",
        "\n",
        "def my_inception_model_v2(input_layer, keep_prob, batch_prob, filter_num_list, layer_num):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[0]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l1 = tf.nn.conv2d(input_layer, w1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l1 = tf.layers.batch_normalization(l1, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  w2_1 = tf.get_variable(\"w2_1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[1]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l2_1 = tf.nn.conv2d(input_layer, w2_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l2_1 = tf.layers.batch_normalization(l2_1, center=True, scale=True, training=batch_prob)\n",
        "  w2_2 = tf.get_variable(\"w2_2_\"+str(layer_num), shape=[3,3,filter_num_list[1],filter_num_list[2]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l2_2 = tf.nn.conv2d(l2_1, w2_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l2_2 = tf.layers.batch_normalization(l2_2, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  w3_1 = tf.get_variable(\"w3_1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[3]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l3_1 = tf.nn.conv2d(input_layer, w3_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l3_1 = tf.layers.batch_normalization(l3_1, center=True, scale=True, training=batch_prob)\n",
        "  w3_2 = tf.get_variable(\"w3_2_\"+str(layer_num), shape=[5,5,filter_num_list[3],filter_num_list[4]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l3_2 = tf.nn.conv2d(l3_1, w3_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l3_2 = tf.layers.batch_normalization(l3_2, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  l4_1 = tf.nn.max_pool(input_layer, ksize=[1,3,3,1], strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w4_1 = tf.get_variable(\"w4_1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[5]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l4_2 = tf.nn.conv2d(l4_1, w4_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l4_2 = tf.layers.batch_normalization(l4_2, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  res = tf.concat([l1, l2_2, l3_2, l4_2], 3) # input의 shape가 [-1,28,28,1] 이라고 가정\n",
        "  \n",
        "  return res # 다시 이 return된 layer를 inception 모델에 돌리기!\n",
        "\n",
        "# 이 함수에는 pooling이 포함되어 있지 않습니다.\n",
        "def my_resNet_model(input_layer, keep_prob, batch_prob, filter_num_list, layer_num):\n",
        "  bn1 = tf.layers.batch_normalization(input_layer, center=True, scale=True, training=batch_prob)\n",
        "  bn1 = tf.nn.relu(bn1)\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[3,3,filter_num_list[0],filter_num_list[1]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  h1 = tf.nn.conv2d(input_layer, w1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  bn2 = tf.layers.batch_normalization(input_layer, center=True, scale=True, training=batch_prob)\n",
        "  bn2 = tf.nn.relu(bn2)\n",
        "  w2 = tf.get_variable(\"w2_\"+str(layer_num), shape=[3,3,filter_num_list[1],filter_num_list[2]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  h2 = tf.nn.conv2d(bn2, w2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  if filter_num_list[0]!=filter_num_list[-1]:\n",
        "    us = tf.get_variable(\"us_\"+str(layer_num), shape=[1,1,filter_num_list[0],filter_num_list[2]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "    input_layer = tf.nn.conv2d(input_layer, us, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  return input_layer + h2\n",
        "  \n",
        "  \n",
        "def shuffleData(images, labels):\n",
        "  tmp = np.concatenate([images.reshape(-1,28*28),labels],axis=1)\n",
        "  tmpd = pd.DataFrame(tmp).sample(frac=1)\n",
        "  ret = tmpd.values.reshape(-1,785)\n",
        "  retImg = ret[:,:-1].reshape(-1,28,28)\n",
        "  retlab = ret[:,-1].reshape(-1,1)\n",
        "  return retImg, retlab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-isJuULW1-P",
        "colab_type": "code",
        "outputId": "87408c7f-c0ef-4343-d486-343ac2f5277e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VVgBufnO2c1",
        "colab_type": "text"
      },
      "source": [
        "# 실제 실행 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y57ViWk_Owej",
        "colab_type": "code",
        "outputId": "d6b35774-cfff-4275-a1d8-1803a629b4c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 32\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0812 16:23:43.272491 139653728552832 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0812 16:23:43.329032 139653728552832 deprecation.py:323] From <ipython-input-2-01a77bc5bd73>:94: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W0812 16:23:47.246823 139653728552832 deprecation.py:506] From <ipython-input-4-d31cd5ee0471>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gic0WtrnQuct",
        "colab_type": "code",
        "outputId": "761b7ed5-5d69-417b-9a16-3d45bb7c6f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i == 900 or i == 1800:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    saver.save(sess, save_file)\n",
        "  \n",
        "  final_acu = 0\n",
        "  for num in range(100):\n",
        "    acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "    final_acu += acu\n",
        "  final_acu = final_acu / 100\n",
        "  print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5199999809265137, avg_cost: 0.9997651163736981\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.5199999809265137, avg_cost: 0.9849679580330843\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.47999998927116394, avg_cost: 0.9779691456754994\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5299999713897705, avg_cost: 0.9669578102231022\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.6299999952316284, avg_cost: 0.9599478222926454\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.5600000023841858, avg_cost: 0.9584405269225437\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5799999833106995, avg_cost: 0.9416050454974174\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.5600000023841858, avg_cost: 0.9386361687382061\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.6399999856948853, avg_cost: 0.9302902710437772\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.5899999737739563, avg_cost: 0.9301680853962903\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.5699999928474426, avg_cost: 0.9202526120344803\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.5600000023841858, avg_cost: 0.9111420182387028\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.5, avg_cost: 0.906685955822467\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.6399999856948853, avg_cost: 0.8989764041701954\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.6499999761581421, avg_cost: 0.8907221458355584\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.5899999737739563, avg_cost: 0.8957964470982547\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.5, avg_cost: 0.8922380998730661\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.6100000143051147, avg_cost: 0.8785748392343519\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.6700000166893005, avg_cost: 0.8809885105490682\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.5199999809265137, avg_cost: 0.869912796914578\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.5899999737739563, avg_cost: 0.8679758891463278\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.5899999737739563, avg_cost: 0.8671677815914155\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.6000000238418579, avg_cost: 0.8654734166463213\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.5600000023841858, avg_cost: 0.8615362991889314\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.6399999856948853, avg_cost: 0.8668234710892035\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.6499999761581421, avg_cost: 0.8664153464635214\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.5799999833106995, avg_cost: 0.8528024462858841\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.6000000238418579, avg_cost: 0.8686522155006734\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.5699999928474426, avg_cost: 0.8689878271023428\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.550000011920929, avg_cost: 0.8586395846803982\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.550000011920929, avg_cost: 0.8522232209642726\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.6399999856948853, avg_cost: 0.8579460100332895\n",
            "**************************************************\n",
            "final_acc: 0.10059999968856573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wy4S609Dzl5",
        "colab_type": "text"
      },
      "source": [
        "# validation : batch_prob : True!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzg193XXU2EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 32\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eHpG_njD8iD",
        "colab_type": "code",
        "outputId": "43538c98-3811-4301-be8f-c66c34915b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"validation start\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "    \n",
        "  final_acu = 0\n",
        "  for num in range(10):\n",
        "    acu = sess.run(acc, feed_dict={x:test_images[1000*num:1000*(num+1),:,:],y:test_labels[1000*num:1000*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "    final_acu += acu\n",
        "  final_acu = final_acu / 10\n",
        "  print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation start\n",
            "final_acc: 0.9282999992370605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSkzlD6TFfET",
        "colab_type": "text"
      },
      "source": [
        "# Batch size가 너무 작았나 ? : 256으로 수정 / 모델 지속적 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLDx-lM_EVZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 10\n",
        "batch_size = 256\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "# load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch.ckpt'\n",
        "# load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_accuracy_val.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiYjpph8Fs02",
        "colab_type": "code",
        "outputId": "9921ff82-3314-4d17-a3ac-f0329ad60d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if total_batch%50 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.44921875, avg_cost: 1.4835703357672079\n",
            "**************************************************\n",
            "test_acc: 0.09999999990686774\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.54296875, avg_cost: 1.1641503050286544\n",
            "**************************************************\n",
            "test_acc: 0.09999999990686774\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.5859375, avg_cost: 1.1033311218787467\n",
            "**************************************************\n",
            "test_acc: 0.1669000004976988\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5546875, avg_cost: 1.0771177719291456\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.5703125, avg_cost: 1.0460005257374196\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.6015625, avg_cost: 1.0341714381152747\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5546875, avg_cost: 1.0215940029702641\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.59375, avg_cost: 1.0038603307344978\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.56640625, avg_cost: 1.0002676538932016\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.54296875, avg_cost: 0.9899300619577751\n",
            "**************************************************\n",
            "test_acc: 0.09999999983236194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGkD_WCaaoDP",
        "colab_type": "text"
      },
      "source": [
        "# Restore해서 계속 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYPXpOo_Ip5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 50\n",
        "batch_size = 256\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_add_50.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIrcJ78sau9T",
        "colab_type": "code",
        "outputId": "4ccf3218-b600-4102-c8ec-0be674a8fec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if total_batch%50 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5625, avg_cost: 0.8366367936643783\n",
            "**************************************************\n",
            "test_acc: 0.9346000015735626\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.61328125, avg_cost: 0.8306661834064716\n",
            "**************************************************\n",
            "test_acc: 0.9338000023365021\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.58984375, avg_cost: 0.8349245249206184\n",
            "**************************************************\n",
            "test_acc: 0.9358000016212463\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5625, avg_cost: 0.8325600341344492\n",
            "**************************************************\n",
            "test_acc: 0.9371000003814697\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.60546875, avg_cost: 0.8285959785820071\n",
            "**************************************************\n",
            "test_acc: 0.935100000500679\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.59375, avg_cost: 0.8293797557170571\n",
            "**************************************************\n",
            "test_acc: 0.9358999991416931\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.6015625, avg_cost: 0.8257910889438078\n",
            "**************************************************\n",
            "test_acc: 0.9365000009536744\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.58984375, avg_cost: 0.8346338203320139\n",
            "**************************************************\n",
            "test_acc: 0.9361999988555908\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.56640625, avg_cost: 0.830291777085035\n",
            "**************************************************\n",
            "test_acc: 0.9371999996900559\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.55859375, avg_cost: 0.8340427855141138\n",
            "**************************************************\n",
            "test_acc: 0.9378000003099441\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.54296875, avg_cost: 0.8368155811077507\n",
            "**************************************************\n",
            "test_acc: 0.9374000006914138\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.578125, avg_cost: 0.8255395270310916\n",
            "**************************************************\n",
            "test_acc: 0.9371000009775162\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.6171875, avg_cost: 0.830983622970744\n",
            "**************************************************\n",
            "test_acc: 0.9357000023126603\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.59375, avg_cost: 0.8566839689882388\n",
            "**************************************************\n",
            "test_acc: 0.9115000027418136\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.61328125, avg_cost: 0.8664581872459151\n",
            "**************************************************\n",
            "test_acc: 0.9274000018835068\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.60546875, avg_cost: 0.8407601301486671\n",
            "**************************************************\n",
            "test_acc: 0.9332999992370605\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.578125, avg_cost: 0.8435161213080092\n",
            "**************************************************\n",
            "test_acc: 0.9321999990940094\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.625, avg_cost: 0.838775946543767\n",
            "**************************************************\n",
            "test_acc: 0.9322000014781952\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.6484375, avg_cost: 0.8359945182107454\n",
            "**************************************************\n",
            "test_acc: 0.9319000029563904\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.56640625, avg_cost: 0.8365147610505425\n",
            "**************************************************\n",
            "test_acc: 0.9261000037193299\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.58203125, avg_cost: 0.8467302284179589\n",
            "**************************************************\n",
            "test_acc: 0.9284000015258789\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.578125, avg_cost: 0.8393728740704369\n",
            "**************************************************\n",
            "test_acc: 0.9274000006914139\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.59375, avg_cost: 0.8403550511241976\n",
            "**************************************************\n",
            "test_acc: 0.9331999999284745\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.58203125, avg_cost: 0.8396766802184604\n",
            "**************************************************\n",
            "test_acc: 0.9319000035524369\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.60546875, avg_cost: 0.8361561568374308\n",
            "**************************************************\n",
            "test_acc: 0.9345000034570694\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.546875, avg_cost: 0.8409606429756198\n",
            "**************************************************\n",
            "test_acc: 0.9334000033140183\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.640625, avg_cost: 0.8377864116277447\n",
            "**************************************************\n",
            "test_acc: 0.9337000036239624\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.62890625, avg_cost: 0.836057935005579\n",
            "**************************************************\n",
            "test_acc: 0.9291000032424926\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.59765625, avg_cost: 0.8410058039375863\n",
            "**************************************************\n",
            "test_acc: 0.9324000018835068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0e638ab37f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-05X_s1EQBW",
        "colab_type": "text"
      },
      "source": [
        "# validation : Inception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpkGGM_ebWmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 50\n",
        "batch_size = 256\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_add_50.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9QOtjFEEVAw",
        "colab_type": "code",
        "outputId": "11710ca9-f137-4874-f9c8-d6c81310dfae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"validation start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, save_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  final_acu = 0\n",
        "  for num in range(100):\n",
        "    acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "    final_acu += acu\n",
        "  final_acu = final_acu / 100\n",
        "  print(\"test_acc: {}\".format(final_acu))\n",
        "\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "test_acc: 0.9378000003099441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhq7y-AYExqm",
        "colab_type": "text"
      },
      "source": [
        "# ResNet 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwmVBYQMEjS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0813_BN_100epoch_mod.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 128, keep_prob, '1b', batch_prob)\n",
        "h1_1 = tf.layers.batch_normalization(h1_1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,512]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList11 = [512,512,512]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [512,512,512]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [512,512,1024]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "\n",
        "h15 = tf.nn.avg_pool(h14, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6bCjHXvE42p",
        "colab_type": "code",
        "outputId": "b691c96e-9aee-451a-8d60-1da552277618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.07999999821186066, avg_cost: 0.005154190063476563\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 1.9964513392249748\n",
            "batch: 200, train_acc: 0.7099999785423279, avg_cost: 2.2375136198600143\n",
            "batch: 300, train_acc: 0.7599999904632568, avg_cost: 2.412682707111044\n",
            "batch: 400, train_acc: 0.8399999737739563, avg_cost: 2.5811256759862133\n",
            "batch: 500, train_acc: 0.7799999713897705, avg_cost: 2.7341592581073457\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.8199999928474426, avg_cost: 2.8510153746604936\n",
            "**************************************************\n",
            "test_acc: 0.7984000009298324\n",
            "batch: 0, train_acc: 0.800000011920929, avg_cost: 0.0009434366226196289\n",
            "batch: 100, train_acc: 0.7300000190734863, avg_cost: 0.19467283651232725\n",
            "batch: 200, train_acc: 0.9200000166893005, avg_cost: 0.33258394047617906\n",
            "batch: 300, train_acc: 0.8700000047683716, avg_cost: 0.44289749021331465\n",
            "batch: 400, train_acc: 0.8500000238418579, avg_cost: 0.5603665421406429\n",
            "batch: 500, train_acc: 0.8500000238418579, avg_cost: 0.6510553870101772\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.8700000047683716, avg_cost: 0.7340909018119177\n",
            "**************************************************\n",
            "test_acc: 0.8440000003576279\n",
            "batch: 0, train_acc: 0.8700000047683716, avg_cost: 0.0006785560647646586\n",
            "batch: 100, train_acc: 0.8600000143051147, avg_cost: 0.07413838220139345\n",
            "batch: 200, train_acc: 0.8799999952316284, avg_cost: 0.14815343352655558\n",
            "batch: 300, train_acc: 0.7300000190734863, avg_cost: 0.22651798225939254\n",
            "batch: 400, train_acc: 0.8399999737739563, avg_cost: 0.3761584546913701\n",
            "batch: 500, train_acc: 0.8799999952316284, avg_cost: 0.46128022243579186\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.8799999952316284, avg_cost: 0.5250356141477822\n",
            "**************************************************\n",
            "test_acc: 0.8662000000476837\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.0008178543051083882\n",
            "batch: 100, train_acc: 0.8799999952316284, avg_cost: 0.07762692620356879\n",
            "batch: 200, train_acc: 0.8500000238418579, avg_cost: 0.15108186744153498\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.21643676385283475\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.27156638869394867\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.3205958382909498\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.8899999856948853, avg_cost: 0.37262253350267877\n",
            "**************************************************\n",
            "test_acc: 0.8949000000953674\n",
            "batch: 0, train_acc: 0.8799999952316284, avg_cost: 0.0004292422036329905\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.04994467178980509\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.10486312953134376\n",
            "batch: 300, train_acc: 0.8700000047683716, avg_cost: 0.1684720111141602\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.2154240253070991\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.26201706329981506\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.8999999761581421, avg_cost: 0.30935231226185983\n",
            "**************************************************\n",
            "test_acc: 0.9000000017881393\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0003716235856215159\n",
            "batch: 100, train_acc: 0.8799999952316284, avg_cost: 0.04718037704626721\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.08913484857728082\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.13563822676738094\n",
            "batch: 400, train_acc: 0.8600000143051147, avg_cost: 0.1809344558169443\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.2274268771087128\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.949999988079071, avg_cost: 0.26988040873159946\n",
            "**************************************************\n",
            "test_acc: 0.8993999993801117\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0003771426280339559\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.03594779495149852\n",
            "batch: 200, train_acc: 0.8700000047683716, avg_cost: 0.07733245609949031\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.11781670412669579\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.1595341245581705\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.20373079172025102\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.9100000262260437, avg_cost: 0.24295661093046236\n",
            "**************************************************\n",
            "test_acc: 0.8955999982357025\n",
            "batch: 0, train_acc: 0.8899999856948853, avg_cost: 0.00037847566107908885\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.034754508739958216\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.06822745151196916\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.10231215183312695\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.13593645414958394\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.1709220537543297\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.7300000190734863, avg_cost: 0.7906106751412153\n",
            "**************************************************\n",
            "test_acc: 0.5671999981999397\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0033081968625386557\n",
            "batch: 100, train_acc: 0.6600000262260437, avg_cost: 0.21098574837048847\n",
            "batch: 200, train_acc: 0.8700000047683716, avg_cost: 0.2801089279105264\n",
            "batch: 300, train_acc: 0.8199999928474426, avg_cost: 0.3538126650204258\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.4108919249723354\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.4636877402663232\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.949999988079071, avg_cost: 0.512335744251808\n",
            "**************************************************\n",
            "test_acc: 0.9043000000715256\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0001747969662149747\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.04348794944584371\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.09031525850296021\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.13260169946899011\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.169166556832691\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.20402826473116878\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.949999988079071, avg_cost: 0.2402246898785233\n",
            "**************************************************\n",
            "test_acc: 0.9123000001907349\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00037488015989462533\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.034157934648295245\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.06603333743289114\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.0997107405401767\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.12988500384613885\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.16278332263852166\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9300000071525574, avg_cost: 0.19782931090022138\n",
            "**************************************************\n",
            "test_acc: 0.9106000024080276\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00025651633739471436\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.027902014907449486\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.05472847208380698\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.08382541075348857\n",
            "batch: 400, train_acc: 0.9100000262260437, avg_cost: 0.11292831844960649\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.14555255100751918\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.9300000071525574, avg_cost: 0.17991232975696517\n",
            "**************************************************\n",
            "test_acc: 0.9135000032186508\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00017642635852098464\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.026034525868793325\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.05249674472957848\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.08502831727266309\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.12273716809848935\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.15616098242501408\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.9399999976158142, avg_cost: 0.18513166400293507\n",
            "**************************************************\n",
            "test_acc: 0.9150999987125397\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0004949122170607249\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.02601604901254176\n",
            "batch: 200, train_acc: 0.9100000262260437, avg_cost: 0.05101431363572672\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.08139121547341345\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.11149654197196163\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.1415777389456828\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.9800000190734863, avg_cost: 0.16784833968927462\n",
            "**************************************************\n",
            "test_acc: 0.9184000027179718\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.65782176454862e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.020354559002444147\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.043200692242632316\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.06974531452481948\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.0976869584682087\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.1257273493458827\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.9200000166893005, avg_cost: 0.1513810210116208\n",
            "**************************************************\n",
            "test_acc: 0.9157000035047531\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00024286056558291117\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.027007957777629297\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.055368055024494746\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.08170814500811197\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.10780632951452082\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.13243553162707633\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.8500000238418579, avg_cost: 0.16057118084126457\n",
            "**************************************************\n",
            "test_acc: 0.9054000020027161\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.00043593987822532654\n",
            "batch: 100, train_acc: 0.8199999928474426, avg_cost: 0.3968072225898505\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.5413956431547802\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.5819758614152671\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.6118448888075847\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.6393211071627833\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.9100000262260437, avg_cost: 0.6651251899761457\n",
            "**************************************************\n",
            "test_acc: 0.9226000016927719\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00026164879401524864\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.017792328366388874\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.03462144194170834\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.05355084470783671\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.07227074357370539\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.08933862015294533\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9800000190734863, avg_cost: 0.10722805393549302\n",
            "**************************************************\n",
            "test_acc: 0.9231000006198883\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00018762451906998952\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.011615546829998493\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.025065884417854237\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.03952878227224573\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.05326021486660466\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.06902643535829459\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9900000095367432, avg_cost: 0.08431598786807934\n",
            "**************************************************\n",
            "test_acc: 0.9234000027179718\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00010834300269683202\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.009195036496967077\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.02205216217475633\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.032308857847626014\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.044032826675102145\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.057707273870085715\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.9599999785423279, avg_cost: 0.07236947623702392\n",
            "**************************************************\n",
            "test_acc: 0.919900004863739\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00012382323543230693\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.010734905476371445\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.025323904876907676\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.03908924233323584\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.056599415871314694\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.0791860213658462\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.9300000071525574, avg_cost: 0.10577716868060329\n",
            "**************************************************\n",
            "test_acc: 0.887899997830391\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00025956315298875175\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.020249451582009595\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.033542432005051505\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.0479182650684379\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.06033759736766421\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.07341019160424675\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.9800000190734863, avg_cost: 0.08659566245041796\n",
            "**************************************************\n",
            "test_acc: 0.9223000025749206\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00019360660264889398\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.010910380359273403\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.024333403439571466\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.03563779110088947\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.05299937951300917\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.08227447484542304\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9700000286102295, avg_cost: 0.12630455905183527\n",
            "**************************************************\n",
            "test_acc: 0.8905000001192093\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00026120165983835856\n",
            "batch: 100, train_acc: 0.9100000262260437, avg_cost: 0.09218759109576545\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.15376985786482686\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.17298281695383283\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.18827015980302036\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.2014167667490739\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9800000190734863, avg_cost: 0.21435297553272284\n",
            "**************************************************\n",
            "test_acc: 0.9296000009775162\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.714831744631132e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005860502882472551\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011503835978995385\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.016491120845700313\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.023585652267211123\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.03182387047350251\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.9900000095367432, avg_cost: 0.039985982528693634\n",
            "**************************************************\n",
            "test_acc: 0.9229000002145767\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 7.503458609183629e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005160309756950786\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.009986662194326831\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.017677490557931986\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.02742482368931328\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.037258533452210627\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.9800000190734863, avg_cost: 0.04665523556390934\n",
            "**************************************************\n",
            "test_acc: 0.9216000008583068\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.126316592097283e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.007253071791686428\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.014411427090332543\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.027622089604847144\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.0920873723637002\n",
            "batch: 500, train_acc: 0.8700000047683716, avg_cost: 0.17474780246227345\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9900000095367432, avg_cost: 0.22697042010491694\n",
            "**************************************************\n",
            "test_acc: 0.911700000166893\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.563871184984843e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.015632512813899664\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.02679218661408714\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.03397213335498233\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.04050534102638871\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.04805747605064727\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9800000190734863, avg_cost: 0.05680983753700274\n",
            "**************************************************\n",
            "test_acc: 0.9220000046491623\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 7.330136994520824e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0032321423784014763\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005971544649413161\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.008382372094783925\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01203823512150847\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.017341542082940586\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 1.0, avg_cost: 0.022915091820153388\n",
            "**************************************************\n",
            "test_acc: 0.9230000030994415\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 6.761778766910235e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0035740326131538792\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.007584341204346856\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0114177423477425\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01705006314143248\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.027631163506424247\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9599999785423279, avg_cost: 0.043443307423464535\n",
            "**************************************************\n",
            "test_acc: 0.9049000036716461\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0003632429490486781\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.03637354676437098\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.1579140740460328\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.21108837697790772\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.2299836677698962\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.2423513595951955\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9900000095367432, avg_cost: 0.25079670004372046\n",
            "**************************************************\n",
            "test_acc: 0.9280000030994415\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 6.803921734293302e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.00475144272970889\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.008314590754792631\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.012569535751050952\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.019368431759988484\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02446280951471028\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.9900000095367432, avg_cost: 0.030191165815170565\n",
            "**************************************************\n",
            "test_acc: 0.9272000002861023\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.8094365522265435e-06\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002334249893368299\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005512297466799886\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.009524576049467819\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013556216878763117\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01791731874451215\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.9900000095367432, avg_cost: 0.02251244361301361\n",
            "**************************************************\n",
            "test_acc: 0.9249000012874603\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.195578844596942e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002887296224071178\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.005498416526024812\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010201890564809824\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.015501150046645007\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.022517738984679465\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.9800000190734863, avg_cost: 0.028231333979359383\n",
            "**************************************************\n",
            "test_acc: 0.9219000041484833\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 7.079495737950007e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.007241880834432477\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.017373104087552438\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.07407380055854447\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.26250799776583644\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.32146264131088764\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.9700000286102295, avg_cost: 0.34256652719443054\n",
            "**************************************************\n",
            "test_acc: 0.9157000035047531\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.0001595854883392652\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009637819903922111\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.015907701307284392\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.020560867570942113\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.02787586698502613\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03242815535879345\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9900000095367432, avg_cost: 0.036325049177733784\n",
            "**************************************************\n",
            "test_acc: 0.9253000020980835\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 1.8463127004603546e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.001275105502542525\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0032773493666597158\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.005305632117588171\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.008043652274200213\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.011082961420540115\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.9900000095367432, avg_cost: 0.01494272288276685\n",
            "**************************************************\n",
            "test_acc: 0.924799998998642\n",
            "batch: 0, train_acc: 1.0, avg_cost: 8.87170433998108e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002661717642128375\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0062473443199905845\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011014164962807618\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.016951700181255844\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.03851967158690665\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.9900000095367432, avg_cost: 0.05101451007049945\n",
            "**************************************************\n",
            "test_acc: 0.9202000027894974\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 6.682669123013814e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.008536867604270812\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.015376318891988682\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.021846519851436222\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.030195358848820138\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03531629606348967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZKhqlP5OqS",
        "colab_type": "text"
      },
      "source": [
        "# Restore : ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqE1cPjud7OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0813_BN_50epoch_mod_v2.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 128, keep_prob, '1b', batch_prob)\n",
        "h1_1 = tf.layers.batch_normalization(h1_1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,512]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList11 = [512,512,512]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [512,512,512]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [512,512,1024]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "\n",
        "h15 = tf.nn.avg_pool(h14, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE14kAK65TNq",
        "colab_type": "code",
        "outputId": "a9698fba-7754-4dbc-9fd0-02aae68d71c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0813 12:14:34.407257 139731626665856 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.3594039355715115e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.006564120243225869\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.012275049300709122\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.019494537977831596\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.02573109921310486\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.03473658621708943\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.9800000190734863, avg_cost: 0.044063390704589735\n",
            "**************************************************\n",
            "test_acc: 0.9285000020265579\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.823817069331805e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.005744013083555426\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011082608808840941\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.01736994489086405\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.024986772895305577\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.032934001492976685\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.9900000095367432, avg_cost: 0.042958695234362194\n",
            "**************************************************\n",
            "test_acc: 0.9063000023365021\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.6871742233633996e-05\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.009301771179113226\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.01963265298196347\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.03311022651721334\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.04849819776592389\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.07322711710187528\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.8999999761581421, avg_cost: 0.1022818606392441\n",
            "**************************************************\n",
            "test_acc: 0.888400001525879\n",
            "batch: 0, train_acc: 0.8799999952316284, avg_cost: 0.0008009669681390126\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.07290826941064246\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.10479098512791094\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.12391609558408771\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.13559337027797794\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.14485219582992906\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.9700000286102295, avg_cost: 0.1554936141998041\n",
            "**************************************************\n",
            "test_acc: 0.9251000010967254\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 6.546299904584885e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003572279870665322\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.006773597080367229\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.010047866893534476\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013520698726885418\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.01877083247042416\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 1.0, avg_cost: 0.0239306476685063\n",
            "**************************************************\n",
            "test_acc: 0.9268999999761581\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 6.084563210606575e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003541863833233945\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00642705894608904\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011030793807197667\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.018037725497658048\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02681458497177421\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9700000286102295, avg_cost: 0.04008837388314227\n",
            "**************************************************\n",
            "test_acc: 0.9163000029325485\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00024123397966225942\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.010113796817652959\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.02385720829755884\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.040080535101830427\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.10116423005747495\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.2754991995035379\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.949999988079071, avg_cost: 0.32677469012215926\n",
            "**************************************************\n",
            "test_acc: 0.9114000004529953\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.7381715675195058e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.014670012573090696\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.02598332097419188\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.034411198488038856\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.041169083964923636\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.047330746420533004\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9900000095367432, avg_cost: 0.05357477208342364\n",
            "**************************************************\n",
            "test_acc: 0.9293000042438507\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.828001596654455e-06\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002158453614926354\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.004214588175964311\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.006292523965836764\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.007905882010700234\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.009822736653856448\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9800000190734863, avg_cost: 0.012673181735447864\n",
            "**************************************************\n",
            "test_acc: 0.9284999984502792\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00013486846039692562\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0024246897946371363\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006903287647576993\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010522123988624416\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.014783638655468771\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.019854355961403287\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 1.0, avg_cost: 0.025208527238652682\n",
            "**************************************************\n",
            "test_acc: 0.9258999979496002\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.9685371555387975e-06\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.004178943103312728\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.008342419077077159\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.013140167565373593\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.01882487662421528\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02974531633409545\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9300000071525574, avg_cost: 0.04943937079223057\n",
            "**************************************************\n",
            "test_acc: 0.9158000022172927\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 6.006135915716489e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.02952331605852427\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.08230097944246756\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.11851425370453697\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.13404574621051624\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.14657087967158688\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.9900000095367432, avg_cost: 0.16033816654535868\n",
            "**************************************************\n",
            "test_acc: 0.9218000024557114\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 3.795489358405272e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.00367254042923984\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006739506506928591\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010794291667383117\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.015790533713079632\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.019042641949975705\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 1.0, avg_cost: 0.02478999066035613\n",
            "**************************************************\n",
            "test_acc: 0.926100001335144\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.399102569247286e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0032612523135321664\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.006520179442431978\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.009414654609129979\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.014250522667850103\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.01905545625150503\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.9900000095367432, avg_cost: 0.024330404868378515\n",
            "**************************************************\n",
            "test_acc: 0.9235000020265579\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 7.952453568577767e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.005941945199059166\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.012339813045897853\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.02681704682114892\n",
            "batch: 400, train_acc: 0.8799999952316284, avg_cost: 0.11999923339570764\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.17076507754576595\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.9900000095367432, avg_cost: 0.19642835714224935\n",
            "**************************************************\n",
            "test_acc: 0.9206000018119812\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.804900373021761e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.005679114826537746\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.013353063277008907\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.02045969450294553\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.024487718061300245\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.029558171550897887\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 1.0, avg_cost: 0.03533540137167617\n",
            "**************************************************\n",
            "test_acc: 0.9288999992609024\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.774680369844039e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003151125311933356\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.005153787123032694\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.008155856053591986\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01038164393623144\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01349225705150579\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 1.0, avg_cost: 0.016540731801960654\n",
            "**************************************************\n",
            "test_acc: 0.9281000036001206\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.7869051943222683e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0020487252445521623\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.004780537748677788\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.006615368177432774\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.009521557763106705\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.022285078862214498\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9700000286102295, avg_cost: 0.06483225764830723\n",
            "**************************************************\n",
            "test_acc: 0.9121000009775162\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00034848277767499287\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.07208848959223059\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.11633075414380688\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.13891027445672094\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.1523561339128356\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.16240635580377183\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9700000286102295, avg_cost: 0.17003672968817227\n",
            "**************************************************\n",
            "test_acc: 0.9278000009059906\n",
            "batch: 0, train_acc: 1.0, avg_cost: 6.3152448274195195e-06\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.004404761909433242\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0070418215851646\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.009416125566566886\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.012200262609248967\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.015060916691198445\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 1.0, avg_cost: 0.01755761008525798\n",
            "**************************************************\n",
            "test_acc: 0.9303000026941299\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.8208883789678414e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002536385343938624\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.005967858285616309\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.008408194382715142\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.011823189170782962\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.01644499002340231\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 1.0, avg_cost: 0.02066225486629672\n",
            "**************************************************\n",
            "test_acc: 0.9272000020742417\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.7649365064377586e-07\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.004559773933360703\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00940944370230075\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.014986092613492775\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.02000371992693469\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.029877594222258122\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.9800000190734863, avg_cost: 0.038233600509956125\n",
            "**************************************************\n",
            "test_acc: 0.921599999666214\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.4043340235948563e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.0074882302248321765\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.016390275315795105\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.025848615219377587\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03393376217163075\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.04177679881043637\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9900000095367432, avg_cost: 0.051324576336652813\n",
            "**************************************************\n",
            "test_acc: 0.922800001502037\n",
            "batch: 0, train_acc: 1.0, avg_cost: 8.118811529129743e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.010388023178420554\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.022125712751925073\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.04776067683740015\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.14565605413125018\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.23335238006446157\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9900000095367432, avg_cost: 0.26854828939981695\n",
            "**************************************************\n",
            "test_acc: 0.9228000020980835\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.0003077072401841482\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.009142035176525188\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.015014281537028178\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.02256338239094324\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.02861430935987149\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03281003644206184\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.9800000190734863, avg_cost: 0.03744375750965869\n",
            "**************************************************\n",
            "test_acc: 0.9275000017881393\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.470999697844188e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.001540119322537521\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0032633122230472135\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.005906443958466714\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.009335183553009566\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.012887766523411708\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 1.0, avg_cost: 0.017614270821842894\n",
            "**************************************************\n",
            "test_acc: 0.9305000007152557\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.048026451220115e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0032844190273280563\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006822421031943216\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010200580012423662\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013874385668255046\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01700098858577434\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9900000095367432, avg_cost: 0.02296669897090898\n",
            "**************************************************\n",
            "test_acc: 0.925699999332428\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.3669766960665583e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.007391537674981618\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.013638489542281609\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.021665635093529534\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.029129579534662747\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.03787454750427666\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9900000095367432, avg_cost: 0.05256202662709108\n",
            "**************************************************\n",
            "test_acc: 0.9178000044822693\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00017626597235600154\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.024888125042953107\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.05137532511407397\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.06683997429036026\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.08300773279538891\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.0930468143261892\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.9900000095367432, avg_cost: 0.10595292783353771\n",
            "**************************************************\n",
            "test_acc: 0.9193000042438507\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00015058829138676324\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005853217934561221\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.013167217787323389\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.01853945508227923\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.027663739127842556\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.04085486032798958\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9900000095367432, avg_cost: 0.051708773204043344\n",
            "**************************************************\n",
            "test_acc: 0.9254000025987625\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00011142197996377945\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.006502009474850372\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.011338335603387569\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.015871482168965556\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.021153453987163003\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02600774618945735\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9599999785423279, avg_cost: 0.03338157553635156\n",
            "**************************************************\n",
            "test_acc: 0.9229000025987625\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.0034153092419728e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005582980136636783\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011737511826302506\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.01649020084589836\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.023883414671147053\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.036874429446276814\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.9900000095367432, avg_cost: 0.054518026489865964\n",
            "**************************************************\n",
            "test_acc: 0.9192000019550324\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.1160445262988406e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.04378177565363086\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.08293276209166596\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.10831007743310765\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.12190246204174245\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.13244830827828333\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 1.0, avg_cost: 0.1380353370169688\n",
            "**************************************************\n",
            "test_acc: 0.9287000036239624\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.5519239696053165e-07\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0031139721585096447\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.007047544942069046\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.009807629038145466\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.014008711606476355\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.017099219239470262\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 1.0, avg_cost: 0.019360090110148345\n",
            "**************************************************\n",
            "test_acc: 0.9282000011205673\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.5238136559977042e-08\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.003216431080113343\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00576581572061152\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.009414610137960341\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013147524002359306\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.019330644366850128\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.9800000190734863, avg_cost: 0.02609624862362964\n",
            "**************************************************\n",
            "test_acc: 0.92280000269413\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.2550186258740724e-07\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.008344629171261792\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.015919660261846037\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.023659097202041166\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.031013727989751293\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.04107663463187929\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9700000286102295, avg_cost: 0.05238254422406169\n",
            "**************************************************\n",
            "test_acc: 0.9230000042915344\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.725895511607329e-07\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.007660744165954819\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.01838174868811195\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.03429545233445871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b9c45522e797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3HV7puWnmMA",
        "colab_type": "text"
      },
      "source": [
        "# ResNet : Restore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byaf3IFL5qq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0813_BN_50epoch_mod_v2.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_150epoch_best_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_150epoch_best_loss.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 128, keep_prob, '1b', batch_prob)\n",
        "h1_1 = tf.layers.batch_normalization(h1_1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,512]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList11 = [512,512,512]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [512,512,512]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [512,512,1024]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "\n",
        "h15 = tf.nn.avg_pool(h14, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTV8_nFYofmm",
        "colab_type": "code",
        "outputId": "a2532fc9-1d90-4bea-b9ea-28fa1383bc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 1.0, avg_cost: 6.078304334854086e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002586844466014214\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.005905651690203326\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011454919537374627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn2pAM3sc5fj",
        "colab_type": "text"
      },
      "source": [
        "# ResNet : for 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GjmVl0Cqt3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_loss.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 128, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,256]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "h12 = tf.nn.avg_pool(h11, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h12_flat = tf.reshape(h12, [-1,256])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h12_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVxHn5c9dlHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "157a05c6-715f-4328-b0c1-900965bdc3e2"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.09000000357627869, avg_cost: 0.0061059157053629555\n",
            "batch: 100, train_acc: 0.6899999976158142, avg_cost: 0.8036426612734793\n",
            "batch: 200, train_acc: 0.7900000214576721, avg_cost: 0.9371734351913129\n",
            "batch: 300, train_acc: 0.8199999928474426, avg_cost: 1.0549825007716815\n",
            "batch: 400, train_acc: 0.8100000023841858, avg_cost: 1.167238837381204\n",
            "batch: 500, train_acc: 0.7900000214576721, avg_cost: 1.273420827388764\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.8500000238418579, avg_cost: 1.3607737915714595\n",
            "**************************************************\n",
            "test_acc: 0.8178999990224838\n",
            "batch: 0, train_acc: 0.8399999737739563, avg_cost: 0.0007805954416592916\n",
            "batch: 100, train_acc: 0.8600000143051147, avg_cost: 0.07833048932254313\n",
            "batch: 200, train_acc: 0.8500000238418579, avg_cost: 0.15970885174969832\n",
            "batch: 300, train_acc: 0.8899999856948853, avg_cost: 0.23530263818800445\n",
            "batch: 400, train_acc: 0.7599999904632568, avg_cost: 0.30812071991463497\n",
            "batch: 500, train_acc: 0.8600000143051147, avg_cost: 0.3760294423749049\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.8799999952316284, avg_cost: 0.44090523242950463\n",
            "**************************************************\n",
            "test_acc: 0.8786999994516372\n",
            "batch: 0, train_acc: 0.8999999761581421, avg_cost: 0.00041339593629042306\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.05921478110055129\n",
            "batch: 200, train_acc: 0.8799999952316284, avg_cost: 0.12253178641200067\n",
            "batch: 300, train_acc: 0.8399999737739563, avg_cost: 0.19056601288417976\n",
            "batch: 400, train_acc: 0.8899999856948853, avg_cost: 0.2481014196077984\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.3085251950720949\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.9200000166893005, avg_cost: 0.3643297523756825\n",
            "**************************************************\n",
            "test_acc: 0.8818000000715256\n",
            "batch: 0, train_acc: 0.8999999761581421, avg_cost: 0.0004161204397678375\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.059946963712573076\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.11221135169267657\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.16263384251544885\n",
            "batch: 400, train_acc: 0.8799999952316284, avg_cost: 0.20929261601219587\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.2558564002936085\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.8799999952316284, avg_cost: 0.30300993002951143\n",
            "**************************************************\n",
            "test_acc: 0.8738000011444091\n",
            "batch: 0, train_acc: 0.8899999856948853, avg_cost: 0.0004336616396903992\n",
            "batch: 100, train_acc: 0.8399999737739563, avg_cost: 0.04829378165304661\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.09160617041091122\n",
            "batch: 300, train_acc: 0.8899999856948853, avg_cost: 0.13858381313582258\n",
            "batch: 400, train_acc: 0.9100000262260437, avg_cost: 0.18556706479440127\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.23409495080510775\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.9399999976158142, avg_cost: 0.27631424972166635\n",
            "**************************************************\n",
            "test_acc: 0.8862000000476837\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.00046072378754615786\n",
            "batch: 100, train_acc: 0.8700000047683716, avg_cost: 0.04441692246745032\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.08613477176676197\n",
            "batch: 300, train_acc: 0.8700000047683716, avg_cost: 0.12803601493438085\n",
            "batch: 400, train_acc: 0.8799999952316284, avg_cost: 0.17026156891758237\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.21005107875292495\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9300000071525574, avg_cost: 0.2505513610504566\n",
            "**************************************************\n",
            "test_acc: 0.8951000022888184\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.0003868178029855092\n",
            "batch: 100, train_acc: 0.8500000238418579, avg_cost: 0.04009774460146825\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.07720169724275666\n",
            "batch: 300, train_acc: 0.8399999737739563, avg_cost: 0.11464580873648324\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.15182072491695495\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.19047157338509974\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.9100000262260437, avg_cost: 0.2310065942754351\n",
            "**************************************************\n",
            "test_acc: 0.8952999991178513\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0003312319020430247\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.034007853704194235\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.07070987080534302\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.10496628159036245\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.14201503614584615\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.1789896167938909\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9599999785423279, avg_cost: 0.2162907440898321\n",
            "**************************************************\n",
            "test_acc: 0.9134000009298324\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.0002035601809620857\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.03400685798376798\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.06810706680019693\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.10188560626159104\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.13538237990811466\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.16766948838407794\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9300000071525574, avg_cost: 0.20171198314055813\n",
            "**************************************************\n",
            "test_acc: 0.9107000011205674\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.0004977899293104808\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.030518981628119942\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.05967010964949928\n",
            "batch: 300, train_acc: 0.9100000262260437, avg_cost: 0.09037672484293584\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.1230943761641781\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.1555408851553996\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.8899999856948853, avg_cost: 0.1885084178981681\n",
            "**************************************************\n",
            "test_acc: 0.9073000025749206\n",
            "[0, 0.8178999990224838, 0.8786999994516372, 0.8818000000715256, 0.8862000000476837, 0.8951000022888184, 0.8952999991178513, 0.9134000009298324]\n",
            "[100, 1.3607737915714595, 0.44090523242950463, 0.3643297523756825, 0.30300993002951143, 0.27631424972166635, 0.2505513610504566, 0.2310065942754351, 0.2162907440898321, 0.20171198314055813, 0.1885084178981681]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfGU1GibofRr",
        "colab_type": "text"
      },
      "source": [
        "# Restore : 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdrAi8fjofD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "1. 맨 처음 conv layer를 64 / 128 => 128 단층으로 전환\n",
        "2. #filter를 기존의 128-256-512-1024 => 128-256-256으로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 40\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 128, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,256]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "h12 = tf.nn.avg_pool(h11, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h12_flat = tf.reshape(h12, [-1,256])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h12_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1XigIAThv-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8f76b0c-ba3f-4727-d302-4bc6327e42a6"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0814 00:58:49.596909 140036658886528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00037185393273830413\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.03114394760380188\n",
            "batch: 200, train_acc: 0.9100000262260437, avg_cost: 0.06337655426313481\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.09797682924816999\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.12937414211531464\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.1645563122754294\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.9200000166893005, avg_cost: 0.19895237430309234\n",
            "**************************************************\n",
            "test_acc: 0.8990999984741211\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0003798498461643855\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.033400914122660955\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.0648000725731254\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.09450658099725856\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.12830489840979398\n",
            "batch: 500, train_acc: 0.8500000238418579, avg_cost: 0.16129126903290572\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.949999988079071, avg_cost: 0.1948283650539817\n",
            "**************************************************\n",
            "test_acc: 0.9101999998092651\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0002473362535238266\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.027727616503834717\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.056803386819859365\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.08694472375015425\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.11636336038510015\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.1453181072945396\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.9300000071525574, avg_cost: 0.17559393061945827\n",
            "**************************************************\n",
            "test_acc: 0.9050999993085861\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0002518148223559062\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.0267825593923529\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.05711550727486609\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.08479675879081085\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.11184278137360998\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.1439722859983642\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.9900000095367432, avg_cost: 0.172177161971728\n",
            "**************************************************\n",
            "test_acc: 0.9131000012159347\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.0003195269157489141\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.024825321373840175\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.04896988676860929\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.07448492910712956\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.09983461180701854\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.12251861600826185\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.9700000286102295, avg_cost: 0.14865885332226764\n",
            "**************************************************\n",
            "test_acc: 0.9060000014305115\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.0002536096672217051\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.025411822696526854\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.04548717812635008\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.06777097576297819\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.09190936721240477\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.11581679444139195\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9399999976158142, avg_cost: 0.1432239948275188\n",
            "**************************************************\n",
            "test_acc: 0.9132000011205673\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.0001615849509835243\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.02241822366913159\n",
            "batch: 200, train_acc: 0.8899999856948853, avg_cost: 0.0444889405121406\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.066088316620638\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.08883931529087331\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.1099191463335108\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.949999988079071, avg_cost: 0.1296765967148046\n",
            "**************************************************\n",
            "test_acc: 0.9006000006198883\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0002506641298532486\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.01781189834854255\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.0384835588792339\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.06200956746780625\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.08069850179521991\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.09974033510157226\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9800000190734863, avg_cost: 0.12184586208468934\n",
            "**************************************************\n",
            "test_acc: 0.9122000020742417\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0002141923705736796\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.020209706605722504\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.03657215018135804\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.05342371374523882\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.07116893001055961\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.09271364919065182\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9599999785423279, avg_cost: 0.11257463848683982\n",
            "**************************************************\n",
            "test_acc: 0.9210000038146973\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00022663543621699014\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.013334867522741354\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.026873806489941976\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.04300935886334627\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.05882670927637565\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.07506332691293202\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.9800000190734863, avg_cost: 0.09447408926828443\n",
            "**************************************************\n",
            "test_acc: 0.9056000012159348\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00030613765120506286\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.012798023610375816\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02566021588786195\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.038871857655855525\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.05223622954450545\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.06934106778819116\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9900000095367432, avg_cost: 0.08442050579624877\n",
            "**************************************************\n",
            "test_acc: 0.9241000056266785\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.6973652119437856e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009787665017647667\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02292725920250328\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.03649663619115016\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.049270835807935016\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.0622659498457021\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.9700000286102295, avg_cost: 0.07452784674280946\n",
            "**************************************************\n",
            "test_acc: 0.9189000010490418\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0003137086828549703\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009982517894047001\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.01957024072762581\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.02986503667353345\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.040454247498419194\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.05359881809912628\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.949999988079071, avg_cost: 0.0685985420644284\n",
            "**************************************************\n",
            "test_acc: 0.9195000010728837\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.4505549420913062e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009444721427280455\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.020978957413074872\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.03280340201687068\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.041688309701470055\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.05237879728355135\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 1.0, avg_cost: 0.06382791464449837\n",
            "**************************************************\n",
            "test_acc: 0.9147000014781952\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00013480714211861294\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009722760558361186\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.02054093095667971\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.03139990717677091\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.04182482226247281\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.052657122599193834\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 1.0, avg_cost: 0.06103638757214259\n",
            "**************************************************\n",
            "test_acc: 0.9195000022649765\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 5.903639520208041e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.008144800061127168\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.014161354335956278\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.023801030168930693\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03326204581962275\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04112456756682753\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.9800000190734863, avg_cost: 0.050056872781133306\n",
            "**************************************************\n",
            "test_acc: 0.911000002026558\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 9.70412107805411e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.005886487210518678\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.0130409144066895\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.024866029412563252\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03321827561187089\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.04548115143921066\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.9900000095367432, avg_cost: 0.054242292123963046\n",
            "**************************************************\n",
            "test_acc: 0.9205000030994416\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.2387423863013584e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.003341312218593278\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008492677914327939\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.013371605856130666\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.021087086367673074\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.03141422889379706\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9599999785423279, avg_cost: 0.040617524694049854\n",
            "**************************************************\n",
            "test_acc: 0.916900001168251\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 8.66919383406639e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.006288593694334847\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.01055829386576079\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0155649559321076\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.023334062086360068\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.03052425411772371\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9900000095367432, avg_cost: 0.0366508000488587\n",
            "**************************************************\n",
            "test_acc: 0.9134000039100647\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.703464642167092e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.006411114054183901\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.01379466981896257\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.024757560581759507\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.035240164657564785\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.043473569405396095\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.9900000095367432, avg_cost: 0.050011937131227244\n",
            "**************************************************\n",
            "test_acc: 0.92230000436306\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.5613818541169166e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0037823412736179307\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.00706064496247563\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.01077074268182816\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01707823319059873\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02618941040952145\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.9800000190734863, avg_cost: 0.031996321470748317\n",
            "**************************************************\n",
            "test_acc: 0.9225000041723251\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.413286879658699e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.005058062033494937\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.01116509195135828\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.018141992177988862\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.022231779459931825\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02817978397142724\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 1.0, avg_cost: 0.03583866359263385\n",
            "**************************************************\n",
            "test_acc: 0.9217000025510788\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.7824795494476955e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0033821088578163965\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.007350991226461101\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.014094025583181066\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.019799493693183963\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02516424439677698\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9800000190734863, avg_cost: 0.03218921877642667\n",
            "**************************************************\n",
            "test_acc: 0.9205000030994416\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 1.8826206214725972e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.004569937115205293\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.008139216975105227\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.014548671333808065\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.02106774456347921\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.027777842989841393\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9900000095367432, avg_cost: 0.0443418674143322\n",
            "**************************************************\n",
            "test_acc: 0.9139000016450882\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.102463833987713e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.006040550111307916\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.010068289296056413\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.015784439051033886\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.02183925332077101\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.026102074947290634\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 1.0, avg_cost: 0.029774599306183525\n",
            "**************************************************\n",
            "test_acc: 0.9235000014305115\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.87525069527328e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002299029073086179\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005496134837837115\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.012440246882227564\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01871683581790422\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.022742807783458083\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 1.0, avg_cost: 0.02668254535344509\n",
            "**************************************************\n",
            "test_acc: 0.9180000019073487\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.0237316687901813e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.006012799344704643\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.012427345504984262\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.018004962344324058\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.021973301907940657\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.026105431941869\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9800000190734863, avg_cost: 0.03039530560382745\n",
            "**************************************************\n",
            "test_acc: 0.9218000018596649\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.6147213367124397e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0037295404364219085\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.006539452376249154\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010808252760070898\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.016366764990416414\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.021747807466830034\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9900000095367432, avg_cost: 0.02841674775732826\n",
            "**************************************************\n",
            "test_acc: 0.9188000011444092\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.924987278878689e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0038408209191402413\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006445716001398974\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.010873969207556606\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01610207851949477\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.022532932772034362\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 1.0, avg_cost: 0.0270496890042705\n",
            "**************************************************\n",
            "test_acc: 0.9195000016689301\n",
            "batch: 0, train_acc: 1.0, avg_cost: 6.643285450991243e-07\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.003649950792435752\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.00746852941687394\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.010429121727108102\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013706396076055793\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02048825212873759\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9900000095367432, avg_cost: 0.026729745805817738\n",
            "**************************************************\n",
            "test_acc: 0.9179000008106232\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 9.726357956727345e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003720907267694807\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008932924635143228\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.012257420750890253\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.014771659228887669\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.01820806561399876\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9700000286102295, avg_cost: 0.025118603507368473\n",
            "**************************************************\n",
            "test_acc: 0.9203000050783158\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00010006737584869067\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.0039942739190397935\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.007029663390154985\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010644330681219187\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.015018636889872134\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02009611168915094\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 1.0, avg_cost: 0.023763127301450956\n",
            "**************************************************\n",
            "test_acc: 0.9210000032186508\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.524591214954853e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.0018682585217114443\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005622206830867071\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010741321652149055\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01765767312452834\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.023419295969112387\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.9800000190734863, avg_cost: 0.02947107032608984\n",
            "**************************************************\n",
            "test_acc: 0.9090000015497207\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00010050339624285698\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0034928956958234863\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.007987013342093027\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011863727641872776\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.015443263013178993\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01939169648256817\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.9800000190734863, avg_cost: 0.024483288950899188\n",
            "**************************************************\n",
            "test_acc: 0.921000002026558\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00014546025544404984\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.004005511279525915\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.006962503103786731\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.009833529490697403\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.013003823984542277\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.015561816758330073\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.9800000190734863, avg_cost: 0.02164134162925317\n",
            "**************************************************\n",
            "test_acc: 0.9216000002622604\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 6.759518757462501e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0022257782614481885\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.004272213070944417\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.0058748929590365095\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.010035770698365615\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.013782465173244426\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9800000190734863, avg_cost: 0.021138029141208478\n",
            "**************************************************\n",
            "test_acc: 0.9204000002145767\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 4.27473708987236e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0050130389775343565\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.00995793058740674\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.014373630759376588\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.017290080378600896\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.020431512704635692\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.9900000095367432, avg_cost: 0.02465695804390636\n",
            "**************************************************\n",
            "test_acc: 0.9242000007629394\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.2409892020126182e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0018512781922011812\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.0030625984200772375\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.004601660643308299\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.006386235096921758\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.009835074393054505\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.9900000095367432, avg_cost: 0.014266575661657723\n",
            "**************************************************\n",
            "test_acc: 0.9206000018119812\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.1963191082080205e-06\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.015021633373798983\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.022842086153796116\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.02917824988526263\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.032499098972560495\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03508609147402541\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 1.0, avg_cost: 0.038076024836770195\n",
            "**************************************************\n",
            "test_acc: 0.9268000030517578\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.952146507799625e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0022137547269479307\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.004094923937388256\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.006884022437091817\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.009426581876490063\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.012123142053675713\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.9900000095367432, avg_cost: 0.015567500140274809\n",
            "**************************************************\n",
            "test_acc: 0.9220000028610229\n",
            "[0, 0.8990999984741211, 0.9101999998092651, 0.9131000012159347, 0.9132000011205673, 0.9210000038146973, 0.9241000056266785, 0.9242000007629394, 0.9268000030517578]\n",
            "[100, 0.19895237430309234, 0.1948283650539817, 0.17559393061945827, 0.172177161971728, 0.14865885332226764, 0.1432239948275188, 0.1296765967148046, 0.12184586208468934, 0.11257463848683982, 0.09447408926828443, 0.08442050579624877, 0.07452784674280946, 0.0685985420644284, 0.06382791464449837, 0.06103638757214259, 0.050056872781133306, 0.040617524694049854, 0.0366508000488587, 0.031996321470748317, 0.029774599306183525, 0.02668254535344509, 0.025118603507368473, 0.023763127301450956, 0.02164134162925317, 0.021138029141208478, 0.014266575661657723]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLw33_uio35D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Czy0KjNDTc2",
        "colab_type": "text"
      },
      "source": [
        "# ResNet : for 10 classes : mod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39-pjlnnDn_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "1. 맨 처음 conv layer를 64 / 128 => 128 단층으로 전환\n",
        "2. #filter를 기존의 128-256-512-1024 => 128-256-256-256으로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_long_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_long_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_long_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 128, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,256]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "iList11 = [256,256,256]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [256,256,256]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [256,256,256]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "iList14 = [256,256,256]\n",
        "h15 = my_resNet_model(h14, keep_prob, batch_prob, iList14, 15)\n",
        "h15 = tf.nn.avg_pool(h15, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,256])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxs9FySRDqwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bbb57f3-28bd-429b-897e-c38e8ff71ddb"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.10999999940395355, avg_cost: 0.015770867665608725\n",
            "batch: 100, train_acc: 0.30000001192092896, avg_cost: 1.638140858610471\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 1.9410578219095875\n",
            "batch: 300, train_acc: 0.4099999964237213, avg_cost: 2.2612243402004237\n",
            "batch: 400, train_acc: 0.44999998807907104, avg_cost: 2.5306990500291184\n",
            "batch: 500, train_acc: 0.4300000071525574, avg_cost: 2.768920793135959\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.46000000834465027, avg_cost: 3.017639006177583\n",
            "**************************************************\n",
            "test_acc: 0.7518999975919723\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0021158464749654136\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.24698745071887968\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.4783831691741942\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.69207024137179\n",
            "batch: 400, train_acc: 0.41999998688697815, avg_cost: 0.9414771817127862\n",
            "batch: 500, train_acc: 0.5, avg_cost: 1.15351056009531\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.4699999988079071, avg_cost: 1.404901708662509\n",
            "**************************************************\n",
            "test_acc: 0.8286999988555909\n",
            "batch: 0, train_acc: 0.46000000834465027, avg_cost: 0.002402584155400594\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.23076161702473955\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.4357388820250825\n",
            "batch: 300, train_acc: 0.41999998688697815, avg_cost: 0.6554819491505616\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.8620366423328706\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 1.0609654687841727\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.5099999904632568, avg_cost: 1.251403709053993\n",
            "**************************************************\n",
            "test_acc: 0.8551000022888183\n",
            "batch: 0, train_acc: 0.47999998927116394, avg_cost: 0.002077165643374125\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.202728397945563\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.4072437344988189\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.6000296446681023\n",
            "batch: 400, train_acc: 0.49000000953674316, avg_cost: 0.8050845730304714\n",
            "batch: 500, train_acc: 0.4300000071525574, avg_cost: 1.031319682300091\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5899999737739563, avg_cost: 1.2207689012090364\n",
            "**************************************************\n",
            "test_acc: 0.8815000033378602\n",
            "batch: 0, train_acc: 0.6800000071525574, avg_cost: 0.0014529165625572205\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.18916065076986951\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.37689770708481496\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.5687159443895025\n",
            "batch: 400, train_acc: 0.4399999976158142, avg_cost: 0.7572735701004669\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.9546877368291223\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.5099999904632568, avg_cost: 1.1419776825110113\n",
            "**************************************************\n",
            "test_acc: 0.8538000017404557\n",
            "batch: 0, train_acc: 0.5, avg_cost: 0.0018240092198053995\n",
            "batch: 100, train_acc: 0.49000000953674316, avg_cost: 0.19338835110267003\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.39526946494976684\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.6126144988338149\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.8173348583777736\n",
            "batch: 500, train_acc: 0.4699999988079071, avg_cost: 1.003794995148976\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.49000000953674316, avg_cost: 1.1856417795022327\n",
            "**************************************************\n",
            "test_acc: 0.8784999984502793\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0018391335010528565\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.19995595653851833\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.38848132739464486\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.5850753339131677\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.7862862166762359\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.9727709584434835\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5199999809265137, avg_cost: 1.1550508777300525\n",
            "**************************************************\n",
            "test_acc: 0.8806999999284745\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0020148664712905883\n",
            "batch: 100, train_acc: 0.4699999988079071, avg_cost: 0.1781777066985767\n",
            "batch: 200, train_acc: 0.41999998688697815, avg_cost: 0.35373177597920114\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5325596374273301\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.7107865380247436\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.885042753418287\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.5199999809265137, avg_cost: 1.0618281445900604\n",
            "**************************************************\n",
            "test_acc: 0.8999000012874603\n",
            "batch: 0, train_acc: 0.5, avg_cost: 0.0020064441363016766\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.18228699554999672\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.36420582920312866\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.5515713715553279\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.7612267033259067\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.9583191806077951\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.5699999928474426, avg_cost: 1.1469994340340284\n",
            "**************************************************\n",
            "test_acc: 0.8893000012636185\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0018856783707936605\n",
            "batch: 100, train_acc: 0.4399999976158142, avg_cost: 0.1806956070661545\n",
            "batch: 200, train_acc: 0.5, avg_cost: 0.36096897919972765\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.5399553512533509\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.7198932337760924\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.9006303190191586\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.5600000023841858, avg_cost: 1.087130457957585\n",
            "**************************************************\n",
            "test_acc: 0.9033000004291535\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0015472333629926046\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.17633515179157253\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.3509639779726663\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.5249448125561076\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6969511451323825\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.869642874499162\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.4699999988079071, avg_cost: 1.0438530356685316\n",
            "**************************************************\n",
            "test_acc: 0.8941000008583069\n",
            "batch: 0, train_acc: 0.44999998807907104, avg_cost: 0.0020092493295669556\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1738398013512293\n",
            "batch: 200, train_acc: 0.46000000834465027, avg_cost: 0.3460434289773305\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.5201209130883216\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6960791734854377\n",
            "batch: 500, train_acc: 0.49000000953674316, avg_cost: 0.8784980673591293\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.5699999928474426, avg_cost: 1.0864502201477677\n",
            "**************************************************\n",
            "test_acc: 0.8944999986886978\n",
            "batch: 0, train_acc: 0.4399999976158142, avg_cost: 0.002089980443318685\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.18929869790871937\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.36266854216655103\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.5398243293166162\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.713421855668227\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8831020758549373\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.4300000071525574, avg_cost: 1.049985561966896\n",
            "**************************************************\n",
            "test_acc: 0.9116000020503998\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0015373054146766664\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.17147190193335204\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.34001409898201623\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.5088265239199002\n",
            "batch: 400, train_acc: 0.49000000953674316, avg_cost: 0.6748083010315893\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.8441986264785122\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.5, avg_cost: 1.0480964427193\n",
            "**************************************************\n",
            "test_acc: 0.8810999965667725\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0015910937388737996\n",
            "batch: 100, train_acc: 0.49000000953674316, avg_cost: 0.17393034617106126\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.3529610632856688\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.5280270239710807\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.7010278911391895\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8700968204935389\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.6100000143051147, avg_cost: 1.0409303674101826\n",
            "**************************************************\n",
            "test_acc: 0.9181000012159347\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.001632967491944631\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.16787515660127003\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.3378973878423374\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.5074264342586201\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.6749557629227637\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.8442967733740802\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.6000000238418579, avg_cost: 1.01107532997926\n",
            "**************************************************\n",
            "test_acc: 0.9177000021934509\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0017692583799362182\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.16481999546289436\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.33014356434345243\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.49455878347158433\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.658366514742374\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8233184420069061\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.5400000214576721, avg_cost: 0.9863828338185947\n",
            "**************************************************\n",
            "test_acc: 0.9130000001192093\n",
            "batch: 0, train_acc: 0.5, avg_cost: 0.00181938370068868\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.1644966939091682\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.32635386000076927\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4923362328608831\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6579758222897844\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.8269214515884714\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.550000011920929, avg_cost: 0.9978623531262072\n",
            "**************************************************\n",
            "test_acc: 0.9047000032663345\n",
            "batch: 0, train_acc: 0.4300000071525574, avg_cost: 0.002065300941467285\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.16667006999254225\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.33026259839534755\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4978649700681367\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.6631257752577469\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.826678808232149\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.5799999833106995, avg_cost: 1.000395535131296\n",
            "**************************************************\n",
            "test_acc: 0.9031000000238418\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0017948816219965616\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.16612667977809897\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.3279706253608068\n",
            "batch: 300, train_acc: 0.6499999761581421, avg_cost: 0.4882186479369798\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6543370573719344\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.8158126272757849\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.5600000023841858, avg_cost: 0.977629222869873\n",
            "**************************************************\n",
            "test_acc: 0.9163000029325485\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.0014995211362838746\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.1626524677872657\n",
            "batch: 200, train_acc: 0.5, avg_cost: 0.3228376670678457\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4804851739605269\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.6394665254155789\n",
            "batch: 500, train_acc: 0.4699999988079071, avg_cost: 0.8020002002517379\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.6299999952316284, avg_cost: 0.9608956383665406\n",
            "**************************************************\n",
            "test_acc: 0.9129000025987625\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.00160061776638031\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1656827198465665\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.33054613153139745\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.4910135294993717\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6514801687995596\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.8125361628333729\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.6100000143051147, avg_cost: 0.968391115566095\n",
            "**************************************************\n",
            "test_acc: 0.916900002360344\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0016230495770772299\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.15782339940468465\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.31599796225627275\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.4746839722990992\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.6314535166819891\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7886273660262422\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.5799999833106995, avg_cost: 0.9439110730091733\n",
            "**************************************************\n",
            "test_acc: 0.9198000007867813\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0014027056097984313\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1544721985856692\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.3106605086723963\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.4659168907999993\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6233986434340477\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7811060761411986\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.5, avg_cost: 0.9335681736469273\n",
            "**************************************************\n",
            "test_acc: 0.916100001335144\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.001619341770807902\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.15368837902943286\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.30364415784676874\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4586776718497278\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.6137807325522106\n",
            "batch: 500, train_acc: 0.6899999976158142, avg_cost: 0.7691808048884072\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.6000000238418579, avg_cost: 0.9223622864484786\n",
            "**************************************************\n",
            "test_acc: 0.9214000004529953\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0014871355891227722\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.15452275574207305\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.3086997465292613\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.46159755359093335\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6154059460759167\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.7721577813227973\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.6200000047683716, avg_cost: 0.9267827627062799\n",
            "**************************************************\n",
            "test_acc: 0.9239000034332275\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0012282295028368632\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.15074952592452365\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.29966557105382274\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.45025955875714607\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6036099095145863\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.753790395458539\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.6600000262260437, avg_cost: 0.9062068222959828\n",
            "**************************************************\n",
            "test_acc: 0.913700001835823\n",
            "batch: 0, train_acc: 0.49000000953674316, avg_cost: 0.0018877939383188883\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.1558522060513496\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.30674700826406487\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.458607969880104\n",
            "batch: 400, train_acc: 0.6600000262260437, avg_cost: 0.6117982466022172\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.7667449500163399\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.5799999833106995, avg_cost: 0.9174059769511228\n",
            "**************************************************\n",
            "test_acc: 0.9171000021696091\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0016400579611460368\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.14986807604630795\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.3032125804821648\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4559126226107275\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.605569903651873\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.756097250978152\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.6600000262260437, avg_cost: 0.9084163015087452\n",
            "**************************************************\n",
            "test_acc: 0.9202000021934509\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.001520228683948517\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14944848964611693\n",
            "batch: 200, train_acc: 0.6899999976158142, avg_cost: 0.30029252618551244\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4508553517858186\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5964030406872428\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.746107180515924\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.5, avg_cost: 0.8948541701833401\n",
            "**************************************************\n",
            "test_acc: 0.9196000027656556\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014651512106259664\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14693583836158114\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.29522968858480453\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.44218973179658244\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.59313371181488\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7403628042340273\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.5299999713897705, avg_cost: 0.8860550942023595\n",
            "**************************************************\n",
            "test_acc: 0.9217000007629395\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0015935019652048748\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.14732343186934782\n",
            "batch: 200, train_acc: 0.49000000953674316, avg_cost: 0.29453530877828604\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.4446719249089558\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.594942417641481\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7443866803248727\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.5699999928474426, avg_cost: 0.8937611613670979\n",
            "**************************************************\n",
            "test_acc: 0.9187000024318696\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0015177741646766663\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1508337124188741\n",
            "batch: 200, train_acc: 0.6499999761581421, avg_cost: 0.29864288081725454\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.44531356424093266\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5901189726591107\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.737096958955129\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.5600000023841858, avg_cost: 0.8824167981743812\n",
            "**************************************************\n",
            "test_acc: 0.918900004029274\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016000460584958394\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14742211689551674\n",
            "batch: 200, train_acc: 0.6499999761581421, avg_cost: 0.293123881816864\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.43937651912371345\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.5893674914042158\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.7376684132218359\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.6000000238418579, avg_cost: 0.8835182647903753\n",
            "**************************************************\n",
            "test_acc: 0.9184000021219254\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0015831697980562846\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.14483190268278123\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.2932764618595439\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.44216893166303634\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5895425656437873\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7377474850416176\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.7099999785423279, avg_cost: 0.8782279849052419\n",
            "**************************************************\n",
            "test_acc: 0.922100003361702\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0014288356900215148\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14540299693743391\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.28691353013118115\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4317030168573061\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5750039392709732\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7241055431962012\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.5699999928474426, avg_cost: 0.8730565730730694\n",
            "**************************************************\n",
            "test_acc: 0.9214000016450882\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013624835014343262\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1475095549225808\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.29494732399781554\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.44069291144609474\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5819465019305549\n",
            "batch: 500, train_acc: 0.47999998927116394, avg_cost: 0.7286094313859941\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.5099999904632568, avg_cost: 0.8716764541467034\n",
            "**************************************************\n",
            "test_acc: 0.9205000013113022\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0012963007887204488\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14419780254364015\n",
            "batch: 200, train_acc: 0.6600000262260437, avg_cost: 0.28824606915315\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4345898657043774\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5793640564878784\n",
            "batch: 500, train_acc: 0.6700000166893005, avg_cost: 0.7236716280380885\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.5899999737739563, avg_cost: 0.8704980588952705\n",
            "**************************************************\n",
            "test_acc: 0.9253000038862228\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013985922932624817\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.14485152562459314\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2902237517635029\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.437274799346924\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5814979296922684\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7264783082405725\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 0.5799999833106995, avg_cost: 0.8692193537950508\n",
            "**************************************************\n",
            "test_acc: 0.9212000018358231\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0014163029193878173\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1449748000502586\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.29660932451486577\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.4426184091965356\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5879486172397927\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7313346848885214\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.6399999856948853, avg_cost: 0.8741828570763268\n",
            "**************************************************\n",
            "test_acc: 0.9249000018835067\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.001267164647579193\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.14003153105576832\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.28346143354972203\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.42653093953927385\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.5718048187096915\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7142837480703994\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 0.6200000047683716, avg_cost: 0.8579746216535572\n",
            "**************************************************\n",
            "test_acc: 0.9182000017166138\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.0014218644301096598\n",
            "batch: 100, train_acc: 0.6600000262260437, avg_cost: 0.14384229322274528\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2842536485195161\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.42645380020141616\n",
            "batch: 400, train_acc: 0.7599999904632568, avg_cost: 0.5698951615889871\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.7134156701962158\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 0.5400000214576721, avg_cost: 0.854616717000803\n",
            "**************************************************\n",
            "test_acc: 0.9213000029325485\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0013923890391985575\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.1440797437230746\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.28660115361213684\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.42926050434509905\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5734600042303403\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7180393113692606\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 0.6499999761581421, avg_cost: 0.862502174874147\n",
            "**************************************************\n",
            "test_acc: 0.9236000055074691\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013613567749659221\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.1446908534566561\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.28761932849884025\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.43292587767044705\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5824029464522998\n",
            "batch: 500, train_acc: 0.699999988079071, avg_cost: 0.7273154092828437\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 0.6600000262260437, avg_cost: 0.8701301971077915\n",
            "**************************************************\n",
            "test_acc: 0.9209000015258789\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0014647474884986876\n",
            "batch: 100, train_acc: 0.6499999761581421, avg_cost: 0.14326305439074838\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.2867601510882377\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4274769430359204\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.5728263209263482\n",
            "batch: 500, train_acc: 0.49000000953674316, avg_cost: 0.7248884676893557\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 0.550000011920929, avg_cost: 0.8703874252239869\n",
            "**************************************************\n",
            "test_acc: 0.9225000035762787\n",
            "batch: 0, train_acc: 0.699999988079071, avg_cost: 0.0012381867567698161\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1462231096625328\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.28774467716614405\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4380473620692888\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5940322826306026\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7426764650146166\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 0.5299999713897705, avg_cost: 0.8836740656693779\n",
            "**************************************************\n",
            "test_acc: 0.9212000054121018\n",
            "batch: 0, train_acc: 0.6700000166893005, avg_cost: 0.0011197208364804585\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14092314491669333\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.2834170900781949\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4254627901315685\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5706347254912055\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.7137821199496577\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 0.6100000143051147, avg_cost: 0.8577897993723539\n",
            "**************************************************\n",
            "test_acc: 0.9227000021934509\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013418928782145182\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14449567029873533\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.2855562484264375\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.4254780303438507\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5690402471025787\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7148404660820963\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 0.6299999952316284, avg_cost: 0.8550484509269402\n",
            "**************************************************\n",
            "test_acc: 0.9216000020503998\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.0013973162571589153\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1428440826137861\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.2868033406138421\n",
            "batch: 300, train_acc: 0.44999998807907104, avg_cost: 0.4306606587767601\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5749732047319414\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7150762736797331\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 0.6600000262260437, avg_cost: 0.8580961922804516\n",
            "**************************************************\n",
            "test_acc: 0.9253000009059906\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.00121026615301768\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.14411174744367594\n",
            "batch: 200, train_acc: 0.6800000071525574, avg_cost: 0.2874068595965702\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4288788436849911\n",
            "batch: 400, train_acc: 0.7300000190734863, avg_cost: 0.5744772265354787\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7203550040721884\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 0.6000000238418579, avg_cost: 0.862183429002761\n",
            "**************************************************\n",
            "test_acc: 0.9263000047206879\n",
            "[0, 0.7518999975919723, 0.8286999988555909, 0.8551000022888183, 0.8815000033378602, 0.8999000012874603, 0.9033000004291535, 0.9116000020503998, 0.9181000012159347, 0.9198000007867813, 0.9214000004529953, 0.9239000034332275, 0.9253000038862228, 0.9263000047206879]\n",
            "[100, 3.017639006177583, 1.404901708662509, 1.251403709053993, 1.2207689012090364, 1.1419776825110113, 1.0618281445900604, 1.0438530356685316, 1.0409303674101826, 1.01107532997926, 0.9863828338185947, 0.977629222869873, 0.9608956383665406, 0.9439110730091733, 0.9335681736469273, 0.9223622864484786, 0.9062068222959828, 0.8948541701833401, 0.8860550942023595, 0.8824167981743812, 0.8782279849052419, 0.8730565730730694, 0.8716764541467034, 0.8704980588952705, 0.8692193537950508, 0.8579746216535572, 0.854616717000803]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip_O_4IwsltP",
        "colab_type": "text"
      },
      "source": [
        "# 18 / 18 /  18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPKofKrIsoHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsQzgEE7srbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e53fbbdb-d322-4b73-bc5e-4d39d0052634"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.10999999940395355, avg_cost: 0.03721494992574056\n",
            "batch: 100, train_acc: 0.36000001430511475, avg_cost: 0.6860487838586172\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.9641724280516307\n",
            "batch: 300, train_acc: 0.4399999976158142, avg_cost: 1.209512250026068\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 1.436432611147565\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 1.670493098696075\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5299999713897705, avg_cost: 1.8978204399347331\n",
            "**************************************************\n",
            "test_acc: 0.8102999967336655\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0016428598761558534\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.2233051987489063\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.4373609753449757\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.6507124202450119\n",
            "batch: 400, train_acc: 0.46000000834465027, avg_cost: 0.8515857092539468\n",
            "batch: 500, train_acc: 0.4399999976158142, avg_cost: 1.0545055331786473\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.5299999713897705, avg_cost: 1.2532382018367445\n",
            "**************************************************\n",
            "test_acc: 0.8665999966859818\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.001922070582707723\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1971242279807727\n",
            "batch: 200, train_acc: 0.4699999988079071, avg_cost: 0.39617143630981455\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.5929702093203861\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.7812535160779952\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.9739043029149372\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.49000000953674316, avg_cost: 1.164676468571027\n",
            "**************************************************\n",
            "test_acc: 0.8817999988794327\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016219774881998698\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.1871316226323446\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.37510572840770096\n",
            "batch: 300, train_acc: 0.4699999988079071, avg_cost: 0.5593486398458483\n",
            "batch: 400, train_acc: 0.4300000071525574, avg_cost: 0.745982523461183\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.936174591978391\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5400000214576721, avg_cost: 1.1203088652094204\n",
            "**************************************************\n",
            "test_acc: 0.8909000009298325\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.001977286736170451\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.18252809445063267\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.3745138325293859\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.558168246746063\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.7409063360095024\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.9199689403176309\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.5699999928474426, avg_cost: 1.100147813161214\n",
            "**************************************************\n",
            "test_acc: 0.8845000022649765\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001610079805056254\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.18088138480981192\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.36611616084973037\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.5489143710335097\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.7322140848636621\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.9093918561935418\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.5400000214576721, avg_cost: 1.0818271846572558\n",
            "**************************************************\n",
            "test_acc: 0.9022000008821487\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.0015615137418111165\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.17489972710609425\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.35201674262682586\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.5328783380985261\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.7052407970031106\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8811382830142981\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.6000000238418579, avg_cost: 1.0573249576489132\n",
            "**************************************************\n",
            "test_acc: 0.8904000025987625\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0018258744478225708\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.17427370886007942\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.3549180124203362\n",
            "batch: 300, train_acc: 0.44999998807907104, avg_cost: 0.5314184270302452\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.7137749381860098\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.8916348082820578\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.5799999833106995, avg_cost: 1.064808894495169\n",
            "**************************************************\n",
            "test_acc: 0.9054000002145767\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.0019061730305353802\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.17305575509866078\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.3446092538038888\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5168180769681926\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6891155244906736\n",
            "batch: 500, train_acc: 0.5099999904632568, avg_cost: 0.8598628736535706\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.5600000023841858, avg_cost: 1.0338271976510678\n",
            "**************************************************\n",
            "test_acc: 0.9023000007867813\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016664394736289978\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.17106983214616775\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.34212117264668174\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.5131343243519468\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6836642481883367\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8568003220359486\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.5899999737739563, avg_cost: 1.0483645706375442\n",
            "**************************************************\n",
            "test_acc: 0.9076000052690506\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016648887594540914\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1709676901499431\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3445729167262714\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5176305568218231\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6883946754535036\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8620095252990717\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.5099999904632568, avg_cost: 1.0324813155333197\n",
            "**************************************************\n",
            "test_acc: 0.9125000023841858\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0017752633492151895\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.16941770533720657\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.34026781737804396\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.5093212572733562\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6819596626361208\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.8508193036913869\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.4699999988079071, avg_cost: 1.0167821553349488\n",
            "**************************************************\n",
            "test_acc: 0.9124000024795532\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0018539486328760784\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.1719003074367841\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.3401409964760143\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5080634114146231\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.674024154841899\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.8413611578941338\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.47999998927116394, avg_cost: 1.00944928218921\n",
            "**************************************************\n",
            "test_acc: 0.9097999972105026\n",
            "batch: 0, train_acc: 0.49000000953674316, avg_cost: 0.0017675801118214925\n",
            "batch: 100, train_acc: 0.5, avg_cost: 0.1684358478585878\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.33477338463067996\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.5010179551442463\n",
            "batch: 400, train_acc: 0.46000000834465027, avg_cost: 0.6760267234841985\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.8502406023939452\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.5199999809265137, avg_cost: 1.019261559844017\n",
            "**************************************************\n",
            "test_acc: 0.9103000038862228\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.001657871901988983\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.16598353574673333\n",
            "batch: 200, train_acc: 0.44999998807907104, avg_cost: 0.33088128238916387\n",
            "batch: 300, train_acc: 0.46000000834465027, avg_cost: 0.49739896287520713\n",
            "batch: 400, train_acc: 0.699999988079071, avg_cost: 0.6645369525750479\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8318112807472549\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.5799999833106995, avg_cost: 0.9966614098350206\n",
            "**************************************************\n",
            "test_acc: 0.9117000031471253\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0015860254565874736\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.16592570980389912\n",
            "batch: 200, train_acc: 0.5, avg_cost: 0.33134491006533306\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.49624182740847267\n",
            "batch: 400, train_acc: 0.47999998927116394, avg_cost: 0.6630451217293738\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8298237544298174\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.5799999833106995, avg_cost: 0.9942503076791764\n",
            "**************************************************\n",
            "test_acc: 0.9231000018119812\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.001589074432849884\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.16534094512462616\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.3296419659256936\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.4965213008721672\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6615778414408368\n",
            "batch: 500, train_acc: 0.6700000166893005, avg_cost: 0.8282031621535622\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.6200000047683716, avg_cost: 0.9890907031297688\n",
            "**************************************************\n",
            "test_acc: 0.9183000004291535\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0016980292399724325\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1624779966473579\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.32606920739014944\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.49082697341839476\n",
            "batch: 400, train_acc: 0.6600000262260437, avg_cost: 0.6549782532453535\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.8194348107775051\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.5699999928474426, avg_cost: 0.9800349339842799\n",
            "**************************************************\n",
            "test_acc: 0.9114000022411346\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0017036910851796468\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.16415131936470664\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.32823019951581955\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.49003716170787864\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6522816195090619\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.8160555780927345\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.5400000214576721, avg_cost: 0.9739183651407564\n",
            "**************************************************\n",
            "test_acc: 0.9014000028371811\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0018472345670064291\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.16508912076552712\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3256639913717905\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.48825736920038854\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6506773334741586\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8169755711158104\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.6000000238418579, avg_cost: 0.9797947867711371\n",
            "**************************************************\n",
            "test_acc: 0.9209000015258789\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0016303894917170207\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.16056380858023964\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.3226784051458042\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4819899551073709\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.640048320194085\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.805091601709524\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.6000000238418579, avg_cost: 0.9650999591747906\n",
            "**************************************************\n",
            "test_acc: 0.9236000013351441\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.0017111259698867798\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.159068401257197\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.31773498177528386\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.48384467482566834\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6453350704908368\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.8030483358105022\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.49000000953674316, avg_cost: 0.9640080659588174\n",
            "**************************************************\n",
            "test_acc: 0.9213000017404557\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.001546637515227\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1601494727532069\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.3180775914589566\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.47552246034145346\n",
            "batch: 400, train_acc: 0.5199999809265137, avg_cost: 0.6353620458642646\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7939552195866908\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.5299999713897705, avg_cost: 0.9528262653946884\n",
            "**************************************************\n",
            "test_acc: 0.9249000042676926\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0016620343923568726\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.1565219468871753\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.31564693719148634\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.47662166376908627\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6364654128750165\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7938466782371206\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.5, avg_cost: 0.9538634167114899\n",
            "**************************************************\n",
            "test_acc: 0.9242000037431717\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0017551650603612264\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15677053481340414\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3144250243902208\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.47231863498687754\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.6291333147883414\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7855708369612693\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.6499999761581421, avg_cost: 0.9447985395789146\n",
            "**************************************************\n",
            "test_acc: 0.923000003695488\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013620305061340333\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.15648242284854252\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.31278771489858637\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4815437969565388\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.6415191552042957\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.8370544541875514\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.6000000238418579, avg_cost: 1.001732053955395\n",
            "**************************************************\n",
            "test_acc: 0.9202000021934509\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001465381383895874\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.16334847768147792\n",
            "batch: 200, train_acc: 0.4399999976158142, avg_cost: 0.3206560856103898\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.48321333209673556\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.6398305067420004\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.7946419365207351\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.49000000953674316, avg_cost: 0.9492533259590459\n",
            "**************************************************\n",
            "test_acc: 0.9232000052928925\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.001669688622156779\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.15996267100175224\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.31634191701809555\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.4725297194719314\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6303369934360183\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7853214471538862\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.6600000262260437, avg_cost: 0.940657325883706\n",
            "**************************************************\n",
            "test_acc: 0.9253000020980835\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0017412185668945312\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.1561575666069985\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.3113521187504133\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4664666041731836\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6222360070546474\n",
            "batch: 500, train_acc: 0.6600000262260437, avg_cost: 0.7721115484833726\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.5099999904632568, avg_cost: 0.9240187591314324\n",
            "**************************************************\n",
            "test_acc: 0.9198000025749207\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.001486358145872752\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.15226041833559673\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.3030173495411875\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4549183287223184\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6080123916268354\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7646470956007649\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.5899999737739563, avg_cost: 0.9186323034763344\n",
            "**************************************************\n",
            "test_acc: 0.9212000036239624\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001388329267501831\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15123887340227757\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.30513132760922107\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4570988863706589\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6104764483372368\n",
            "batch: 500, train_acc: 0.6499999761581421, avg_cost: 0.7628748877843216\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.6000000238418579, avg_cost: 0.9223256555199617\n",
            "**************************************************\n",
            "test_acc: 0.9213000005483627\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0016530025005340575\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.15649674216906223\n",
            "batch: 200, train_acc: 0.5799999833106995, avg_cost: 0.3096571495135624\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.46109262913465476\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6135461870829266\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.7682262881596885\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.6399999856948853, avg_cost: 0.9268993445237478\n",
            "**************************************************\n",
            "test_acc: 0.9202000057697296\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016461339592933654\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.15378544012705492\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.3042066861192384\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.4570727259914077\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6083320509394009\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.759951880276203\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.5799999833106995, avg_cost: 0.9137130545576408\n",
            "**************************************************\n",
            "test_acc: 0.9231000036001206\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0016498331228892008\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15333367923895516\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.3048273947834968\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.45484877943992635\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6087045024832093\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7601633712649348\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.5199999809265137, avg_cost: 0.9119089285532636\n",
            "**************************************************\n",
            "test_acc: 0.9223000019788742\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001482876936594645\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.15360248277584715\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.30315579940875365\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4541626931230227\n",
            "batch: 400, train_acc: 0.6700000166893005, avg_cost: 0.6045796139041586\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7543112960457812\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.6299999952316284, avg_cost: 0.9022834172844892\n",
            "**************************************************\n",
            "test_acc: 0.9261000043153763\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0012901011109352112\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1530394111076991\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3034122750163078\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.45155153940121323\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.6031282715996111\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.75686627060175\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.5099999904632568, avg_cost: 0.9073286353548374\n",
            "**************************************************\n",
            "test_acc: 0.9177000004053116\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.0012925430138905844\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.1528276489178339\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.30114980310201644\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.452044669489066\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6018640168507898\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7519303198655448\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.6299999952316284, avg_cost: 0.9009390192230545\n",
            "**************************************************\n",
            "test_acc: 0.9237000024318696\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.001507921516895294\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.15121813396612802\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2996033352613449\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.44606197237968437\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.5951159245769191\n",
            "batch: 500, train_acc: 0.6600000262260437, avg_cost: 0.7449978492657351\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.5899999737739563, avg_cost: 0.8925277495384226\n",
            "**************************************************\n",
            "test_acc: 0.9230000042915344\n",
            "batch: 0, train_acc: 0.49000000953674316, avg_cost: 0.001788790225982666\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15340772916873302\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.3045812533299127\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4536261663834249\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.6039583913485205\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.75587065766255\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 0.5600000023841858, avg_cost: 0.9055073579152421\n",
            "**************************************************\n",
            "test_acc: 0.9221000015735626\n",
            "batch: 0, train_acc: 0.6800000071525574, avg_cost: 0.0012827428181966145\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.14824335505565014\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.2934695717692375\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.4444453957676886\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5914530398448314\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7415991481145225\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.5799999833106995, avg_cost: 0.8935356239477793\n",
            "**************************************************\n",
            "test_acc: 0.9239000028371811\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.0016113575299580892\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.14604742536942175\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2935340211788814\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4442389713724455\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5989474215110142\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7510859890778859\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 0.6499999761581421, avg_cost: 0.9006939444939298\n",
            "**************************************************\n",
            "test_acc: 0.9261000025272369\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.001314026415348053\n",
            "batch: 100, train_acc: 0.5, avg_cost: 0.14766603628794353\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.3042215190331141\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5272189748287203\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.7014964889486635\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.8643664722641311\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 0.6000000238418579, avg_cost: 1.0181406842668852\n",
            "**************************************************\n",
            "test_acc: 0.9222000020742417\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.001586294968922933\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.1565912773211797\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.3090302594502766\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4578900742530821\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6089400828878082\n",
            "batch: 500, train_acc: 0.6700000166893005, avg_cost: 0.762489742040634\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 0.5899999737739563, avg_cost: 0.9091295949618018\n",
            "**************************************************\n",
            "test_acc: 0.9244000035524368\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0014378682772318522\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14870663414398827\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2972438165545464\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4454718457659085\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5913523937265077\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7378497882684069\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 0.6000000238418579, avg_cost: 0.883198817074298\n",
            "**************************************************\n",
            "test_acc: 0.9267000025510788\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0012644618749618531\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.144998153646787\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.28878823935985565\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4341972638169925\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5823908080657325\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.7284335243701936\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 0.6200000047683716, avg_cost: 0.8706561682621642\n",
            "**************************************************\n",
            "test_acc: 0.9238000017404556\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0014437526464462281\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.15210461179415385\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.3029460830489794\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4520552273591359\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6020091790954268\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.7523055309057234\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 0.6000000238418579, avg_cost: 0.8970545738935467\n",
            "**************************************************\n",
            "test_acc: 0.9272000002861023\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001488727331161499\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14529661178588874\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.28971979041894286\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4360331336657207\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5823621443907421\n",
            "batch: 500, train_acc: 0.6600000262260437, avg_cost: 0.7253148893515267\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 0.5199999809265137, avg_cost: 0.8716991986831025\n",
            "**************************************************\n",
            "test_acc: 0.9263000017404557\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0014960276087125142\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1470221279064814\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.29165803660949063\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.43675421198209136\n",
            "batch: 400, train_acc: 0.6700000166893005, avg_cost: 0.5812778653701147\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7269025401274363\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 0.6200000047683716, avg_cost: 0.8728617007533702\n",
            "**************************************************\n",
            "test_acc: 0.9186000001430511\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0014946148792902628\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1473657666643461\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.29532369603713365\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.444563999970754\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5921522788206738\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.7401491477092114\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 0.5, avg_cost: 0.8853694162766139\n",
            "**************************************************\n",
            "test_acc: 0.9256000000238419\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014502183596293132\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14785595049460723\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.29639519880215315\n",
            "batch: 300, train_acc: 0.6499999761581421, avg_cost: 0.4424273937940595\n",
            "batch: 400, train_acc: 0.699999988079071, avg_cost: 0.5878352870543799\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7339081214865051\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 0.5799999833106995, avg_cost: 0.8797802656888971\n",
            "**************************************************\n",
            "test_acc: 0.9180000007152558\n",
            "[0, 0.8102999967336655, 0.8665999966859818, 0.8817999988794327, 0.8909000009298325, 0.9022000008821487, 0.9054000002145767, 0.9076000052690506, 0.9125000023841858, 0.9231000018119812, 0.9236000013351441, 0.9249000042676926, 0.9253000020980835, 0.9261000043153763, 0.9267000025510788, 0.9272000002861023]\n",
            "[100, 1.8978204399347331, 1.2532382018367445, 1.164676468571027, 1.1203088652094204, 1.100147813161214, 1.0818271846572558, 1.0573249576489132, 1.0338271976510678, 1.0324813155333197, 1.0167821553349488, 1.00944928218921, 0.9966614098350206, 0.9942503076791764, 0.9890907031297688, 0.9800349339842799, 0.9739183651407564, 0.9650999591747906, 0.9640080659588174, 0.9528262653946884, 0.9447985395789146, 0.940657325883706, 0.9240187591314324, 0.9186323034763344, 0.9137130545576408, 0.9119089285532636, 0.9022834172844892, 0.9009390192230545, 0.8925277495384226, 0.883198817074298, 0.8706561682621642]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlXB9NkwDeXF",
        "colab_type": "text"
      },
      "source": [
        "# Cont. : Restore ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3vvDbZlDccC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 25\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2UUj0GDjM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54a9711a-4a4e-4c44-aaa7-459608f79e36"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001661586860815684\n",
            "batch: 100, train_acc: 0.6499999761581421, avg_cost: 0.14738524446884793\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.29692100803057364\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.4422937098145486\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5899402118722595\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7372463192542391\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5099999904632568, avg_cost: 0.883684594730536\n",
            "**************************************************\n",
            "test_acc: 0.9254000020027161\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0014896883567174276\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.1473659917712212\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.2952321117122968\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.44170152455568307\n",
            "batch: 400, train_acc: 0.46000000834465027, avg_cost: 0.5860645527640987\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7348385425408687\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.5600000023841858, avg_cost: 0.8833632872502013\n",
            "**************************************************\n",
            "test_acc: 0.9231000036001206\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0013817313313484192\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.14772873520851135\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.292416245539983\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.44108227759599655\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5894107566277179\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.734433029989401\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.5699999928474426, avg_cost: 0.8766082748770708\n",
            "**************************************************\n",
            "test_acc: 0.9192000031471252\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0017205711205800374\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.14715008020401005\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.29199857980012883\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.4397620686888694\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5870153268178305\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7325372717777892\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.6000000238418579, avg_cost: 0.8791154943903293\n",
            "**************************************************\n",
            "test_acc: 0.9195000004768371\n",
            "batch: 0, train_acc: 0.699999988079071, avg_cost: 0.0011523231863975526\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.14383550276358928\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.29025223225355157\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.4368620684742926\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5866048733393345\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7341084985931705\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.699999988079071, avg_cost: 0.8776531547307955\n",
            "**************************************************\n",
            "test_acc: 0.9253000015020371\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016354037324587505\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1475003884236018\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.2908634280165036\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.43705149998267495\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5830977903803187\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7301594648758567\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.5400000214576721, avg_cost: 0.8774612913529071\n",
            "**************************************************\n",
            "test_acc: 0.9238000011444092\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.0010984901587168376\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.14486827860275905\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.2894331559538839\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4347886947790776\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5795569518208499\n",
            "batch: 500, train_acc: 0.6899999976158142, avg_cost: 0.7283340197801585\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5099999904632568, avg_cost: 0.8730974697073298\n",
            "**************************************************\n",
            "test_acc: 0.9247000026702881\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.0010874660809834797\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14305982848008475\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.2900319064656892\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4349971057971316\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5832373054822282\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7282396146655077\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.6100000143051147, avg_cost: 0.8743270584940902\n",
            "**************************************************\n",
            "test_acc: 0.9220000022649765\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.001423911948998769\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14700399398803712\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.29156769096851365\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.44118189056714413\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5879901611804964\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7332719887296363\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.6200000047683716, avg_cost: 0.8785881827274955\n",
            "**************************************************\n",
            "test_acc: 0.9225000035762787\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016042677561442058\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14749469170967738\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.29271823694308574\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.43677643646796505\n",
            "batch: 400, train_acc: 0.6800000071525574, avg_cost: 0.5811444037159273\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7288491684198364\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.6399999856948853, avg_cost: 0.8696587119499827\n",
            "**************************************************\n",
            "test_acc: 0.923299999833107\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.001439322829246521\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14849634716908136\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.29397797505060846\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.4404954694708189\n",
            "batch: 400, train_acc: 0.6600000262260437, avg_cost: 0.5867563813924793\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.7301940181851386\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.699999988079071, avg_cost: 0.8702065826455754\n",
            "**************************************************\n",
            "test_acc: 0.9246000015735626\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014400928219159444\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.14407291054725643\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.2859017378091812\n",
            "batch: 300, train_acc: 0.49000000953674316, avg_cost: 0.4301553716262181\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5765737357735633\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7217071800430611\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.6100000143051147, avg_cost: 0.8635277869304016\n",
            "**************************************************\n",
            "test_acc: 0.9230000019073487\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013503387570381164\n",
            "batch: 100, train_acc: 0.49000000953674316, avg_cost: 0.14368060052394863\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.2881266985336938\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4295374226570128\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5780859946211179\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.7246100323398906\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.5299999713897705, avg_cost: 0.8718360503514604\n",
            "**************************************************\n",
            "test_acc: 0.9167000031471253\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0016380872329076132\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.14877692083517713\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2947646460930506\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4426345709959666\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5868442986408868\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7303355606396988\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.5400000214576721, avg_cost: 0.8716506760319075\n",
            "**************************************************\n",
            "test_acc: 0.9265000033378601\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016769647598266601\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.146008524398009\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.2877361825108529\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.42990314135948837\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5717894650499025\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7194136714935297\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.5299999713897705, avg_cost: 0.8627500519156451\n",
            "**************************************************\n",
            "test_acc: 0.9263000017404557\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.0014219629764556886\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.14353452573219935\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.28401819169521336\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.42683940629164385\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.571215407450994\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.7142134439945224\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.5299999713897705, avg_cost: 0.8577161649862929\n",
            "**************************************************\n",
            "test_acc: 0.9254000014066697\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.001246168812115987\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.14492906570434572\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.2871596839030581\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.4309889803330102\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5745427652200059\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.7189822793006891\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.6399999856948853, avg_cost: 0.8616355436046913\n",
            "**************************************************\n",
            "test_acc: 0.9263000041246414\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.0012772065401077271\n",
            "batch: 100, train_acc: 0.6499999761581421, avg_cost: 0.14656093925237643\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.29061032583316143\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4352763950824735\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5811560508608814\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7242238555351892\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.6600000262260437, avg_cost: 0.8678872105479236\n",
            "**************************************************\n",
            "test_acc: 0.9223000025749206\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.001282845139503479\n",
            "batch: 100, train_acc: 0.6899999976158142, avg_cost: 0.14646752536296853\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.2872030108173688\n",
            "batch: 300, train_acc: 0.7300000190734863, avg_cost: 0.43230588525533675\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.575652819077174\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7198341256380082\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.5899999737739563, avg_cost: 0.8657786551117895\n",
            "**************************************************\n",
            "test_acc: 0.9252000015974045\n",
            "batch: 0, train_acc: 0.6899999976158142, avg_cost: 0.0011163072784741719\n",
            "batch: 100, train_acc: 0.6700000166893005, avg_cost: 0.1457338098684946\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.2886162270108858\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.42969775944948174\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5733882033824921\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.7174586594104765\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.6100000143051147, avg_cost: 0.8618747164805732\n",
            "**************************************************\n",
            "test_acc: 0.9213000017404557\n",
            "batch: 0, train_acc: 0.6800000071525574, avg_cost: 0.0013071436683336894\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.14241497625907262\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.2851181936264038\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.4272935010989504\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5725569225351014\n",
            "batch: 500, train_acc: 0.49000000953674316, avg_cost: 0.7154347781340279\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.5600000023841858, avg_cost: 0.8577022924025857\n",
            "**************************************************\n",
            "test_acc: 0.9264000010490417\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013585448265075683\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14261244515577953\n",
            "batch: 200, train_acc: 0.6600000262260437, avg_cost: 0.2850741691390672\n",
            "batch: 300, train_acc: 0.6700000166893005, avg_cost: 0.42678296794493953\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5697038989265756\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7150638942917183\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.5899999737739563, avg_cost: 0.8617093919714289\n",
            "**************************************************\n",
            "test_acc: 0.9232999992370605\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001422190566857656\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14588216443856564\n",
            "batch: 200, train_acc: 0.6499999761581421, avg_cost: 0.29100667963425336\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4342900180816654\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5783248408635461\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7193829832474392\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.5799999833106995, avg_cost: 0.8631522494554518\n",
            "**************************************************\n",
            "test_acc: 0.9262000036239624\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0014886772632598877\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14289235840241107\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.2834191178282101\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.42750014593203844\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5735405671596524\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7194738422830896\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.6000000238418579, avg_cost: 0.8636630873878791\n",
            "**************************************************\n",
            "test_acc: 0.9265000027418137\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001521776815255483\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14512155344088878\n",
            "batch: 200, train_acc: 0.6899999976158142, avg_cost: 0.2892702978849411\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.43254771490891775\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5747078054149948\n",
            "batch: 500, train_acc: 0.6899999976158142, avg_cost: 0.715600510438284\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.5699999928474426, avg_cost: 0.8563343659043317\n",
            "**************************************************\n",
            "test_acc: 0.9255000001192093\n",
            "[0, 0.9254000020027161, 0.9265000033378601]\n",
            "[100, 0.883684594730536, 0.8833632872502013, 0.8766082748770708, 0.8730974697073298, 0.8696587119499827, 0.8635277869304016, 0.8627500519156451, 0.8577161649862929, 0.8577022924025857, 0.8563343659043317]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXqEPCqLLTou",
        "colab_type": "text"
      },
      "source": [
        "#Cont. : fc에 dropout 없앰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAJSLm49LVJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.00005\n",
        "training_epochs = 25\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost_flatten.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc_flatten.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost_flatten.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ZMw-S7LXHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b7eefaf-68e9-4532-fb3c-d8fa61b87499"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:1, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.721866960404441e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.797082392225092e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 7.734919592318566e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.0766358715178124e-05\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.5381735605899385e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.9013841062758263e-05\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 1.0, avg_cost: 2.1905047409613088e-05\n",
            "**************************************************\n",
            "test_acc: 0.9306000018119812\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.274706007990365e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.989370826564179e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.5289407667516275e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.0182079522470152e-05\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.3014637777833588e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.6631117388120248e-05\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 1.0, avg_cost: 2.1328664594951385e-05\n",
            "**************************************************\n",
            "test_acc: 0.9310000038146973\n",
            "batch: 0, train_acc: 1.0, avg_cost: 7.369131102071455e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.461843913105591e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.841741287738992e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 9.934823411394219e-06\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.6927595165583783e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 3.283541240714762e-05\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 1.0, avg_cost: 3.7796036686188486e-05\n",
            "**************************************************\n",
            "test_acc: 0.9296000015735626\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.351583134441171e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 4.02503632779864e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.929011314914156e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.1491729354370741e-05\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.6235503879992553e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.985372876205815e-05\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 1.0, avg_cost: 2.285125309683167e-05\n",
            "**************************************************\n",
            "test_acc: 0.9301000028848648\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.3372006883922345e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 4.296544586092447e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 7.087048440250023e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.073418264468273e-05\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.3384301226911549e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.614316043031749e-05\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 1.0, avg_cost: 1.8799603353727437e-05\n",
            "**************************************************\n",
            "test_acc: 0.9302000027894973\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.2869614137259e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.1626278905273466e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 5.370349630879899e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 8.070395779734934e-06\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.0552856291970617e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3833514807591487e-05\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 1.0, avg_cost: 1.6237877295850434e-05\n",
            "**************************************************\n",
            "test_acc: 0.929600003361702\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.0574340800909947e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.3911779740615204e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.5039980499339525e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 6.5423955162557934e-06\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.4181400713830782e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.9096904404172234e-05\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 1.0, avg_cost: 2.234390258790835e-05\n",
            "**************************************************\n",
            "test_acc: 0.9304000020027161\n",
            "batch: 0, train_acc: 1.0, avg_cost: 7.201470604438024e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.546843468328082e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.055529432700269e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 8.496173023075925e-06\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.0242874166361083e-05\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.2381439235393058e-05\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 1.0, avg_cost: 1.4257956001320795e-05\n",
            "**************************************************\n",
            "test_acc: 0.9301000028848648\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.9708647465061707e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.5490215868870413e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.0208522245800415e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 4.503089350578191e-06\n",
            "batch: 400, train_acc: 1.0, avg_cost: 5.909000176581947e-06\n",
            "batch: 500, train_acc: 1.0, avg_cost: 7.802778164294934e-06\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 1.0, avg_cost: 9.330802105385048e-06\n",
            "**************************************************\n",
            "test_acc: 0.9302000021934509\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.0207750518845083e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.9644380084571373e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.265555586532782e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 4.454406616597836e-06\n",
            "batch: 400, train_acc: 1.0, avg_cost: 7.187630832774521e-06\n",
            "batch: 500, train_acc: 1.0, avg_cost: 8.7558314229123e-06\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 1.0, avg_cost: 1.0361419324335479e-05\n",
            "**************************************************\n",
            "test_acc: 0.9307000029087067\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.375766249926528e-09\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.7693829924307163e-06\n",
            "batch: 200, train_acc: 1.0, avg_cost: 2.9422063494344276e-06\n",
            "batch: 300, train_acc: 1.0, avg_cost: 4.150994833214128e-06\n",
            "batch: 400, train_acc: 1.0, avg_cost: 5.401994406491659e-06\n",
            "batch: 500, train_acc: 1.0, avg_cost: 7.028474480635555e-06\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 1.0, avg_cost: 9.148655040854455e-06\n",
            "**************************************************\n",
            "test_acc: 0.9307000029087067\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.909250037599122e-09\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.7379483306948867e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d10539a47ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3D92TdekJ0r",
        "colab_type": "text"
      },
      "source": [
        "# 텐서 보드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjxaSJdkE4Tf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "1f09623c-7e8e-47a1-c83f-100d7ee56b3e"
      },
      "source": [
        "## 텐서 보드 생성.\n",
        "from tensorboardcolab import * \n",
        "from datetime import datetime\n",
        "\n",
        "current_time = str(datetime.now().timestamp())\n",
        "train_log_dir = 'logs/tensorboard/train/' + current_time\n",
        "\n",
        "tbc = TensorBoardColab(graph_path = train_log_dir) # To create a tensorboardcolab object it will automatically creat a link\n",
        "writer = tbc.get_writer() # To create a FileWriter\n",
        "writer.add_graph(tf.get_default_graph()) # add the graph \n",
        "writer.flush()\n",
        "\n",
        "\n",
        "## 텐서 보드 생성.\n",
        "from tensorboardcolab import * "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0814 07:34:45.742098 140036658886528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorBoard link:\n",
            "https://b2bb17ae.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xxPXsaCDbIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "|"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}