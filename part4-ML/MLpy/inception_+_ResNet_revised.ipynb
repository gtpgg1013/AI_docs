{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inception + ResNet",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPBlN93TOPtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poX7_GniOypD",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 로드 & 함수 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWeR-AWtOZrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "data = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = data.load_data()\n",
        "\n",
        "test_labels = test_labels.reshape(-1,1)\n",
        "train_labels = train_labels.reshape(-1,1)\n",
        "\n",
        "train_images = train_images / 255\n",
        "test_images = test_images /255\n",
        "\n",
        "def make_conv_twice(input_layer, num_filter, keep_prob, layer_num,  batch_prob, filter_size=3, strides=1):\n",
        "#   w1 = tf.Variable(tf.random_normal([filter_size,filter_size,1,num_filter]) )\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding='SAME')\n",
        "  L1 = tf.layers.batch_normalization(L1, center=True, scale=True, training=batch_prob)\n",
        "  L1 = tf.nn.relu(L1)\n",
        "#   w1_1 = tf.Variable(tf.random_normal([filter_size,filter_size,num_filter,num_filter]))\n",
        "  w1_1 = tf.get_variable(\"w2_\"+str(layer_num), shape=[filter_size,filter_size,num_filter,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1_1 = tf.nn.conv2d(L1, w1_1, strides=[1,strides,strides,1], padding='SAME') \n",
        "  L1_1 = tf.nn.relu(L1_1)\n",
        "  L1_1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "  res = L1_1\n",
        "#   res = tf.nn.dropout(L1_1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once(input_layer, num_filter, keep_prob, layer_num, batch_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding='SAME')\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once_not_same(input_layer, num_filter, keep_prob, layer_num, batch_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  w1 = tf.Variable(tf.random_normal([filter_size,filter_size,1,num_filter]))\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding=\"VALID\")\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once_not_same_wo_pooling(input_layer, num_filter, keep_prob, layer_num, batch_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding=\"VALID\")\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def make_conv_once_wo_pooling(input_layer, num_filter, keep_prob, layer_num, batch_prob, padding=\"SAME\", filter_size=3, strides=1):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[filter_size,filter_size,1,num_filter], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding=padding)\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  res = L1\n",
        "#   res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res\n",
        "\n",
        "def my_inception_model(input_layer, keep_prob, batch_prob, filter_num_list):\n",
        "  w1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[0]]))\n",
        "  l1 = tf.nn.conv2d(input_layer, w1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  w2_1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[1]]))\n",
        "  l2_1 = tf.nn.conv2d(input_layer, w2_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w2_2 = tf.Variable(tf.random_normal([3, 3, filter_num_list[1], filter_num_list[2]]))\n",
        "  l2_2 = tf.nn.conv2d(l2_1, w2_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  w3_1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[3]]))\n",
        "  l3_1 = tf.nn.conv2d(input_layer, w3_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w3_2 = tf.Variable(tf.random_normal([5, 5, filter_num_list[3], filter_num_list[4]]))\n",
        "  l3_2 = tf.nn.conv2d(l3_1, w3_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  l4_1 = tf.nn.max_pool(input_layer, ksize=[1,3,3,1], strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w4_1 = tf.Variable(tf.random_normal([1,1,filter_num_list[6],filter_num_list[5]]))\n",
        "  l4_2 = tf.nn.conv2d(l4_1, w4_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  res = tf.concat([l1, l2_2, l3_2, l4_2], 3) # input의 shape가 [-1,28,28,1] 이라고 가정\n",
        "  \n",
        "  return res # 다시 이 return된 layer를 inception 모델에 돌리기!\n",
        "\n",
        "def my_inception_model_v2(input_layer, keep_prob, batch_prob, filter_num_list, layer_num):\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[0]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l1 = tf.nn.conv2d(input_layer, w1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l1 = tf.layers.batch_normalization(l1, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  w2_1 = tf.get_variable(\"w2_1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[1]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l2_1 = tf.nn.conv2d(input_layer, w2_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l2_1 = tf.layers.batch_normalization(l2_1, center=True, scale=True, training=batch_prob)\n",
        "  w2_2 = tf.get_variable(\"w2_2_\"+str(layer_num), shape=[3,3,filter_num_list[1],filter_num_list[2]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l2_2 = tf.nn.conv2d(l2_1, w2_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l2_2 = tf.layers.batch_normalization(l2_2, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  w3_1 = tf.get_variable(\"w3_1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[3]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l3_1 = tf.nn.conv2d(input_layer, w3_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l3_1 = tf.layers.batch_normalization(l3_1, center=True, scale=True, training=batch_prob)\n",
        "  w3_2 = tf.get_variable(\"w3_2_\"+str(layer_num), shape=[5,5,filter_num_list[3],filter_num_list[4]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l3_2 = tf.nn.conv2d(l3_1, w3_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l3_2 = tf.layers.batch_normalization(l3_2, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  l4_1 = tf.nn.max_pool(input_layer, ksize=[1,3,3,1], strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w4_1 = tf.get_variable(\"w4_1_\"+str(layer_num), shape=[1,1,filter_num_list[6],filter_num_list[5]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  l4_2 = tf.nn.conv2d(l4_1, w4_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  l4_2 = tf.layers.batch_normalization(l4_2, center=True, scale=True, training=batch_prob)\n",
        "  \n",
        "  res = tf.concat([l1, l2_2, l3_2, l4_2], 3) # input의 shape가 [-1,28,28,1] 이라고 가정\n",
        "  \n",
        "  return res # 다시 이 return된 layer를 inception 모델에 돌리기!\n",
        "\n",
        "# 이 함수에는 pooling이 포함되어 있지 않습니다.\n",
        "def my_resNet_model(input_layer, keep_prob, batch_prob, filter_num_list, layer_num):\n",
        "  bn1 = tf.layers.batch_normalization(input_layer, center=True, scale=True, training=batch_prob)\n",
        "  bn1 = tf.nn.relu(bn1)\n",
        "  w1 = tf.get_variable(\"w1_\"+str(layer_num), shape=[3,3,filter_num_list[0],filter_num_list[1]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  h1 = tf.nn.conv2d(input_layer, w1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  h1 = tf.nn.dropout(h1, keep_prob=keep_prob)\n",
        "  bn2 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "  bn2 = tf.nn.relu(bn2)\n",
        "  w2 = tf.get_variable(\"w2_\"+str(layer_num), shape=[3,3,filter_num_list[1],filter_num_list[2]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "  h2 = tf.nn.conv2d(bn2, w2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  if filter_num_list[0]!=filter_num_list[-1]:\n",
        "    us = tf.get_variable(\"us_\"+str(layer_num), shape=[1,1,filter_num_list[0],filter_num_list[2]], initializer=tf.contrib.layers.xavier_initializer())\n",
        "    input_layer = tf.nn.conv2d(input_layer, us, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  return input_layer + h2\n",
        "  \n",
        "  \n",
        "def shuffleData(images, labels):\n",
        "  tmp = np.concatenate([images.reshape(-1,28*28),labels],axis=1)\n",
        "  tmpd = pd.DataFrame(tmp).sample(frac=1)\n",
        "  ret = tmpd.values.reshape(-1,785)\n",
        "  retImg = ret[:,:-1].reshape(-1,28,28)\n",
        "  retlab = ret[:,-1].reshape(-1,1)\n",
        "  return retImg, retlab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-isJuULW1-P",
        "colab_type": "code",
        "outputId": "e8b42cfe-a319-4d82-840c-b77b09227c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0b0X7o9_nW8",
        "colab_type": "text"
      },
      "source": [
        "# 텐서 보드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mezVmJBm_n2o",
        "colab_type": "code",
        "outputId": "dad6d028-6546-4142-a9ac-779aa50740b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "## 텐서 보드 생성.\n",
        "from tensorboardcolab import * \n",
        "from datetime import datetime\n",
        "\n",
        "current_time = str(datetime.now().timestamp())\n",
        "train_log_dir = 'logs/tensorboard/train/' + current_time\n",
        "\n",
        "tbc = TensorBoardColab(graph_path = train_log_dir) # To create a tensorboardcolab object it will automatically creat a link\n",
        "writer = tbc.get_writer() # To create a FileWriter\n",
        "writer.add_graph(tf.get_default_graph()) # add the graph \n",
        "writer.flush()\n",
        "\n",
        "\n",
        "## 텐서 보드 생성.\n",
        "from tensorboardcolab import * "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 22:37:03.505860 140165723359104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorBoard link:\n",
            "https://7f0b5604.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VVgBufnO2c1",
        "colab_type": "text"
      },
      "source": [
        "# 실제 실행 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y57ViWk_Owej",
        "colab_type": "code",
        "outputId": "63be6811-7932-4ad2-855e-eab25f7c18c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 32\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0814 16:34:39.582724 140365125961600 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0814 16:34:39.634604 140365125961600 deprecation.py:323] From <ipython-input-1-79d4041f1000>:94: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W0814 16:34:43.635774 140365125961600 deprecation.py:506] From <ipython-input-3-d31cd5ee0471>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gic0WtrnQuct",
        "colab_type": "code",
        "outputId": "761b7ed5-5d69-417b-9a16-3d45bb7c6f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i == 900 or i == 1800:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    saver.save(sess, save_file)\n",
        "  \n",
        "  final_acu = 0\n",
        "  for num in range(100):\n",
        "    acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "    final_acu += acu\n",
        "  final_acu = final_acu / 100\n",
        "  print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5199999809265137, avg_cost: 0.9997651163736981\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.5199999809265137, avg_cost: 0.9849679580330843\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.47999998927116394, avg_cost: 0.9779691456754994\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5299999713897705, avg_cost: 0.9669578102231022\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.6299999952316284, avg_cost: 0.9599478222926454\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.5600000023841858, avg_cost: 0.9584405269225437\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5799999833106995, avg_cost: 0.9416050454974174\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.5600000023841858, avg_cost: 0.9386361687382061\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.6399999856948853, avg_cost: 0.9302902710437772\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.5899999737739563, avg_cost: 0.9301680853962903\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.5699999928474426, avg_cost: 0.9202526120344803\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.5600000023841858, avg_cost: 0.9111420182387028\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.5, avg_cost: 0.906685955822467\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.6399999856948853, avg_cost: 0.8989764041701954\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.6499999761581421, avg_cost: 0.8907221458355584\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.5899999737739563, avg_cost: 0.8957964470982547\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.5, avg_cost: 0.8922380998730661\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.6100000143051147, avg_cost: 0.8785748392343519\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.6700000166893005, avg_cost: 0.8809885105490682\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.5199999809265137, avg_cost: 0.869912796914578\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.5899999737739563, avg_cost: 0.8679758891463278\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.5899999737739563, avg_cost: 0.8671677815914155\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.6000000238418579, avg_cost: 0.8654734166463213\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.5600000023841858, avg_cost: 0.8615362991889314\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.6399999856948853, avg_cost: 0.8668234710892035\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.6499999761581421, avg_cost: 0.8664153464635214\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.5799999833106995, avg_cost: 0.8528024462858841\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.6000000238418579, avg_cost: 0.8686522155006734\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.5699999928474426, avg_cost: 0.8689878271023428\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.550000011920929, avg_cost: 0.8586395846803982\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.550000011920929, avg_cost: 0.8522232209642726\n",
            "**************************************************\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.6399999856948853, avg_cost: 0.8579460100332895\n",
            "**************************************************\n",
            "final_acc: 0.10059999968856573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wy4S609Dzl5",
        "colab_type": "text"
      },
      "source": [
        "# validation : batch_prob : True!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzg193XXU2EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 32\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eHpG_njD8iD",
        "colab_type": "code",
        "outputId": "43538c98-3811-4301-be8f-c66c34915b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"validation start\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "    \n",
        "  final_acu = 0\n",
        "  for num in range(10):\n",
        "    acu = sess.run(acc, feed_dict={x:test_images[1000*num:1000*(num+1),:,:],y:test_labels[1000*num:1000*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "    final_acu += acu\n",
        "  final_acu = final_acu / 10\n",
        "  print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation start\n",
            "final_acc: 0.9282999992370605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSkzlD6TFfET",
        "colab_type": "text"
      },
      "source": [
        "# Batch size가 너무 작았나 ? : 256으로 수정 / 모델 지속적 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLDx-lM_EVZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 10\n",
        "batch_size = 256\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "# load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch.ckpt'\n",
        "# load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_accuracy_val.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiYjpph8Fs02",
        "colab_type": "code",
        "outputId": "9921ff82-3314-4d17-a3ac-f0329ad60d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if total_batch%50 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.44921875, avg_cost: 1.4835703357672079\n",
            "**************************************************\n",
            "test_acc: 0.09999999990686774\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.54296875, avg_cost: 1.1641503050286544\n",
            "**************************************************\n",
            "test_acc: 0.09999999990686774\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.5859375, avg_cost: 1.1033311218787467\n",
            "**************************************************\n",
            "test_acc: 0.1669000004976988\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5546875, avg_cost: 1.0771177719291456\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.5703125, avg_cost: 1.0460005257374196\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.6015625, avg_cost: 1.0341714381152747\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5546875, avg_cost: 1.0215940029702641\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.59375, avg_cost: 1.0038603307344978\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.56640625, avg_cost: 1.0002676538932016\n",
            "**************************************************\n",
            "test_acc: 0.09999999994412065\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.54296875, avg_cost: 0.9899300619577751\n",
            "**************************************************\n",
            "test_acc: 0.09999999983236194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGkD_WCaaoDP",
        "colab_type": "text"
      },
      "source": [
        "# Restore해서 계속 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYPXpOo_Ip5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 50\n",
        "batch_size = 256\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_add_50.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIrcJ78sau9T",
        "colab_type": "code",
        "outputId": "4ccf3218-b600-4102-c8ec-0be674a8fec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if total_batch%50 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5625, avg_cost: 0.8366367936643783\n",
            "**************************************************\n",
            "test_acc: 0.9346000015735626\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.61328125, avg_cost: 0.8306661834064716\n",
            "**************************************************\n",
            "test_acc: 0.9338000023365021\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.58984375, avg_cost: 0.8349245249206184\n",
            "**************************************************\n",
            "test_acc: 0.9358000016212463\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5625, avg_cost: 0.8325600341344492\n",
            "**************************************************\n",
            "test_acc: 0.9371000003814697\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.60546875, avg_cost: 0.8285959785820071\n",
            "**************************************************\n",
            "test_acc: 0.935100000500679\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.59375, avg_cost: 0.8293797557170571\n",
            "**************************************************\n",
            "test_acc: 0.9358999991416931\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.6015625, avg_cost: 0.8257910889438078\n",
            "**************************************************\n",
            "test_acc: 0.9365000009536744\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.58984375, avg_cost: 0.8346338203320139\n",
            "**************************************************\n",
            "test_acc: 0.9361999988555908\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.56640625, avg_cost: 0.830291777085035\n",
            "**************************************************\n",
            "test_acc: 0.9371999996900559\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.55859375, avg_cost: 0.8340427855141138\n",
            "**************************************************\n",
            "test_acc: 0.9378000003099441\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.54296875, avg_cost: 0.8368155811077507\n",
            "**************************************************\n",
            "test_acc: 0.9374000006914138\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.578125, avg_cost: 0.8255395270310916\n",
            "**************************************************\n",
            "test_acc: 0.9371000009775162\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.6171875, avg_cost: 0.830983622970744\n",
            "**************************************************\n",
            "test_acc: 0.9357000023126603\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.59375, avg_cost: 0.8566839689882388\n",
            "**************************************************\n",
            "test_acc: 0.9115000027418136\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.61328125, avg_cost: 0.8664581872459151\n",
            "**************************************************\n",
            "test_acc: 0.9274000018835068\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.60546875, avg_cost: 0.8407601301486671\n",
            "**************************************************\n",
            "test_acc: 0.9332999992370605\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.578125, avg_cost: 0.8435161213080092\n",
            "**************************************************\n",
            "test_acc: 0.9321999990940094\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.625, avg_cost: 0.838775946543767\n",
            "**************************************************\n",
            "test_acc: 0.9322000014781952\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.6484375, avg_cost: 0.8359945182107454\n",
            "**************************************************\n",
            "test_acc: 0.9319000029563904\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.56640625, avg_cost: 0.8365147610505425\n",
            "**************************************************\n",
            "test_acc: 0.9261000037193299\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.58203125, avg_cost: 0.8467302284179589\n",
            "**************************************************\n",
            "test_acc: 0.9284000015258789\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.578125, avg_cost: 0.8393728740704369\n",
            "**************************************************\n",
            "test_acc: 0.9274000006914139\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.59375, avg_cost: 0.8403550511241976\n",
            "**************************************************\n",
            "test_acc: 0.9331999999284745\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.58203125, avg_cost: 0.8396766802184604\n",
            "**************************************************\n",
            "test_acc: 0.9319000035524369\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.60546875, avg_cost: 0.8361561568374308\n",
            "**************************************************\n",
            "test_acc: 0.9345000034570694\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.546875, avg_cost: 0.8409606429756198\n",
            "**************************************************\n",
            "test_acc: 0.9334000033140183\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.640625, avg_cost: 0.8377864116277447\n",
            "**************************************************\n",
            "test_acc: 0.9337000036239624\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.62890625, avg_cost: 0.836057935005579\n",
            "**************************************************\n",
            "test_acc: 0.9291000032424926\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.59765625, avg_cost: 0.8410058039375863\n",
            "**************************************************\n",
            "test_acc: 0.9324000018835068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0e638ab37f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-05X_s1EQBW",
        "colab_type": "text"
      },
      "source": [
        "# validation : Inception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpkGGM_ebWmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "training_epochs = 50\n",
        "batch_size = 256\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_inception_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_inception_0813_BN_add_50.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, 1, batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 192, keep_prob, 1_1, batch_prob)\n",
        "\n",
        "iList1 = [64,96,128,16,32,32,192]\n",
        "h2 = my_inception_model_v2(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "\n",
        "iList2 = [128,128,192,32,96,64,256]\n",
        "h3 = my_inception_model_v2(h2, keep_prob,batch_prob,iList2,3)\n",
        "h3 = tf.nn.max_pool(h3, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList3 = [192,96,208,16,48,64,480]\n",
        "h4 = my_inception_model_v2(h3, keep_prob,batch_prob, iList3, 4)\n",
        "\n",
        "iList4 = [160,112,224,24,64,64,512]\n",
        "h5 = my_inception_model_v2(h4, keep_prob, batch_prob, iList4, 5)\n",
        "\n",
        "iList5 = [128,128,256,24,64,64,512]\n",
        "h6 = my_inception_model_v2(h5, keep_prob, batch_prob, iList5, 6)\n",
        "\n",
        "iList6 = [112,144,288,32,64,64,512]\n",
        "h7 = my_inception_model_v2(h6, keep_prob, batch_prob, iList6, 7)\n",
        "\n",
        "iList7 = [256,160,320,32,128,128,528]\n",
        "h8 = my_inception_model_v2(h7, keep_prob, batch_prob, iList7, 8)\n",
        "h8 = tf.nn.max_pool(h8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList8 = [256,160,320,32,128,128,832]\n",
        "h9 = my_inception_model_v2(h8, keep_prob, batch_prob, iList8, 9)\n",
        "\n",
        "iList9 = [384,192,384,48,128,128,832]\n",
        "h10 = my_inception_model_v2(h9, keep_prob, batch_prob, iList9, 10)\n",
        "\n",
        "h11 = tf.nn.avg_pool(h10, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h11_flat = tf.reshape(h11, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h11_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9QOtjFEEVAw",
        "colab_type": "code",
        "outputId": "11710ca9-f137-4874-f9c8-d6c81310dfae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"validation start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, save_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  final_acu = 0\n",
        "  for num in range(100):\n",
        "    acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "    final_acu += acu\n",
        "  final_acu = final_acu / 100\n",
        "  print(\"test_acc: {}\".format(final_acu))\n",
        "\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "test_acc: 0.9378000003099441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhq7y-AYExqm",
        "colab_type": "text"
      },
      "source": [
        "# ResNet 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwmVBYQMEjS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0813_BN_100epoch_mod.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 128, keep_prob, '1b', batch_prob)\n",
        "h1_1 = tf.layers.batch_normalization(h1_1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,512]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList11 = [512,512,512]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [512,512,512]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [512,512,1024]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "\n",
        "h15 = tf.nn.avg_pool(h14, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6bCjHXvE42p",
        "colab_type": "code",
        "outputId": "b691c96e-9aee-451a-8d60-1da552277618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.07999999821186066, avg_cost: 0.005154190063476563\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 1.9964513392249748\n",
            "batch: 200, train_acc: 0.7099999785423279, avg_cost: 2.2375136198600143\n",
            "batch: 300, train_acc: 0.7599999904632568, avg_cost: 2.412682707111044\n",
            "batch: 400, train_acc: 0.8399999737739563, avg_cost: 2.5811256759862133\n",
            "batch: 500, train_acc: 0.7799999713897705, avg_cost: 2.7341592581073457\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.8199999928474426, avg_cost: 2.8510153746604936\n",
            "**************************************************\n",
            "test_acc: 0.7984000009298324\n",
            "batch: 0, train_acc: 0.800000011920929, avg_cost: 0.0009434366226196289\n",
            "batch: 100, train_acc: 0.7300000190734863, avg_cost: 0.19467283651232725\n",
            "batch: 200, train_acc: 0.9200000166893005, avg_cost: 0.33258394047617906\n",
            "batch: 300, train_acc: 0.8700000047683716, avg_cost: 0.44289749021331465\n",
            "batch: 400, train_acc: 0.8500000238418579, avg_cost: 0.5603665421406429\n",
            "batch: 500, train_acc: 0.8500000238418579, avg_cost: 0.6510553870101772\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.8700000047683716, avg_cost: 0.7340909018119177\n",
            "**************************************************\n",
            "test_acc: 0.8440000003576279\n",
            "batch: 0, train_acc: 0.8700000047683716, avg_cost: 0.0006785560647646586\n",
            "batch: 100, train_acc: 0.8600000143051147, avg_cost: 0.07413838220139345\n",
            "batch: 200, train_acc: 0.8799999952316284, avg_cost: 0.14815343352655558\n",
            "batch: 300, train_acc: 0.7300000190734863, avg_cost: 0.22651798225939254\n",
            "batch: 400, train_acc: 0.8399999737739563, avg_cost: 0.3761584546913701\n",
            "batch: 500, train_acc: 0.8799999952316284, avg_cost: 0.46128022243579186\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.8799999952316284, avg_cost: 0.5250356141477822\n",
            "**************************************************\n",
            "test_acc: 0.8662000000476837\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.0008178543051083882\n",
            "batch: 100, train_acc: 0.8799999952316284, avg_cost: 0.07762692620356879\n",
            "batch: 200, train_acc: 0.8500000238418579, avg_cost: 0.15108186744153498\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.21643676385283475\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.27156638869394867\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.3205958382909498\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.8899999856948853, avg_cost: 0.37262253350267877\n",
            "**************************************************\n",
            "test_acc: 0.8949000000953674\n",
            "batch: 0, train_acc: 0.8799999952316284, avg_cost: 0.0004292422036329905\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.04994467178980509\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.10486312953134376\n",
            "batch: 300, train_acc: 0.8700000047683716, avg_cost: 0.1684720111141602\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.2154240253070991\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.26201706329981506\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.8999999761581421, avg_cost: 0.30935231226185983\n",
            "**************************************************\n",
            "test_acc: 0.9000000017881393\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0003716235856215159\n",
            "batch: 100, train_acc: 0.8799999952316284, avg_cost: 0.04718037704626721\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.08913484857728082\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.13563822676738094\n",
            "batch: 400, train_acc: 0.8600000143051147, avg_cost: 0.1809344558169443\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.2274268771087128\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.949999988079071, avg_cost: 0.26988040873159946\n",
            "**************************************************\n",
            "test_acc: 0.8993999993801117\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0003771426280339559\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.03594779495149852\n",
            "batch: 200, train_acc: 0.8700000047683716, avg_cost: 0.07733245609949031\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.11781670412669579\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.1595341245581705\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.20373079172025102\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.9100000262260437, avg_cost: 0.24295661093046236\n",
            "**************************************************\n",
            "test_acc: 0.8955999982357025\n",
            "batch: 0, train_acc: 0.8899999856948853, avg_cost: 0.00037847566107908885\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.034754508739958216\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.06822745151196916\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.10231215183312695\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.13593645414958394\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.1709220537543297\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.7300000190734863, avg_cost: 0.7906106751412153\n",
            "**************************************************\n",
            "test_acc: 0.5671999981999397\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0033081968625386557\n",
            "batch: 100, train_acc: 0.6600000262260437, avg_cost: 0.21098574837048847\n",
            "batch: 200, train_acc: 0.8700000047683716, avg_cost: 0.2801089279105264\n",
            "batch: 300, train_acc: 0.8199999928474426, avg_cost: 0.3538126650204258\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.4108919249723354\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.4636877402663232\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.949999988079071, avg_cost: 0.512335744251808\n",
            "**************************************************\n",
            "test_acc: 0.9043000000715256\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0001747969662149747\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.04348794944584371\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.09031525850296021\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.13260169946899011\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.169166556832691\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.20402826473116878\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.949999988079071, avg_cost: 0.2402246898785233\n",
            "**************************************************\n",
            "test_acc: 0.9123000001907349\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00037488015989462533\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.034157934648295245\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.06603333743289114\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.0997107405401767\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.12988500384613885\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.16278332263852166\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9300000071525574, avg_cost: 0.19782931090022138\n",
            "**************************************************\n",
            "test_acc: 0.9106000024080276\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00025651633739471436\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.027902014907449486\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.05472847208380698\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.08382541075348857\n",
            "batch: 400, train_acc: 0.9100000262260437, avg_cost: 0.11292831844960649\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.14555255100751918\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.9300000071525574, avg_cost: 0.17991232975696517\n",
            "**************************************************\n",
            "test_acc: 0.9135000032186508\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00017642635852098464\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.026034525868793325\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.05249674472957848\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.08502831727266309\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.12273716809848935\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.15616098242501408\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.9399999976158142, avg_cost: 0.18513166400293507\n",
            "**************************************************\n",
            "test_acc: 0.9150999987125397\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0004949122170607249\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.02601604901254176\n",
            "batch: 200, train_acc: 0.9100000262260437, avg_cost: 0.05101431363572672\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.08139121547341345\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.11149654197196163\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.1415777389456828\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.9800000190734863, avg_cost: 0.16784833968927462\n",
            "**************************************************\n",
            "test_acc: 0.9184000027179718\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.65782176454862e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.020354559002444147\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.043200692242632316\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.06974531452481948\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.0976869584682087\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.1257273493458827\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.9200000166893005, avg_cost: 0.1513810210116208\n",
            "**************************************************\n",
            "test_acc: 0.9157000035047531\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00024286056558291117\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.027007957777629297\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.055368055024494746\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.08170814500811197\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.10780632951452082\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.13243553162707633\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.8500000238418579, avg_cost: 0.16057118084126457\n",
            "**************************************************\n",
            "test_acc: 0.9054000020027161\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.00043593987822532654\n",
            "batch: 100, train_acc: 0.8199999928474426, avg_cost: 0.3968072225898505\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.5413956431547802\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.5819758614152671\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.6118448888075847\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.6393211071627833\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.9100000262260437, avg_cost: 0.6651251899761457\n",
            "**************************************************\n",
            "test_acc: 0.9226000016927719\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00026164879401524864\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.017792328366388874\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.03462144194170834\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.05355084470783671\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.07227074357370539\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.08933862015294533\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9800000190734863, avg_cost: 0.10722805393549302\n",
            "**************************************************\n",
            "test_acc: 0.9231000006198883\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00018762451906998952\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.011615546829998493\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.025065884417854237\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.03952878227224573\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.05326021486660466\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.06902643535829459\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9900000095367432, avg_cost: 0.08431598786807934\n",
            "**************************************************\n",
            "test_acc: 0.9234000027179718\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00010834300269683202\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.009195036496967077\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.02205216217475633\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.032308857847626014\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.044032826675102145\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.057707273870085715\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.9599999785423279, avg_cost: 0.07236947623702392\n",
            "**************************************************\n",
            "test_acc: 0.919900004863739\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00012382323543230693\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.010734905476371445\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.025323904876907676\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.03908924233323584\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.056599415871314694\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.0791860213658462\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.9300000071525574, avg_cost: 0.10577716868060329\n",
            "**************************************************\n",
            "test_acc: 0.887899997830391\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00025956315298875175\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.020249451582009595\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.033542432005051505\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.0479182650684379\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.06033759736766421\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.07341019160424675\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.9800000190734863, avg_cost: 0.08659566245041796\n",
            "**************************************************\n",
            "test_acc: 0.9223000025749206\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00019360660264889398\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.010910380359273403\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.024333403439571466\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.03563779110088947\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.05299937951300917\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.08227447484542304\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9700000286102295, avg_cost: 0.12630455905183527\n",
            "**************************************************\n",
            "test_acc: 0.8905000001192093\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00026120165983835856\n",
            "batch: 100, train_acc: 0.9100000262260437, avg_cost: 0.09218759109576545\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.15376985786482686\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.17298281695383283\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.18827015980302036\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.2014167667490739\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9800000190734863, avg_cost: 0.21435297553272284\n",
            "**************************************************\n",
            "test_acc: 0.9296000009775162\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.714831744631132e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005860502882472551\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011503835978995385\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.016491120845700313\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.023585652267211123\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.03182387047350251\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.9900000095367432, avg_cost: 0.039985982528693634\n",
            "**************************************************\n",
            "test_acc: 0.9229000002145767\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 7.503458609183629e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005160309756950786\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.009986662194326831\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.017677490557931986\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.02742482368931328\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.037258533452210627\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.9800000190734863, avg_cost: 0.04665523556390934\n",
            "**************************************************\n",
            "test_acc: 0.9216000008583068\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.126316592097283e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.007253071791686428\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.014411427090332543\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.027622089604847144\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.0920873723637002\n",
            "batch: 500, train_acc: 0.8700000047683716, avg_cost: 0.17474780246227345\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9900000095367432, avg_cost: 0.22697042010491694\n",
            "**************************************************\n",
            "test_acc: 0.911700000166893\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.563871184984843e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.015632512813899664\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.02679218661408714\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.03397213335498233\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.04050534102638871\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.04805747605064727\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9800000190734863, avg_cost: 0.05680983753700274\n",
            "**************************************************\n",
            "test_acc: 0.9220000046491623\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 7.330136994520824e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0032321423784014763\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005971544649413161\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.008382372094783925\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01203823512150847\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.017341542082940586\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 1.0, avg_cost: 0.022915091820153388\n",
            "**************************************************\n",
            "test_acc: 0.9230000030994415\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 6.761778766910235e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0035740326131538792\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.007584341204346856\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0114177423477425\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01705006314143248\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.027631163506424247\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9599999785423279, avg_cost: 0.043443307423464535\n",
            "**************************************************\n",
            "test_acc: 0.9049000036716461\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0003632429490486781\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.03637354676437098\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.1579140740460328\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.21108837697790772\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.2299836677698962\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.2423513595951955\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9900000095367432, avg_cost: 0.25079670004372046\n",
            "**************************************************\n",
            "test_acc: 0.9280000030994415\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 6.803921734293302e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.00475144272970889\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.008314590754792631\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.012569535751050952\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.019368431759988484\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02446280951471028\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.9900000095367432, avg_cost: 0.030191165815170565\n",
            "**************************************************\n",
            "test_acc: 0.9272000002861023\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.8094365522265435e-06\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002334249893368299\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005512297466799886\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.009524576049467819\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013556216878763117\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01791731874451215\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.9900000095367432, avg_cost: 0.02251244361301361\n",
            "**************************************************\n",
            "test_acc: 0.9249000012874603\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.195578844596942e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002887296224071178\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.005498416526024812\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010201890564809824\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.015501150046645007\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.022517738984679465\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.9800000190734863, avg_cost: 0.028231333979359383\n",
            "**************************************************\n",
            "test_acc: 0.9219000041484833\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 7.079495737950007e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.007241880834432477\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.017373104087552438\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.07407380055854447\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.26250799776583644\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.32146264131088764\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.9700000286102295, avg_cost: 0.34256652719443054\n",
            "**************************************************\n",
            "test_acc: 0.9157000035047531\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.0001595854883392652\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009637819903922111\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.015907701307284392\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.020560867570942113\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.02787586698502613\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03242815535879345\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9900000095367432, avg_cost: 0.036325049177733784\n",
            "**************************************************\n",
            "test_acc: 0.9253000020980835\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 1.8463127004603546e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.001275105502542525\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0032773493666597158\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.005305632117588171\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.008043652274200213\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.011082961420540115\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.9900000095367432, avg_cost: 0.01494272288276685\n",
            "**************************************************\n",
            "test_acc: 0.924799998998642\n",
            "batch: 0, train_acc: 1.0, avg_cost: 8.87170433998108e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002661717642128375\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0062473443199905845\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011014164962807618\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.016951700181255844\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.03851967158690665\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.9900000095367432, avg_cost: 0.05101451007049945\n",
            "**************************************************\n",
            "test_acc: 0.9202000027894974\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 6.682669123013814e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.008536867604270812\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.015376318891988682\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.021846519851436222\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.030195358848820138\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03531629606348967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZKhqlP5OqS",
        "colab_type": "text"
      },
      "source": [
        "# Restore : ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqE1cPjud7OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0813_BN_100epoch_mod.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0813_BN_50epoch_mod_v2.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 128, keep_prob, '1b', batch_prob)\n",
        "h1_1 = tf.layers.batch_normalization(h1_1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,512]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList11 = [512,512,512]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [512,512,512]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [512,512,1024]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "\n",
        "h15 = tf.nn.avg_pool(h14, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE14kAK65TNq",
        "colab_type": "code",
        "outputId": "a9698fba-7754-4dbc-9fd0-02aae68d71c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0813 12:14:34.407257 139731626665856 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.3594039355715115e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.006564120243225869\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.012275049300709122\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.019494537977831596\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.02573109921310486\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.03473658621708943\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.9800000190734863, avg_cost: 0.044063390704589735\n",
            "**************************************************\n",
            "test_acc: 0.9285000020265579\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.823817069331805e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.005744013083555426\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011082608808840941\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.01736994489086405\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.024986772895305577\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.032934001492976685\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.9900000095367432, avg_cost: 0.042958695234362194\n",
            "**************************************************\n",
            "test_acc: 0.9063000023365021\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.6871742233633996e-05\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.009301771179113226\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.01963265298196347\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.03311022651721334\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.04849819776592389\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.07322711710187528\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.8999999761581421, avg_cost: 0.1022818606392441\n",
            "**************************************************\n",
            "test_acc: 0.888400001525879\n",
            "batch: 0, train_acc: 0.8799999952316284, avg_cost: 0.0008009669681390126\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.07290826941064246\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.10479098512791094\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.12391609558408771\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.13559337027797794\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.14485219582992906\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.9700000286102295, avg_cost: 0.1554936141998041\n",
            "**************************************************\n",
            "test_acc: 0.9251000010967254\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 6.546299904584885e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003572279870665322\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.006773597080367229\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.010047866893534476\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013520698726885418\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.01877083247042416\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 1.0, avg_cost: 0.0239306476685063\n",
            "**************************************************\n",
            "test_acc: 0.9268999999761581\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 6.084563210606575e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003541863833233945\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00642705894608904\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011030793807197667\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.018037725497658048\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02681458497177421\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9700000286102295, avg_cost: 0.04008837388314227\n",
            "**************************************************\n",
            "test_acc: 0.9163000029325485\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00024123397966225942\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.010113796817652959\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.02385720829755884\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.040080535101830427\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.10116423005747495\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.2754991995035379\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.949999988079071, avg_cost: 0.32677469012215926\n",
            "**************************************************\n",
            "test_acc: 0.9114000004529953\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.7381715675195058e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.014670012573090696\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.02598332097419188\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.034411198488038856\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.041169083964923636\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.047330746420533004\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9900000095367432, avg_cost: 0.05357477208342364\n",
            "**************************************************\n",
            "test_acc: 0.9293000042438507\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.828001596654455e-06\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002158453614926354\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.004214588175964311\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.006292523965836764\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.007905882010700234\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.009822736653856448\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9800000190734863, avg_cost: 0.012673181735447864\n",
            "**************************************************\n",
            "test_acc: 0.9284999984502792\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00013486846039692562\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0024246897946371363\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006903287647576993\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010522123988624416\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.014783638655468771\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.019854355961403287\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 1.0, avg_cost: 0.025208527238652682\n",
            "**************************************************\n",
            "test_acc: 0.9258999979496002\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.9685371555387975e-06\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.004178943103312728\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.008342419077077159\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.013140167565373593\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.01882487662421528\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02974531633409545\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9300000071525574, avg_cost: 0.04943937079223057\n",
            "**************************************************\n",
            "test_acc: 0.9158000022172927\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 6.006135915716489e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.02952331605852427\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.08230097944246756\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.11851425370453697\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.13404574621051624\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.14657087967158688\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.9900000095367432, avg_cost: 0.16033816654535868\n",
            "**************************************************\n",
            "test_acc: 0.9218000024557114\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 3.795489358405272e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.00367254042923984\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006739506506928591\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010794291667383117\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.015790533713079632\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.019042641949975705\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 1.0, avg_cost: 0.02478999066035613\n",
            "**************************************************\n",
            "test_acc: 0.926100001335144\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.399102569247286e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0032612523135321664\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.006520179442431978\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.009414654609129979\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.014250522667850103\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.01905545625150503\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.9900000095367432, avg_cost: 0.024330404868378515\n",
            "**************************************************\n",
            "test_acc: 0.9235000020265579\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 7.952453568577767e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.005941945199059166\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.012339813045897853\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.02681704682114892\n",
            "batch: 400, train_acc: 0.8799999952316284, avg_cost: 0.11999923339570764\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.17076507754576595\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.9900000095367432, avg_cost: 0.19642835714224935\n",
            "**************************************************\n",
            "test_acc: 0.9206000018119812\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.804900373021761e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.005679114826537746\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.013353063277008907\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.02045969450294553\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.024487718061300245\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.029558171550897887\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 1.0, avg_cost: 0.03533540137167617\n",
            "**************************************************\n",
            "test_acc: 0.9288999992609024\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.774680369844039e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003151125311933356\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.005153787123032694\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.008155856053591986\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01038164393623144\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01349225705150579\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 1.0, avg_cost: 0.016540731801960654\n",
            "**************************************************\n",
            "test_acc: 0.9281000036001206\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.7869051943222683e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0020487252445521623\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.004780537748677788\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.006615368177432774\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.009521557763106705\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.022285078862214498\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9700000286102295, avg_cost: 0.06483225764830723\n",
            "**************************************************\n",
            "test_acc: 0.9121000009775162\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00034848277767499287\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.07208848959223059\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.11633075414380688\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.13891027445672094\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.1523561339128356\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.16240635580377183\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9700000286102295, avg_cost: 0.17003672968817227\n",
            "**************************************************\n",
            "test_acc: 0.9278000009059906\n",
            "batch: 0, train_acc: 1.0, avg_cost: 6.3152448274195195e-06\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.004404761909433242\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0070418215851646\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.009416125566566886\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.012200262609248967\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.015060916691198445\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 1.0, avg_cost: 0.01755761008525798\n",
            "**************************************************\n",
            "test_acc: 0.9303000026941299\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.8208883789678414e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002536385343938624\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.005967858285616309\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.008408194382715142\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.011823189170782962\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.01644499002340231\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 1.0, avg_cost: 0.02066225486629672\n",
            "**************************************************\n",
            "test_acc: 0.9272000020742417\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.7649365064377586e-07\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.004559773933360703\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00940944370230075\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.014986092613492775\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.02000371992693469\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.029877594222258122\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.9800000190734863, avg_cost: 0.038233600509956125\n",
            "**************************************************\n",
            "test_acc: 0.921599999666214\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.4043340235948563e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.0074882302248321765\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.016390275315795105\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.025848615219377587\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03393376217163075\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.04177679881043637\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9900000095367432, avg_cost: 0.051324576336652813\n",
            "**************************************************\n",
            "test_acc: 0.922800001502037\n",
            "batch: 0, train_acc: 1.0, avg_cost: 8.118811529129743e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.010388023178420554\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.022125712751925073\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.04776067683740015\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.14565605413125018\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.23335238006446157\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9900000095367432, avg_cost: 0.26854828939981695\n",
            "**************************************************\n",
            "test_acc: 0.9228000020980835\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.0003077072401841482\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.009142035176525188\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.015014281537028178\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.02256338239094324\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.02861430935987149\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03281003644206184\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.9800000190734863, avg_cost: 0.03744375750965869\n",
            "**************************************************\n",
            "test_acc: 0.9275000017881393\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.470999697844188e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.001540119322537521\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0032633122230472135\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.005906443958466714\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.009335183553009566\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.012887766523411708\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 1.0, avg_cost: 0.017614270821842894\n",
            "**************************************************\n",
            "test_acc: 0.9305000007152557\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.048026451220115e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0032844190273280563\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006822421031943216\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010200580012423662\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013874385668255046\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01700098858577434\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9900000095367432, avg_cost: 0.02296669897090898\n",
            "**************************************************\n",
            "test_acc: 0.925699999332428\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.3669766960665583e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.007391537674981618\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.013638489542281609\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.021665635093529534\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.029129579534662747\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.03787454750427666\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9900000095367432, avg_cost: 0.05256202662709108\n",
            "**************************************************\n",
            "test_acc: 0.9178000044822693\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00017626597235600154\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.024888125042953107\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.05137532511407397\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.06683997429036026\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.08300773279538891\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.0930468143261892\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.9900000095367432, avg_cost: 0.10595292783353771\n",
            "**************************************************\n",
            "test_acc: 0.9193000042438507\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00015058829138676324\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005853217934561221\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.013167217787323389\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.01853945508227923\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.027663739127842556\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.04085486032798958\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9900000095367432, avg_cost: 0.051708773204043344\n",
            "**************************************************\n",
            "test_acc: 0.9254000025987625\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00011142197996377945\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.006502009474850372\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.011338335603387569\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.015871482168965556\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.021153453987163003\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02600774618945735\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9599999785423279, avg_cost: 0.03338157553635156\n",
            "**************************************************\n",
            "test_acc: 0.9229000025987625\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.0034153092419728e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005582980136636783\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011737511826302506\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.01649020084589836\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.023883414671147053\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.036874429446276814\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.9900000095367432, avg_cost: 0.054518026489865964\n",
            "**************************************************\n",
            "test_acc: 0.9192000019550324\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.1160445262988406e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.04378177565363086\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.08293276209166596\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.10831007743310765\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.12190246204174245\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.13244830827828333\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 1.0, avg_cost: 0.1380353370169688\n",
            "**************************************************\n",
            "test_acc: 0.9287000036239624\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.5519239696053165e-07\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0031139721585096447\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.007047544942069046\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.009807629038145466\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.014008711606476355\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.017099219239470262\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 1.0, avg_cost: 0.019360090110148345\n",
            "**************************************************\n",
            "test_acc: 0.9282000011205673\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.5238136559977042e-08\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.003216431080113343\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00576581572061152\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.009414610137960341\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013147524002359306\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.019330644366850128\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.9800000190734863, avg_cost: 0.02609624862362964\n",
            "**************************************************\n",
            "test_acc: 0.92280000269413\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.2550186258740724e-07\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.008344629171261792\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.015919660261846037\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.023659097202041166\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.031013727989751293\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.04107663463187929\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9700000286102295, avg_cost: 0.05238254422406169\n",
            "**************************************************\n",
            "test_acc: 0.9230000042915344\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.725895511607329e-07\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.007660744165954819\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.01838174868811195\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.03429545233445871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b9c45522e797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3HV7puWnmMA",
        "colab_type": "text"
      },
      "source": [
        "# ResNet : Restore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byaf3IFL5qq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0813_BN_50epoch_mod_v2.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_150epoch_best_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_150epoch_best_loss.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 64, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1_1 = make_conv_once_wo_pooling(h1, 128, keep_prob, '1b', batch_prob)\n",
        "h1_1 = tf.layers.batch_normalization(h1_1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1_1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,512]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList11 = [512,512,512]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [512,512,512]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [512,512,1024]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "\n",
        "h15 = tf.nn.avg_pool(h14, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,1024])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTV8_nFYofmm",
        "colab_type": "code",
        "outputId": "a2532fc9-1d90-4bea-b9ea-28fa1383bc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 1.0, avg_cost: 6.078304334854086e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002586844466014214\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.005905651690203326\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011454919537374627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn2pAM3sc5fj",
        "colab_type": "text"
      },
      "source": [
        "# ResNet : for 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GjmVl0Cqt3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_loss.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 128, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,256]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "h12 = tf.nn.avg_pool(h11, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h12_flat = tf.reshape(h12, [-1,256])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h12_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVxHn5c9dlHz",
        "colab_type": "code",
        "outputId": "157a05c6-715f-4328-b0c1-900965bdc3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.09000000357627869, avg_cost: 0.0061059157053629555\n",
            "batch: 100, train_acc: 0.6899999976158142, avg_cost: 0.8036426612734793\n",
            "batch: 200, train_acc: 0.7900000214576721, avg_cost: 0.9371734351913129\n",
            "batch: 300, train_acc: 0.8199999928474426, avg_cost: 1.0549825007716815\n",
            "batch: 400, train_acc: 0.8100000023841858, avg_cost: 1.167238837381204\n",
            "batch: 500, train_acc: 0.7900000214576721, avg_cost: 1.273420827388764\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.8500000238418579, avg_cost: 1.3607737915714595\n",
            "**************************************************\n",
            "test_acc: 0.8178999990224838\n",
            "batch: 0, train_acc: 0.8399999737739563, avg_cost: 0.0007805954416592916\n",
            "batch: 100, train_acc: 0.8600000143051147, avg_cost: 0.07833048932254313\n",
            "batch: 200, train_acc: 0.8500000238418579, avg_cost: 0.15970885174969832\n",
            "batch: 300, train_acc: 0.8899999856948853, avg_cost: 0.23530263818800445\n",
            "batch: 400, train_acc: 0.7599999904632568, avg_cost: 0.30812071991463497\n",
            "batch: 500, train_acc: 0.8600000143051147, avg_cost: 0.3760294423749049\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.8799999952316284, avg_cost: 0.44090523242950463\n",
            "**************************************************\n",
            "test_acc: 0.8786999994516372\n",
            "batch: 0, train_acc: 0.8999999761581421, avg_cost: 0.00041339593629042306\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.05921478110055129\n",
            "batch: 200, train_acc: 0.8799999952316284, avg_cost: 0.12253178641200067\n",
            "batch: 300, train_acc: 0.8399999737739563, avg_cost: 0.19056601288417976\n",
            "batch: 400, train_acc: 0.8899999856948853, avg_cost: 0.2481014196077984\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.3085251950720949\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.9200000166893005, avg_cost: 0.3643297523756825\n",
            "**************************************************\n",
            "test_acc: 0.8818000000715256\n",
            "batch: 0, train_acc: 0.8999999761581421, avg_cost: 0.0004161204397678375\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.059946963712573076\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.11221135169267657\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.16263384251544885\n",
            "batch: 400, train_acc: 0.8799999952316284, avg_cost: 0.20929261601219587\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.2558564002936085\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.8799999952316284, avg_cost: 0.30300993002951143\n",
            "**************************************************\n",
            "test_acc: 0.8738000011444091\n",
            "batch: 0, train_acc: 0.8899999856948853, avg_cost: 0.0004336616396903992\n",
            "batch: 100, train_acc: 0.8399999737739563, avg_cost: 0.04829378165304661\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.09160617041091122\n",
            "batch: 300, train_acc: 0.8899999856948853, avg_cost: 0.13858381313582258\n",
            "batch: 400, train_acc: 0.9100000262260437, avg_cost: 0.18556706479440127\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.23409495080510775\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.9399999976158142, avg_cost: 0.27631424972166635\n",
            "**************************************************\n",
            "test_acc: 0.8862000000476837\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.00046072378754615786\n",
            "batch: 100, train_acc: 0.8700000047683716, avg_cost: 0.04441692246745032\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.08613477176676197\n",
            "batch: 300, train_acc: 0.8700000047683716, avg_cost: 0.12803601493438085\n",
            "batch: 400, train_acc: 0.8799999952316284, avg_cost: 0.17026156891758237\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.21005107875292495\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9300000071525574, avg_cost: 0.2505513610504566\n",
            "**************************************************\n",
            "test_acc: 0.8951000022888184\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.0003868178029855092\n",
            "batch: 100, train_acc: 0.8500000238418579, avg_cost: 0.04009774460146825\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.07720169724275666\n",
            "batch: 300, train_acc: 0.8399999737739563, avg_cost: 0.11464580873648324\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.15182072491695495\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.19047157338509974\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.9100000262260437, avg_cost: 0.2310065942754351\n",
            "**************************************************\n",
            "test_acc: 0.8952999991178513\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0003312319020430247\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.034007853704194235\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.07070987080534302\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.10496628159036245\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.14201503614584615\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.1789896167938909\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9599999785423279, avg_cost: 0.2162907440898321\n",
            "**************************************************\n",
            "test_acc: 0.9134000009298324\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.0002035601809620857\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.03400685798376798\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.06810706680019693\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.10188560626159104\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.13538237990811466\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.16766948838407794\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9300000071525574, avg_cost: 0.20171198314055813\n",
            "**************************************************\n",
            "test_acc: 0.9107000011205674\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.0004977899293104808\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.030518981628119942\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.05967010964949928\n",
            "batch: 300, train_acc: 0.9100000262260437, avg_cost: 0.09037672484293584\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.1230943761641781\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.1555408851553996\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.8899999856948853, avg_cost: 0.1885084178981681\n",
            "**************************************************\n",
            "test_acc: 0.9073000025749206\n",
            "[0, 0.8178999990224838, 0.8786999994516372, 0.8818000000715256, 0.8862000000476837, 0.8951000022888184, 0.8952999991178513, 0.9134000009298324]\n",
            "[100, 1.3607737915714595, 0.44090523242950463, 0.3643297523756825, 0.30300993002951143, 0.27631424972166635, 0.2505513610504566, 0.2310065942754351, 0.2162907440898321, 0.20171198314055813, 0.1885084178981681]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfGU1GibofRr",
        "colab_type": "text"
      },
      "source": [
        "# Restore : 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdrAi8fjofD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "1. 맨 처음 conv layer를 64 / 128 => 128 단층으로 전환\n",
        "2. #filter를 기존의 128-256-512-1024 => 128-256-256으로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 40\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_bst_acc.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 128, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,256]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "h12 = tf.nn.avg_pool(h11, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h12_flat = tf.reshape(h12, [-1,256])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h12_flat,fcw1) + fcb1\n",
        "# logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1XigIAThv-Z",
        "colab_type": "code",
        "outputId": "d8f76b0c-ba3f-4727-d302-4bc6327e42a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0814 00:58:49.596909 140036658886528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00037185393273830413\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.03114394760380188\n",
            "batch: 200, train_acc: 0.9100000262260437, avg_cost: 0.06337655426313481\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.09797682924816999\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.12937414211531464\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.1645563122754294\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.9200000166893005, avg_cost: 0.19895237430309234\n",
            "**************************************************\n",
            "test_acc: 0.8990999984741211\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0003798498461643855\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.033400914122660955\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.0648000725731254\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.09450658099725856\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.12830489840979398\n",
            "batch: 500, train_acc: 0.8500000238418579, avg_cost: 0.16129126903290572\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.949999988079071, avg_cost: 0.1948283650539817\n",
            "**************************************************\n",
            "test_acc: 0.9101999998092651\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0002473362535238266\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.027727616503834717\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.056803386819859365\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.08694472375015425\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.11636336038510015\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.1453181072945396\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.9300000071525574, avg_cost: 0.17559393061945827\n",
            "**************************************************\n",
            "test_acc: 0.9050999993085861\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0002518148223559062\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.0267825593923529\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.05711550727486609\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.08479675879081085\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.11184278137360998\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.1439722859983642\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.9900000095367432, avg_cost: 0.172177161971728\n",
            "**************************************************\n",
            "test_acc: 0.9131000012159347\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.0003195269157489141\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.024825321373840175\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.04896988676860929\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.07448492910712956\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.09983461180701854\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.12251861600826185\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.9700000286102295, avg_cost: 0.14865885332226764\n",
            "**************************************************\n",
            "test_acc: 0.9060000014305115\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.0002536096672217051\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.025411822696526854\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.04548717812635008\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.06777097576297819\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.09190936721240477\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.11581679444139195\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9399999976158142, avg_cost: 0.1432239948275188\n",
            "**************************************************\n",
            "test_acc: 0.9132000011205673\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.0001615849509835243\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.02241822366913159\n",
            "batch: 200, train_acc: 0.8899999856948853, avg_cost: 0.0444889405121406\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.066088316620638\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.08883931529087331\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.1099191463335108\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.949999988079071, avg_cost: 0.1296765967148046\n",
            "**************************************************\n",
            "test_acc: 0.9006000006198883\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0002506641298532486\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.01781189834854255\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.0384835588792339\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.06200956746780625\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.08069850179521991\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.09974033510157226\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9800000190734863, avg_cost: 0.12184586208468934\n",
            "**************************************************\n",
            "test_acc: 0.9122000020742417\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0002141923705736796\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.020209706605722504\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.03657215018135804\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.05342371374523882\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.07116893001055961\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.09271364919065182\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9599999785423279, avg_cost: 0.11257463848683982\n",
            "**************************************************\n",
            "test_acc: 0.9210000038146973\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00022663543621699014\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.013334867522741354\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.026873806489941976\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.04300935886334627\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.05882670927637565\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.07506332691293202\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.9800000190734863, avg_cost: 0.09447408926828443\n",
            "**************************************************\n",
            "test_acc: 0.9056000012159348\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00030613765120506286\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.012798023610375816\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02566021588786195\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.038871857655855525\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.05223622954450545\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.06934106778819116\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9900000095367432, avg_cost: 0.08442050579624877\n",
            "**************************************************\n",
            "test_acc: 0.9241000056266785\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.6973652119437856e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009787665017647667\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02292725920250328\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.03649663619115016\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.049270835807935016\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.0622659498457021\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.9700000286102295, avg_cost: 0.07452784674280946\n",
            "**************************************************\n",
            "test_acc: 0.9189000010490418\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0003137086828549703\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009982517894047001\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.01957024072762581\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.02986503667353345\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.040454247498419194\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.05359881809912628\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.949999988079071, avg_cost: 0.0685985420644284\n",
            "**************************************************\n",
            "test_acc: 0.9195000010728837\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.4505549420913062e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009444721427280455\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.020978957413074872\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.03280340201687068\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.041688309701470055\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.05237879728355135\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 1.0, avg_cost: 0.06382791464449837\n",
            "**************************************************\n",
            "test_acc: 0.9147000014781952\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00013480714211861294\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009722760558361186\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.02054093095667971\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.03139990717677091\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.04182482226247281\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.052657122599193834\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 1.0, avg_cost: 0.06103638757214259\n",
            "**************************************************\n",
            "test_acc: 0.9195000022649765\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 5.903639520208041e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.008144800061127168\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.014161354335956278\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.023801030168930693\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03326204581962275\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04112456756682753\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.9800000190734863, avg_cost: 0.050056872781133306\n",
            "**************************************************\n",
            "test_acc: 0.911000002026558\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 9.70412107805411e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.005886487210518678\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.0130409144066895\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.024866029412563252\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03321827561187089\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.04548115143921066\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.9900000095367432, avg_cost: 0.054242292123963046\n",
            "**************************************************\n",
            "test_acc: 0.9205000030994416\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.2387423863013584e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.003341312218593278\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008492677914327939\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.013371605856130666\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.021087086367673074\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.03141422889379706\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9599999785423279, avg_cost: 0.040617524694049854\n",
            "**************************************************\n",
            "test_acc: 0.916900001168251\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 8.66919383406639e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.006288593694334847\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.01055829386576079\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0155649559321076\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.023334062086360068\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.03052425411772371\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9900000095367432, avg_cost: 0.0366508000488587\n",
            "**************************************************\n",
            "test_acc: 0.9134000039100647\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.703464642167092e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.006411114054183901\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.01379466981896257\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.024757560581759507\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.035240164657564785\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.043473569405396095\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.9900000095367432, avg_cost: 0.050011937131227244\n",
            "**************************************************\n",
            "test_acc: 0.92230000436306\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.5613818541169166e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0037823412736179307\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.00706064496247563\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.01077074268182816\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01707823319059873\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02618941040952145\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.9800000190734863, avg_cost: 0.031996321470748317\n",
            "**************************************************\n",
            "test_acc: 0.9225000041723251\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.413286879658699e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.005058062033494937\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.01116509195135828\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.018141992177988862\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.022231779459931825\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02817978397142724\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 1.0, avg_cost: 0.03583866359263385\n",
            "**************************************************\n",
            "test_acc: 0.9217000025510788\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.7824795494476955e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0033821088578163965\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.007350991226461101\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.014094025583181066\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.019799493693183963\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.02516424439677698\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9800000190734863, avg_cost: 0.03218921877642667\n",
            "**************************************************\n",
            "test_acc: 0.9205000030994416\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 1.8826206214725972e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.004569937115205293\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.008139216975105227\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.014548671333808065\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.02106774456347921\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.027777842989841393\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9900000095367432, avg_cost: 0.0443418674143322\n",
            "**************************************************\n",
            "test_acc: 0.9139000016450882\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.102463833987713e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.006040550111307916\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.010068289296056413\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.015784439051033886\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.02183925332077101\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.026102074947290634\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 1.0, avg_cost: 0.029774599306183525\n",
            "**************************************************\n",
            "test_acc: 0.9235000014305115\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.87525069527328e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.002299029073086179\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005496134837837115\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.012440246882227564\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01871683581790422\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.022742807783458083\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 1.0, avg_cost: 0.02668254535344509\n",
            "**************************************************\n",
            "test_acc: 0.9180000019073487\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.0237316687901813e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.006012799344704643\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.012427345504984262\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.018004962344324058\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.021973301907940657\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.026105431941869\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9800000190734863, avg_cost: 0.03039530560382745\n",
            "**************************************************\n",
            "test_acc: 0.9218000018596649\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.6147213367124397e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0037295404364219085\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.006539452376249154\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010808252760070898\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.016366764990416414\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.021747807466830034\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9900000095367432, avg_cost: 0.02841674775732826\n",
            "**************************************************\n",
            "test_acc: 0.9188000011444092\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.924987278878689e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0038408209191402413\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006445716001398974\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.010873969207556606\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01610207851949477\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.022532932772034362\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 1.0, avg_cost: 0.0270496890042705\n",
            "**************************************************\n",
            "test_acc: 0.9195000016689301\n",
            "batch: 0, train_acc: 1.0, avg_cost: 6.643285450991243e-07\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.003649950792435752\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.00746852941687394\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.010429121727108102\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.013706396076055793\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02048825212873759\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9900000095367432, avg_cost: 0.026729745805817738\n",
            "**************************************************\n",
            "test_acc: 0.9179000008106232\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 9.726357956727345e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003720907267694807\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008932924635143228\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.012257420750890253\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.014771659228887669\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.01820806561399876\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9700000286102295, avg_cost: 0.025118603507368473\n",
            "**************************************************\n",
            "test_acc: 0.9203000050783158\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00010006737584869067\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.0039942739190397935\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.007029663390154985\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010644330681219187\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.015018636889872134\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02009611168915094\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 1.0, avg_cost: 0.023763127301450956\n",
            "**************************************************\n",
            "test_acc: 0.9210000032186508\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.524591214954853e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.0018682585217114443\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005622206830867071\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010741321652149055\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01765767312452834\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.023419295969112387\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.9800000190734863, avg_cost: 0.02947107032608984\n",
            "**************************************************\n",
            "test_acc: 0.9090000015497207\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00010050339624285698\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0034928956958234863\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.007987013342093027\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.011863727641872776\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.015443263013178993\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01939169648256817\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.9800000190734863, avg_cost: 0.024483288950899188\n",
            "**************************************************\n",
            "test_acc: 0.921000002026558\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00014546025544404984\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.004005511279525915\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.006962503103786731\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.009833529490697403\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.013003823984542277\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.015561816758330073\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.9800000190734863, avg_cost: 0.02164134162925317\n",
            "**************************************************\n",
            "test_acc: 0.9216000002622604\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 6.759518757462501e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0022257782614481885\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.004272213070944417\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.0058748929590365095\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.010035770698365615\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.013782465173244426\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9800000190734863, avg_cost: 0.021138029141208478\n",
            "**************************************************\n",
            "test_acc: 0.9204000002145767\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 4.27473708987236e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0050130389775343565\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.00995793058740674\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.014373630759376588\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.017290080378600896\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.020431512704635692\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.9900000095367432, avg_cost: 0.02465695804390636\n",
            "**************************************************\n",
            "test_acc: 0.9242000007629394\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.2409892020126182e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0018512781922011812\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.0030625984200772375\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.004601660643308299\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.006386235096921758\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.009835074393054505\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.9900000095367432, avg_cost: 0.014266575661657723\n",
            "**************************************************\n",
            "test_acc: 0.9206000018119812\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.1963191082080205e-06\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.015021633373798983\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.022842086153796116\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.02917824988526263\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.032499098972560495\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03508609147402541\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 1.0, avg_cost: 0.038076024836770195\n",
            "**************************************************\n",
            "test_acc: 0.9268000030517578\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.952146507799625e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0022137547269479307\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.004094923937388256\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.006884022437091817\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.009426581876490063\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.012123142053675713\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.9900000095367432, avg_cost: 0.015567500140274809\n",
            "**************************************************\n",
            "test_acc: 0.9220000028610229\n",
            "[0, 0.8990999984741211, 0.9101999998092651, 0.9131000012159347, 0.9132000011205673, 0.9210000038146973, 0.9241000056266785, 0.9242000007629394, 0.9268000030517578]\n",
            "[100, 0.19895237430309234, 0.1948283650539817, 0.17559393061945827, 0.172177161971728, 0.14865885332226764, 0.1432239948275188, 0.1296765967148046, 0.12184586208468934, 0.11257463848683982, 0.09447408926828443, 0.08442050579624877, 0.07452784674280946, 0.0685985420644284, 0.06382791464449837, 0.06103638757214259, 0.050056872781133306, 0.040617524694049854, 0.0366508000488587, 0.031996321470748317, 0.029774599306183525, 0.02668254535344509, 0.025118603507368473, 0.023763127301450956, 0.02164134162925317, 0.021138029141208478, 0.014266575661657723]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLw33_uio35D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Czy0KjNDTc2",
        "colab_type": "text"
      },
      "source": [
        "# ResNet : for 10 classes : mod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39-pjlnnDn_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "1. 맨 처음 conv layer를 64 / 128 => 128 단층으로 전환\n",
        "2. #filter를 기존의 128-256-512-1024 => 128-256-256-256으로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_long_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_long_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_long_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 128, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [128,128,128]\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "iList2 = [128,128,128]\n",
        "h3 = my_resNet_model(h2, keep_prob, batch_prob, iList2, 3)\n",
        "iList3 = [128,128,128]\n",
        "h4 = my_resNet_model(h3, keep_prob, batch_prob, iList3, 4)\n",
        "iList4 = [128,128,256]\n",
        "h5 = my_resNet_model(h4, keep_prob, batch_prob, iList4, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "\n",
        "iList5 = [256,256,256]\n",
        "h6 = my_resNet_model(h5, keep_prob, batch_prob, iList5, 6)\n",
        "iList6 = [256,256,256]\n",
        "h7 = my_resNet_model(h6, keep_prob, batch_prob, iList6, 7)\n",
        "iList7 = [256,256,256]\n",
        "h8 = my_resNet_model(h7, keep_prob, batch_prob, iList7, 8)\n",
        "iList8 = [256,256,256]\n",
        "h9 = my_resNet_model(h8, keep_prob, batch_prob, iList8, 9)\n",
        "iList9 = [256,256,256]\n",
        "h10 = my_resNet_model(h9, keep_prob, batch_prob, iList9, 10)\n",
        "iList10 = [256,256,256]\n",
        "h11 = my_resNet_model(h10, keep_prob, batch_prob, iList10, 11)\n",
        "h11 = tf.nn.max_pool(h11, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "iList11 = [256,256,256]\n",
        "h12 = my_resNet_model(h11, keep_prob, batch_prob, iList11, 12)\n",
        "iList12 = [256,256,256]\n",
        "h13 = my_resNet_model(h12, keep_prob, batch_prob, iList12, 13)\n",
        "iList13 = [256,256,256]\n",
        "h14 = my_resNet_model(h13, keep_prob, batch_prob, iList13, 14)\n",
        "iList14 = [256,256,256]\n",
        "h15 = my_resNet_model(h14, keep_prob, batch_prob, iList14, 15)\n",
        "h15 = tf.nn.avg_pool(h15, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h15_flat = tf.reshape(h15, [-1,256])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h15_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxs9FySRDqwr",
        "colab_type": "code",
        "outputId": "7bbb57f3-28bd-429b-897e-c38e8ff71ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.10999999940395355, avg_cost: 0.015770867665608725\n",
            "batch: 100, train_acc: 0.30000001192092896, avg_cost: 1.638140858610471\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 1.9410578219095875\n",
            "batch: 300, train_acc: 0.4099999964237213, avg_cost: 2.2612243402004237\n",
            "batch: 400, train_acc: 0.44999998807907104, avg_cost: 2.5306990500291184\n",
            "batch: 500, train_acc: 0.4300000071525574, avg_cost: 2.768920793135959\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.46000000834465027, avg_cost: 3.017639006177583\n",
            "**************************************************\n",
            "test_acc: 0.7518999975919723\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0021158464749654136\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.24698745071887968\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.4783831691741942\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.69207024137179\n",
            "batch: 400, train_acc: 0.41999998688697815, avg_cost: 0.9414771817127862\n",
            "batch: 500, train_acc: 0.5, avg_cost: 1.15351056009531\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.4699999988079071, avg_cost: 1.404901708662509\n",
            "**************************************************\n",
            "test_acc: 0.8286999988555909\n",
            "batch: 0, train_acc: 0.46000000834465027, avg_cost: 0.002402584155400594\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.23076161702473955\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.4357388820250825\n",
            "batch: 300, train_acc: 0.41999998688697815, avg_cost: 0.6554819491505616\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.8620366423328706\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 1.0609654687841727\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.5099999904632568, avg_cost: 1.251403709053993\n",
            "**************************************************\n",
            "test_acc: 0.8551000022888183\n",
            "batch: 0, train_acc: 0.47999998927116394, avg_cost: 0.002077165643374125\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.202728397945563\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.4072437344988189\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.6000296446681023\n",
            "batch: 400, train_acc: 0.49000000953674316, avg_cost: 0.8050845730304714\n",
            "batch: 500, train_acc: 0.4300000071525574, avg_cost: 1.031319682300091\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5899999737739563, avg_cost: 1.2207689012090364\n",
            "**************************************************\n",
            "test_acc: 0.8815000033378602\n",
            "batch: 0, train_acc: 0.6800000071525574, avg_cost: 0.0014529165625572205\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.18916065076986951\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.37689770708481496\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.5687159443895025\n",
            "batch: 400, train_acc: 0.4399999976158142, avg_cost: 0.7572735701004669\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.9546877368291223\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.5099999904632568, avg_cost: 1.1419776825110113\n",
            "**************************************************\n",
            "test_acc: 0.8538000017404557\n",
            "batch: 0, train_acc: 0.5, avg_cost: 0.0018240092198053995\n",
            "batch: 100, train_acc: 0.49000000953674316, avg_cost: 0.19338835110267003\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.39526946494976684\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.6126144988338149\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.8173348583777736\n",
            "batch: 500, train_acc: 0.4699999988079071, avg_cost: 1.003794995148976\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.49000000953674316, avg_cost: 1.1856417795022327\n",
            "**************************************************\n",
            "test_acc: 0.8784999984502793\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0018391335010528565\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.19995595653851833\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.38848132739464486\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.5850753339131677\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.7862862166762359\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.9727709584434835\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5199999809265137, avg_cost: 1.1550508777300525\n",
            "**************************************************\n",
            "test_acc: 0.8806999999284745\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0020148664712905883\n",
            "batch: 100, train_acc: 0.4699999988079071, avg_cost: 0.1781777066985767\n",
            "batch: 200, train_acc: 0.41999998688697815, avg_cost: 0.35373177597920114\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5325596374273301\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.7107865380247436\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.885042753418287\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.5199999809265137, avg_cost: 1.0618281445900604\n",
            "**************************************************\n",
            "test_acc: 0.8999000012874603\n",
            "batch: 0, train_acc: 0.5, avg_cost: 0.0020064441363016766\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.18228699554999672\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.36420582920312866\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.5515713715553279\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.7612267033259067\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.9583191806077951\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.5699999928474426, avg_cost: 1.1469994340340284\n",
            "**************************************************\n",
            "test_acc: 0.8893000012636185\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0018856783707936605\n",
            "batch: 100, train_acc: 0.4399999976158142, avg_cost: 0.1806956070661545\n",
            "batch: 200, train_acc: 0.5, avg_cost: 0.36096897919972765\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.5399553512533509\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.7198932337760924\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.9006303190191586\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.5600000023841858, avg_cost: 1.087130457957585\n",
            "**************************************************\n",
            "test_acc: 0.9033000004291535\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0015472333629926046\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.17633515179157253\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.3509639779726663\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.5249448125561076\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6969511451323825\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.869642874499162\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.4699999988079071, avg_cost: 1.0438530356685316\n",
            "**************************************************\n",
            "test_acc: 0.8941000008583069\n",
            "batch: 0, train_acc: 0.44999998807907104, avg_cost: 0.0020092493295669556\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1738398013512293\n",
            "batch: 200, train_acc: 0.46000000834465027, avg_cost: 0.3460434289773305\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.5201209130883216\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6960791734854377\n",
            "batch: 500, train_acc: 0.49000000953674316, avg_cost: 0.8784980673591293\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.5699999928474426, avg_cost: 1.0864502201477677\n",
            "**************************************************\n",
            "test_acc: 0.8944999986886978\n",
            "batch: 0, train_acc: 0.4399999976158142, avg_cost: 0.002089980443318685\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.18929869790871937\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.36266854216655103\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.5398243293166162\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.713421855668227\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8831020758549373\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.4300000071525574, avg_cost: 1.049985561966896\n",
            "**************************************************\n",
            "test_acc: 0.9116000020503998\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0015373054146766664\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.17147190193335204\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.34001409898201623\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.5088265239199002\n",
            "batch: 400, train_acc: 0.49000000953674316, avg_cost: 0.6748083010315893\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.8441986264785122\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.5, avg_cost: 1.0480964427193\n",
            "**************************************************\n",
            "test_acc: 0.8810999965667725\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0015910937388737996\n",
            "batch: 100, train_acc: 0.49000000953674316, avg_cost: 0.17393034617106126\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.3529610632856688\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.5280270239710807\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.7010278911391895\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8700968204935389\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.6100000143051147, avg_cost: 1.0409303674101826\n",
            "**************************************************\n",
            "test_acc: 0.9181000012159347\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.001632967491944631\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.16787515660127003\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.3378973878423374\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.5074264342586201\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.6749557629227637\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.8442967733740802\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.6000000238418579, avg_cost: 1.01107532997926\n",
            "**************************************************\n",
            "test_acc: 0.9177000021934509\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0017692583799362182\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.16481999546289436\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.33014356434345243\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.49455878347158433\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.658366514742374\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8233184420069061\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.5400000214576721, avg_cost: 0.9863828338185947\n",
            "**************************************************\n",
            "test_acc: 0.9130000001192093\n",
            "batch: 0, train_acc: 0.5, avg_cost: 0.00181938370068868\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.1644966939091682\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.32635386000076927\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4923362328608831\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6579758222897844\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.8269214515884714\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.550000011920929, avg_cost: 0.9978623531262072\n",
            "**************************************************\n",
            "test_acc: 0.9047000032663345\n",
            "batch: 0, train_acc: 0.4300000071525574, avg_cost: 0.002065300941467285\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.16667006999254225\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.33026259839534755\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4978649700681367\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.6631257752577469\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.826678808232149\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.5799999833106995, avg_cost: 1.000395535131296\n",
            "**************************************************\n",
            "test_acc: 0.9031000000238418\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0017948816219965616\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.16612667977809897\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.3279706253608068\n",
            "batch: 300, train_acc: 0.6499999761581421, avg_cost: 0.4882186479369798\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6543370573719344\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.8158126272757849\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.5600000023841858, avg_cost: 0.977629222869873\n",
            "**************************************************\n",
            "test_acc: 0.9163000029325485\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.0014995211362838746\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.1626524677872657\n",
            "batch: 200, train_acc: 0.5, avg_cost: 0.3228376670678457\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4804851739605269\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.6394665254155789\n",
            "batch: 500, train_acc: 0.4699999988079071, avg_cost: 0.8020002002517379\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.6299999952316284, avg_cost: 0.9608956383665406\n",
            "**************************************************\n",
            "test_acc: 0.9129000025987625\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.00160061776638031\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1656827198465665\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.33054613153139745\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.4910135294993717\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6514801687995596\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.8125361628333729\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.6100000143051147, avg_cost: 0.968391115566095\n",
            "**************************************************\n",
            "test_acc: 0.916900002360344\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0016230495770772299\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.15782339940468465\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.31599796225627275\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.4746839722990992\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.6314535166819891\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7886273660262422\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.5799999833106995, avg_cost: 0.9439110730091733\n",
            "**************************************************\n",
            "test_acc: 0.9198000007867813\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0014027056097984313\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1544721985856692\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.3106605086723963\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.4659168907999993\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6233986434340477\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7811060761411986\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.5, avg_cost: 0.9335681736469273\n",
            "**************************************************\n",
            "test_acc: 0.916100001335144\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.001619341770807902\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.15368837902943286\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.30364415784676874\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4586776718497278\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.6137807325522106\n",
            "batch: 500, train_acc: 0.6899999976158142, avg_cost: 0.7691808048884072\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.6000000238418579, avg_cost: 0.9223622864484786\n",
            "**************************************************\n",
            "test_acc: 0.9214000004529953\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0014871355891227722\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.15452275574207305\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.3086997465292613\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.46159755359093335\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6154059460759167\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.7721577813227973\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.6200000047683716, avg_cost: 0.9267827627062799\n",
            "**************************************************\n",
            "test_acc: 0.9239000034332275\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0012282295028368632\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.15074952592452365\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.29966557105382274\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.45025955875714607\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6036099095145863\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.753790395458539\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.6600000262260437, avg_cost: 0.9062068222959828\n",
            "**************************************************\n",
            "test_acc: 0.913700001835823\n",
            "batch: 0, train_acc: 0.49000000953674316, avg_cost: 0.0018877939383188883\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.1558522060513496\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.30674700826406487\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.458607969880104\n",
            "batch: 400, train_acc: 0.6600000262260437, avg_cost: 0.6117982466022172\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.7667449500163399\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.5799999833106995, avg_cost: 0.9174059769511228\n",
            "**************************************************\n",
            "test_acc: 0.9171000021696091\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0016400579611460368\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.14986807604630795\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.3032125804821648\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4559126226107275\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.605569903651873\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.756097250978152\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.6600000262260437, avg_cost: 0.9084163015087452\n",
            "**************************************************\n",
            "test_acc: 0.9202000021934509\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.001520228683948517\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14944848964611693\n",
            "batch: 200, train_acc: 0.6899999976158142, avg_cost: 0.30029252618551244\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4508553517858186\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5964030406872428\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.746107180515924\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.5, avg_cost: 0.8948541701833401\n",
            "**************************************************\n",
            "test_acc: 0.9196000027656556\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014651512106259664\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14693583836158114\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.29522968858480453\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.44218973179658244\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.59313371181488\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7403628042340273\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.5299999713897705, avg_cost: 0.8860550942023595\n",
            "**************************************************\n",
            "test_acc: 0.9217000007629395\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0015935019652048748\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.14732343186934782\n",
            "batch: 200, train_acc: 0.49000000953674316, avg_cost: 0.29453530877828604\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.4446719249089558\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.594942417641481\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7443866803248727\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.5699999928474426, avg_cost: 0.8937611613670979\n",
            "**************************************************\n",
            "test_acc: 0.9187000024318696\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0015177741646766663\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1508337124188741\n",
            "batch: 200, train_acc: 0.6499999761581421, avg_cost: 0.29864288081725454\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.44531356424093266\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5901189726591107\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.737096958955129\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.5600000023841858, avg_cost: 0.8824167981743812\n",
            "**************************************************\n",
            "test_acc: 0.918900004029274\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016000460584958394\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14742211689551674\n",
            "batch: 200, train_acc: 0.6499999761581421, avg_cost: 0.293123881816864\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.43937651912371345\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.5893674914042158\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.7376684132218359\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.6000000238418579, avg_cost: 0.8835182647903753\n",
            "**************************************************\n",
            "test_acc: 0.9184000021219254\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0015831697980562846\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.14483190268278123\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.2932764618595439\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.44216893166303634\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5895425656437873\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7377474850416176\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.7099999785423279, avg_cost: 0.8782279849052419\n",
            "**************************************************\n",
            "test_acc: 0.922100003361702\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0014288356900215148\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14540299693743391\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.28691353013118115\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4317030168573061\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5750039392709732\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7241055431962012\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.5699999928474426, avg_cost: 0.8730565730730694\n",
            "**************************************************\n",
            "test_acc: 0.9214000016450882\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013624835014343262\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1475095549225808\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.29494732399781554\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.44069291144609474\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5819465019305549\n",
            "batch: 500, train_acc: 0.47999998927116394, avg_cost: 0.7286094313859941\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.5099999904632568, avg_cost: 0.8716764541467034\n",
            "**************************************************\n",
            "test_acc: 0.9205000013113022\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0012963007887204488\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14419780254364015\n",
            "batch: 200, train_acc: 0.6600000262260437, avg_cost: 0.28824606915315\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4345898657043774\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5793640564878784\n",
            "batch: 500, train_acc: 0.6700000166893005, avg_cost: 0.7236716280380885\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.5899999737739563, avg_cost: 0.8704980588952705\n",
            "**************************************************\n",
            "test_acc: 0.9253000038862228\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013985922932624817\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.14485152562459314\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2902237517635029\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.437274799346924\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5814979296922684\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7264783082405725\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 0.5799999833106995, avg_cost: 0.8692193537950508\n",
            "**************************************************\n",
            "test_acc: 0.9212000018358231\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0014163029193878173\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1449748000502586\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.29660932451486577\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.4426184091965356\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5879486172397927\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7313346848885214\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.6399999856948853, avg_cost: 0.8741828570763268\n",
            "**************************************************\n",
            "test_acc: 0.9249000018835067\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.001267164647579193\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.14003153105576832\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.28346143354972203\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.42653093953927385\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.5718048187096915\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7142837480703994\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 0.6200000047683716, avg_cost: 0.8579746216535572\n",
            "**************************************************\n",
            "test_acc: 0.9182000017166138\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.0014218644301096598\n",
            "batch: 100, train_acc: 0.6600000262260437, avg_cost: 0.14384229322274528\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2842536485195161\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.42645380020141616\n",
            "batch: 400, train_acc: 0.7599999904632568, avg_cost: 0.5698951615889871\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.7134156701962158\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 0.5400000214576721, avg_cost: 0.854616717000803\n",
            "**************************************************\n",
            "test_acc: 0.9213000029325485\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0013923890391985575\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.1440797437230746\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.28660115361213684\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.42926050434509905\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5734600042303403\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7180393113692606\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 0.6499999761581421, avg_cost: 0.862502174874147\n",
            "**************************************************\n",
            "test_acc: 0.9236000055074691\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013613567749659221\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.1446908534566561\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.28761932849884025\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.43292587767044705\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5824029464522998\n",
            "batch: 500, train_acc: 0.699999988079071, avg_cost: 0.7273154092828437\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 0.6600000262260437, avg_cost: 0.8701301971077915\n",
            "**************************************************\n",
            "test_acc: 0.9209000015258789\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0014647474884986876\n",
            "batch: 100, train_acc: 0.6499999761581421, avg_cost: 0.14326305439074838\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.2867601510882377\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4274769430359204\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.5728263209263482\n",
            "batch: 500, train_acc: 0.49000000953674316, avg_cost: 0.7248884676893557\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 0.550000011920929, avg_cost: 0.8703874252239869\n",
            "**************************************************\n",
            "test_acc: 0.9225000035762787\n",
            "batch: 0, train_acc: 0.699999988079071, avg_cost: 0.0012381867567698161\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1462231096625328\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.28774467716614405\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4380473620692888\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5940322826306026\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7426764650146166\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 0.5299999713897705, avg_cost: 0.8836740656693779\n",
            "**************************************************\n",
            "test_acc: 0.9212000054121018\n",
            "batch: 0, train_acc: 0.6700000166893005, avg_cost: 0.0011197208364804585\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14092314491669333\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.2834170900781949\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4254627901315685\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5706347254912055\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.7137821199496577\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 0.6100000143051147, avg_cost: 0.8577897993723539\n",
            "**************************************************\n",
            "test_acc: 0.9227000021934509\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013418928782145182\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14449567029873533\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.2855562484264375\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.4254780303438507\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5690402471025787\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7148404660820963\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 0.6299999952316284, avg_cost: 0.8550484509269402\n",
            "**************************************************\n",
            "test_acc: 0.9216000020503998\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.0013973162571589153\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1428440826137861\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.2868033406138421\n",
            "batch: 300, train_acc: 0.44999998807907104, avg_cost: 0.4306606587767601\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5749732047319414\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7150762736797331\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 0.6600000262260437, avg_cost: 0.8580961922804516\n",
            "**************************************************\n",
            "test_acc: 0.9253000009059906\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.00121026615301768\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.14411174744367594\n",
            "batch: 200, train_acc: 0.6800000071525574, avg_cost: 0.2874068595965702\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4288788436849911\n",
            "batch: 400, train_acc: 0.7300000190734863, avg_cost: 0.5744772265354787\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7203550040721884\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 0.6000000238418579, avg_cost: 0.862183429002761\n",
            "**************************************************\n",
            "test_acc: 0.9263000047206879\n",
            "[0, 0.7518999975919723, 0.8286999988555909, 0.8551000022888183, 0.8815000033378602, 0.8999000012874603, 0.9033000004291535, 0.9116000020503998, 0.9181000012159347, 0.9198000007867813, 0.9214000004529953, 0.9239000034332275, 0.9253000038862228, 0.9263000047206879]\n",
            "[100, 3.017639006177583, 1.404901708662509, 1.251403709053993, 1.2207689012090364, 1.1419776825110113, 1.0618281445900604, 1.0438530356685316, 1.0409303674101826, 1.01107532997926, 0.9863828338185947, 0.977629222869873, 0.9608956383665406, 0.9439110730091733, 0.9335681736469273, 0.9223622864484786, 0.9062068222959828, 0.8948541701833401, 0.8860550942023595, 0.8824167981743812, 0.8782279849052419, 0.8730565730730694, 0.8716764541467034, 0.8704980588952705, 0.8692193537950508, 0.8579746216535572, 0.854616717000803]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip_O_4IwsltP",
        "colab_type": "text"
      },
      "source": [
        "# 18 / 18 /  18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPKofKrIsoHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsQzgEE7srbM",
        "colab_type": "code",
        "outputId": "e53fbbdb-d322-4b73-bc5e-4d39d0052634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.10999999940395355, avg_cost: 0.03721494992574056\n",
            "batch: 100, train_acc: 0.36000001430511475, avg_cost: 0.6860487838586172\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.9641724280516307\n",
            "batch: 300, train_acc: 0.4399999976158142, avg_cost: 1.209512250026068\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 1.436432611147565\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 1.670493098696075\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5299999713897705, avg_cost: 1.8978204399347331\n",
            "**************************************************\n",
            "test_acc: 0.8102999967336655\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0016428598761558534\n",
            "batch: 100, train_acc: 0.5099999904632568, avg_cost: 0.2233051987489063\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.4373609753449757\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.6507124202450119\n",
            "batch: 400, train_acc: 0.46000000834465027, avg_cost: 0.8515857092539468\n",
            "batch: 500, train_acc: 0.4399999976158142, avg_cost: 1.0545055331786473\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.5299999713897705, avg_cost: 1.2532382018367445\n",
            "**************************************************\n",
            "test_acc: 0.8665999966859818\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.001922070582707723\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1971242279807727\n",
            "batch: 200, train_acc: 0.4699999988079071, avg_cost: 0.39617143630981455\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.5929702093203861\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.7812535160779952\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.9739043029149372\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.49000000953674316, avg_cost: 1.164676468571027\n",
            "**************************************************\n",
            "test_acc: 0.8817999988794327\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016219774881998698\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.1871316226323446\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.37510572840770096\n",
            "batch: 300, train_acc: 0.4699999988079071, avg_cost: 0.5593486398458483\n",
            "batch: 400, train_acc: 0.4300000071525574, avg_cost: 0.745982523461183\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.936174591978391\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5400000214576721, avg_cost: 1.1203088652094204\n",
            "**************************************************\n",
            "test_acc: 0.8909000009298325\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.001977286736170451\n",
            "batch: 100, train_acc: 0.47999998927116394, avg_cost: 0.18252809445063267\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.3745138325293859\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.558168246746063\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.7409063360095024\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.9199689403176309\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.5699999928474426, avg_cost: 1.100147813161214\n",
            "**************************************************\n",
            "test_acc: 0.8845000022649765\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001610079805056254\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.18088138480981192\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.36611616084973037\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.5489143710335097\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.7322140848636621\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.9093918561935418\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.5400000214576721, avg_cost: 1.0818271846572558\n",
            "**************************************************\n",
            "test_acc: 0.9022000008821487\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.0015615137418111165\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.17489972710609425\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.35201674262682586\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.5328783380985261\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.7052407970031106\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8811382830142981\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.6000000238418579, avg_cost: 1.0573249576489132\n",
            "**************************************************\n",
            "test_acc: 0.8904000025987625\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0018258744478225708\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.17427370886007942\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.3549180124203362\n",
            "batch: 300, train_acc: 0.44999998807907104, avg_cost: 0.5314184270302452\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.7137749381860098\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.8916348082820578\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.5799999833106995, avg_cost: 1.064808894495169\n",
            "**************************************************\n",
            "test_acc: 0.9054000002145767\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.0019061730305353802\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.17305575509866078\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.3446092538038888\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5168180769681926\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6891155244906736\n",
            "batch: 500, train_acc: 0.5099999904632568, avg_cost: 0.8598628736535706\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.5600000023841858, avg_cost: 1.0338271976510678\n",
            "**************************************************\n",
            "test_acc: 0.9023000007867813\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016664394736289978\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.17106983214616775\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.34212117264668174\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.5131343243519468\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6836642481883367\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8568003220359486\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.5899999737739563, avg_cost: 1.0483645706375442\n",
            "**************************************************\n",
            "test_acc: 0.9076000052690506\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016648887594540914\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1709676901499431\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3445729167262714\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5176305568218231\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6883946754535036\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8620095252990717\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.5099999904632568, avg_cost: 1.0324813155333197\n",
            "**************************************************\n",
            "test_acc: 0.9125000023841858\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0017752633492151895\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.16941770533720657\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.34026781737804396\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.5093212572733562\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6819596626361208\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.8508193036913869\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.4699999988079071, avg_cost: 1.0167821553349488\n",
            "**************************************************\n",
            "test_acc: 0.9124000024795532\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0018539486328760784\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.1719003074367841\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.3401409964760143\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5080634114146231\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.674024154841899\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.8413611578941338\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.47999998927116394, avg_cost: 1.00944928218921\n",
            "**************************************************\n",
            "test_acc: 0.9097999972105026\n",
            "batch: 0, train_acc: 0.49000000953674316, avg_cost: 0.0017675801118214925\n",
            "batch: 100, train_acc: 0.5, avg_cost: 0.1684358478585878\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.33477338463067996\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.5010179551442463\n",
            "batch: 400, train_acc: 0.46000000834465027, avg_cost: 0.6760267234841985\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.8502406023939452\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.5199999809265137, avg_cost: 1.019261559844017\n",
            "**************************************************\n",
            "test_acc: 0.9103000038862228\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.001657871901988983\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.16598353574673333\n",
            "batch: 200, train_acc: 0.44999998807907104, avg_cost: 0.33088128238916387\n",
            "batch: 300, train_acc: 0.46000000834465027, avg_cost: 0.49739896287520713\n",
            "batch: 400, train_acc: 0.699999988079071, avg_cost: 0.6645369525750479\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8318112807472549\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.5799999833106995, avg_cost: 0.9966614098350206\n",
            "**************************************************\n",
            "test_acc: 0.9117000031471253\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0015860254565874736\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.16592570980389912\n",
            "batch: 200, train_acc: 0.5, avg_cost: 0.33134491006533306\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.49624182740847267\n",
            "batch: 400, train_acc: 0.47999998927116394, avg_cost: 0.6630451217293738\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.8298237544298174\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.5799999833106995, avg_cost: 0.9942503076791764\n",
            "**************************************************\n",
            "test_acc: 0.9231000018119812\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.001589074432849884\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.16534094512462616\n",
            "batch: 200, train_acc: 0.47999998927116394, avg_cost: 0.3296419659256936\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.4965213008721672\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6615778414408368\n",
            "batch: 500, train_acc: 0.6700000166893005, avg_cost: 0.8282031621535622\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.6200000047683716, avg_cost: 0.9890907031297688\n",
            "**************************************************\n",
            "test_acc: 0.9183000004291535\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0016980292399724325\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1624779966473579\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.32606920739014944\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.49082697341839476\n",
            "batch: 400, train_acc: 0.6600000262260437, avg_cost: 0.6549782532453535\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.8194348107775051\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.5699999928474426, avg_cost: 0.9800349339842799\n",
            "**************************************************\n",
            "test_acc: 0.9114000022411346\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0017036910851796468\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.16415131936470664\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.32823019951581955\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.49003716170787864\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6522816195090619\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.8160555780927345\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.5400000214576721, avg_cost: 0.9739183651407564\n",
            "**************************************************\n",
            "test_acc: 0.9014000028371811\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0018472345670064291\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.16508912076552712\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3256639913717905\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.48825736920038854\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6506773334741586\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.8169755711158104\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.6000000238418579, avg_cost: 0.9797947867711371\n",
            "**************************************************\n",
            "test_acc: 0.9209000015258789\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0016303894917170207\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.16056380858023964\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.3226784051458042\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4819899551073709\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.640048320194085\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.805091601709524\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.6000000238418579, avg_cost: 0.9650999591747906\n",
            "**************************************************\n",
            "test_acc: 0.9236000013351441\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.0017111259698867798\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.159068401257197\n",
            "batch: 200, train_acc: 0.5099999904632568, avg_cost: 0.31773498177528386\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.48384467482566834\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6453350704908368\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.8030483358105022\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.49000000953674316, avg_cost: 0.9640080659588174\n",
            "**************************************************\n",
            "test_acc: 0.9213000017404557\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.001546637515227\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.1601494727532069\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.3180775914589566\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.47552246034145346\n",
            "batch: 400, train_acc: 0.5199999809265137, avg_cost: 0.6353620458642646\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7939552195866908\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.5299999713897705, avg_cost: 0.9528262653946884\n",
            "**************************************************\n",
            "test_acc: 0.9249000042676926\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0016620343923568726\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.1565219468871753\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.31564693719148634\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.47662166376908627\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6364654128750165\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7938466782371206\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.5, avg_cost: 0.9538634167114899\n",
            "**************************************************\n",
            "test_acc: 0.9242000037431717\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0017551650603612264\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15677053481340414\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3144250243902208\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.47231863498687754\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.6291333147883414\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7855708369612693\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.6499999761581421, avg_cost: 0.9447985395789146\n",
            "**************************************************\n",
            "test_acc: 0.923000003695488\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013620305061340333\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.15648242284854252\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.31278771489858637\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4815437969565388\n",
            "batch: 400, train_acc: 0.5099999904632568, avg_cost: 0.6415191552042957\n",
            "batch: 500, train_acc: 0.5, avg_cost: 0.8370544541875514\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.6000000238418579, avg_cost: 1.001732053955395\n",
            "**************************************************\n",
            "test_acc: 0.9202000021934509\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001465381383895874\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.16334847768147792\n",
            "batch: 200, train_acc: 0.4399999976158142, avg_cost: 0.3206560856103898\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.48321333209673556\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.6398305067420004\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.7946419365207351\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.49000000953674316, avg_cost: 0.9492533259590459\n",
            "**************************************************\n",
            "test_acc: 0.9232000052928925\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.001669688622156779\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.15996267100175224\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.31634191701809555\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.4725297194719314\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6303369934360183\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7853214471538862\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.6600000262260437, avg_cost: 0.940657325883706\n",
            "**************************************************\n",
            "test_acc: 0.9253000020980835\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0017412185668945312\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.1561575666069985\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.3113521187504133\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4664666041731836\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.6222360070546474\n",
            "batch: 500, train_acc: 0.6600000262260437, avg_cost: 0.7721115484833726\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.5099999904632568, avg_cost: 0.9240187591314324\n",
            "**************************************************\n",
            "test_acc: 0.9198000025749207\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.001486358145872752\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.15226041833559673\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.3030173495411875\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4549183287223184\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6080123916268354\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7646470956007649\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.5899999737739563, avg_cost: 0.9186323034763344\n",
            "**************************************************\n",
            "test_acc: 0.9212000036239624\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001388329267501831\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15123887340227757\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.30513132760922107\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4570988863706589\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.6104764483372368\n",
            "batch: 500, train_acc: 0.6499999761581421, avg_cost: 0.7628748877843216\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.6000000238418579, avg_cost: 0.9223256555199617\n",
            "**************************************************\n",
            "test_acc: 0.9213000005483627\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0016530025005340575\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.15649674216906223\n",
            "batch: 200, train_acc: 0.5799999833106995, avg_cost: 0.3096571495135624\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.46109262913465476\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6135461870829266\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.7682262881596885\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.6399999856948853, avg_cost: 0.9268993445237478\n",
            "**************************************************\n",
            "test_acc: 0.9202000057697296\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016461339592933654\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.15378544012705492\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.3042066861192384\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.4570727259914077\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6083320509394009\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.759951880276203\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.5799999833106995, avg_cost: 0.9137130545576408\n",
            "**************************************************\n",
            "test_acc: 0.9231000036001206\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0016498331228892008\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15333367923895516\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.3048273947834968\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.45484877943992635\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.6087045024832093\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7601633712649348\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.5199999809265137, avg_cost: 0.9119089285532636\n",
            "**************************************************\n",
            "test_acc: 0.9223000019788742\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001482876936594645\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.15360248277584715\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.30315579940875365\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4541626931230227\n",
            "batch: 400, train_acc: 0.6700000166893005, avg_cost: 0.6045796139041586\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7543112960457812\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.6299999952316284, avg_cost: 0.9022834172844892\n",
            "**************************************************\n",
            "test_acc: 0.9261000043153763\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0012901011109352112\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1530394111076991\n",
            "batch: 200, train_acc: 0.5400000214576721, avg_cost: 0.3034122750163078\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.45155153940121323\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.6031282715996111\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.75686627060175\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.5099999904632568, avg_cost: 0.9073286353548374\n",
            "**************************************************\n",
            "test_acc: 0.9177000004053116\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.0012925430138905844\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.1528276489178339\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.30114980310201644\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.452044669489066\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.6018640168507898\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7519303198655448\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.6299999952316284, avg_cost: 0.9009390192230545\n",
            "**************************************************\n",
            "test_acc: 0.9237000024318696\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.001507921516895294\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.15121813396612802\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2996033352613449\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.44606197237968437\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.5951159245769191\n",
            "batch: 500, train_acc: 0.6600000262260437, avg_cost: 0.7449978492657351\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.5899999737739563, avg_cost: 0.8925277495384226\n",
            "**************************************************\n",
            "test_acc: 0.9230000042915344\n",
            "batch: 0, train_acc: 0.49000000953674316, avg_cost: 0.001788790225982666\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.15340772916873302\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.3045812533299127\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4536261663834249\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.6039583913485205\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.75587065766255\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 0.5600000023841858, avg_cost: 0.9055073579152421\n",
            "**************************************************\n",
            "test_acc: 0.9221000015735626\n",
            "batch: 0, train_acc: 0.6800000071525574, avg_cost: 0.0012827428181966145\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.14824335505565014\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.2934695717692375\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.4444453957676886\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5914530398448314\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7415991481145225\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.5799999833106995, avg_cost: 0.8935356239477793\n",
            "**************************************************\n",
            "test_acc: 0.9239000028371811\n",
            "batch: 0, train_acc: 0.5299999713897705, avg_cost: 0.0016113575299580892\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.14604742536942175\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2935340211788814\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4442389713724455\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5989474215110142\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7510859890778859\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 0.6499999761581421, avg_cost: 0.9006939444939298\n",
            "**************************************************\n",
            "test_acc: 0.9261000025272369\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.001314026415348053\n",
            "batch: 100, train_acc: 0.5, avg_cost: 0.14766603628794353\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.3042215190331141\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.5272189748287203\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.7014964889486635\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.8643664722641311\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 0.6000000238418579, avg_cost: 1.0181406842668852\n",
            "**************************************************\n",
            "test_acc: 0.9222000020742417\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.001586294968922933\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.1565912773211797\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.3090302594502766\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.4578900742530821\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.6089400828878082\n",
            "batch: 500, train_acc: 0.6700000166893005, avg_cost: 0.762489742040634\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 0.5899999737739563, avg_cost: 0.9091295949618018\n",
            "**************************************************\n",
            "test_acc: 0.9244000035524368\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0014378682772318522\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14870663414398827\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2972438165545464\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4454718457659085\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5913523937265077\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7378497882684069\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 0.6000000238418579, avg_cost: 0.883198817074298\n",
            "**************************************************\n",
            "test_acc: 0.9267000025510788\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0012644618749618531\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.144998153646787\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.28878823935985565\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4341972638169925\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5823908080657325\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.7284335243701936\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 0.6200000047683716, avg_cost: 0.8706561682621642\n",
            "**************************************************\n",
            "test_acc: 0.9238000017404556\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0014437526464462281\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.15210461179415385\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.3029460830489794\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4520552273591359\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.6020091790954268\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.7523055309057234\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 0.6000000238418579, avg_cost: 0.8970545738935467\n",
            "**************************************************\n",
            "test_acc: 0.9272000002861023\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001488727331161499\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14529661178588874\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.28971979041894286\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4360331336657207\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5823621443907421\n",
            "batch: 500, train_acc: 0.6600000262260437, avg_cost: 0.7253148893515267\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 0.5199999809265137, avg_cost: 0.8716991986831025\n",
            "**************************************************\n",
            "test_acc: 0.9263000017404557\n",
            "batch: 0, train_acc: 0.5099999904632568, avg_cost: 0.0014960276087125142\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1470221279064814\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.29165803660949063\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.43675421198209136\n",
            "batch: 400, train_acc: 0.6700000166893005, avg_cost: 0.5812778653701147\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.7269025401274363\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 0.6200000047683716, avg_cost: 0.8728617007533702\n",
            "**************************************************\n",
            "test_acc: 0.9186000001430511\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0014946148792902628\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.1473657666643461\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.29532369603713365\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.444563999970754\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5921522788206738\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.7401491477092114\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 0.5, avg_cost: 0.8853694162766139\n",
            "**************************************************\n",
            "test_acc: 0.9256000000238419\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014502183596293132\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14785595049460723\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.29639519880215315\n",
            "batch: 300, train_acc: 0.6499999761581421, avg_cost: 0.4424273937940595\n",
            "batch: 400, train_acc: 0.699999988079071, avg_cost: 0.5878352870543799\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7339081214865051\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 0.5799999833106995, avg_cost: 0.8797802656888971\n",
            "**************************************************\n",
            "test_acc: 0.9180000007152558\n",
            "[0, 0.8102999967336655, 0.8665999966859818, 0.8817999988794327, 0.8909000009298325, 0.9022000008821487, 0.9054000002145767, 0.9076000052690506, 0.9125000023841858, 0.9231000018119812, 0.9236000013351441, 0.9249000042676926, 0.9253000020980835, 0.9261000043153763, 0.9267000025510788, 0.9272000002861023]\n",
            "[100, 1.8978204399347331, 1.2532382018367445, 1.164676468571027, 1.1203088652094204, 1.100147813161214, 1.0818271846572558, 1.0573249576489132, 1.0338271976510678, 1.0324813155333197, 1.0167821553349488, 1.00944928218921, 0.9966614098350206, 0.9942503076791764, 0.9890907031297688, 0.9800349339842799, 0.9739183651407564, 0.9650999591747906, 0.9640080659588174, 0.9528262653946884, 0.9447985395789146, 0.940657325883706, 0.9240187591314324, 0.9186323034763344, 0.9137130545576408, 0.9119089285532636, 0.9022834172844892, 0.9009390192230545, 0.8925277495384226, 0.883198817074298, 0.8706561682621642]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlXB9NkwDeXF",
        "colab_type": "text"
      },
      "source": [
        "# Cont. : Restore ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3vvDbZlDccC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 25\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2UUj0GDjM1",
        "colab_type": "code",
        "outputId": "54a9711a-4a4e-4c44-aaa7-459608f79e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.001661586860815684\n",
            "batch: 100, train_acc: 0.6499999761581421, avg_cost: 0.14738524446884793\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.29692100803057364\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.4422937098145486\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5899402118722595\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7372463192542391\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.5099999904632568, avg_cost: 0.883684594730536\n",
            "**************************************************\n",
            "test_acc: 0.9254000020027161\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0014896883567174276\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.1473659917712212\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.2952321117122968\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.44170152455568307\n",
            "batch: 400, train_acc: 0.46000000834465027, avg_cost: 0.5860645527640987\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.7348385425408687\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.5600000023841858, avg_cost: 0.8833632872502013\n",
            "**************************************************\n",
            "test_acc: 0.9231000036001206\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0013817313313484192\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.14772873520851135\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.292416245539983\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.44108227759599655\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5894107566277179\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.734433029989401\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.5699999928474426, avg_cost: 0.8766082748770708\n",
            "**************************************************\n",
            "test_acc: 0.9192000031471252\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0017205711205800374\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.14715008020401005\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.29199857980012883\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.4397620686888694\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5870153268178305\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7325372717777892\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.6000000238418579, avg_cost: 0.8791154943903293\n",
            "**************************************************\n",
            "test_acc: 0.9195000004768371\n",
            "batch: 0, train_acc: 0.699999988079071, avg_cost: 0.0011523231863975526\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.14383550276358928\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.29025223225355157\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.4368620684742926\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5866048733393345\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7341084985931705\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.699999988079071, avg_cost: 0.8776531547307955\n",
            "**************************************************\n",
            "test_acc: 0.9253000015020371\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016354037324587505\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1475003884236018\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.2908634280165036\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.43705149998267495\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5830977903803187\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7301594648758567\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.5400000214576721, avg_cost: 0.8774612913529071\n",
            "**************************************************\n",
            "test_acc: 0.9238000011444092\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.0010984901587168376\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.14486827860275905\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.2894331559538839\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4347886947790776\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5795569518208499\n",
            "batch: 500, train_acc: 0.6899999976158142, avg_cost: 0.7283340197801585\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5099999904632568, avg_cost: 0.8730974697073298\n",
            "**************************************************\n",
            "test_acc: 0.9247000026702881\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.0010874660809834797\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14305982848008475\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.2900319064656892\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4349971057971316\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5832373054822282\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7282396146655077\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.6100000143051147, avg_cost: 0.8743270584940902\n",
            "**************************************************\n",
            "test_acc: 0.9220000022649765\n",
            "batch: 0, train_acc: 0.6600000262260437, avg_cost: 0.001423911948998769\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14700399398803712\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.29156769096851365\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.44118189056714413\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5879901611804964\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7332719887296363\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.6200000047683716, avg_cost: 0.8785881827274955\n",
            "**************************************************\n",
            "test_acc: 0.9225000035762787\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0016042677561442058\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14749469170967738\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.29271823694308574\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.43677643646796505\n",
            "batch: 400, train_acc: 0.6800000071525574, avg_cost: 0.5811444037159273\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7288491684198364\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.6399999856948853, avg_cost: 0.8696587119499827\n",
            "**************************************************\n",
            "test_acc: 0.923299999833107\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.001439322829246521\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14849634716908136\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.29397797505060846\n",
            "batch: 300, train_acc: 0.5299999713897705, avg_cost: 0.4404954694708189\n",
            "batch: 400, train_acc: 0.6600000262260437, avg_cost: 0.5867563813924793\n",
            "batch: 500, train_acc: 0.5400000214576721, avg_cost: 0.7301940181851386\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.699999988079071, avg_cost: 0.8702065826455754\n",
            "**************************************************\n",
            "test_acc: 0.9246000015735626\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014400928219159444\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.14407291054725643\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.2859017378091812\n",
            "batch: 300, train_acc: 0.49000000953674316, avg_cost: 0.4301553716262181\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5765737357735633\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7217071800430611\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.6100000143051147, avg_cost: 0.8635277869304016\n",
            "**************************************************\n",
            "test_acc: 0.9230000019073487\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013503387570381164\n",
            "batch: 100, train_acc: 0.49000000953674316, avg_cost: 0.14368060052394863\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.2881266985336938\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4295374226570128\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5780859946211179\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.7246100323398906\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.5299999713897705, avg_cost: 0.8718360503514604\n",
            "**************************************************\n",
            "test_acc: 0.9167000031471253\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0016380872329076132\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.14877692083517713\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2947646460930506\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4426345709959666\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5868442986408868\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7303355606396988\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.5400000214576721, avg_cost: 0.8716506760319075\n",
            "**************************************************\n",
            "test_acc: 0.9265000033378601\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.0016769647598266601\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.146008524398009\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.2877361825108529\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.42990314135948837\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5717894650499025\n",
            "batch: 500, train_acc: 0.5600000023841858, avg_cost: 0.7194136714935297\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.5299999713897705, avg_cost: 0.8627500519156451\n",
            "**************************************************\n",
            "test_acc: 0.9263000017404557\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.0014219629764556886\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.14353452573219935\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.28401819169521336\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.42683940629164385\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.571215407450994\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.7142134439945224\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.5299999713897705, avg_cost: 0.8577161649862929\n",
            "**************************************************\n",
            "test_acc: 0.9254000014066697\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.001246168812115987\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.14492906570434572\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.2871596839030581\n",
            "batch: 300, train_acc: 0.5400000214576721, avg_cost: 0.4309889803330102\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5745427652200059\n",
            "batch: 500, train_acc: 0.5199999809265137, avg_cost: 0.7189822793006891\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.6399999856948853, avg_cost: 0.8616355436046913\n",
            "**************************************************\n",
            "test_acc: 0.9263000041246414\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.0012772065401077271\n",
            "batch: 100, train_acc: 0.6499999761581421, avg_cost: 0.14656093925237643\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.29061032583316143\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4352763950824735\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5811560508608814\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7242238555351892\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.6600000262260437, avg_cost: 0.8678872105479236\n",
            "**************************************************\n",
            "test_acc: 0.9223000025749206\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.001282845139503479\n",
            "batch: 100, train_acc: 0.6899999976158142, avg_cost: 0.14646752536296853\n",
            "batch: 200, train_acc: 0.5199999809265137, avg_cost: 0.2872030108173688\n",
            "batch: 300, train_acc: 0.7300000190734863, avg_cost: 0.43230588525533675\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.575652819077174\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.7198341256380082\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.5899999737739563, avg_cost: 0.8657786551117895\n",
            "**************************************************\n",
            "test_acc: 0.9252000015974045\n",
            "batch: 0, train_acc: 0.6899999976158142, avg_cost: 0.0011163072784741719\n",
            "batch: 100, train_acc: 0.6700000166893005, avg_cost: 0.1457338098684946\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.2886162270108858\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.42969775944948174\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5733882033824921\n",
            "batch: 500, train_acc: 0.5299999713897705, avg_cost: 0.7174586594104765\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.6100000143051147, avg_cost: 0.8618747164805732\n",
            "**************************************************\n",
            "test_acc: 0.9213000017404557\n",
            "batch: 0, train_acc: 0.6800000071525574, avg_cost: 0.0013071436683336894\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.14241497625907262\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.2851181936264038\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.4272935010989504\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5725569225351014\n",
            "batch: 500, train_acc: 0.49000000953674316, avg_cost: 0.7154347781340279\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.5600000023841858, avg_cost: 0.8577022924025857\n",
            "**************************************************\n",
            "test_acc: 0.9264000010490417\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013585448265075683\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14261244515577953\n",
            "batch: 200, train_acc: 0.6600000262260437, avg_cost: 0.2850741691390672\n",
            "batch: 300, train_acc: 0.6700000166893005, avg_cost: 0.42678296794493953\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5697038989265756\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7150638942917183\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.5899999737739563, avg_cost: 0.8617093919714289\n",
            "**************************************************\n",
            "test_acc: 0.9232999992370605\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001422190566857656\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14588216443856564\n",
            "batch: 200, train_acc: 0.6499999761581421, avg_cost: 0.29100667963425336\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.4342900180816654\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5783248408635461\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7193829832474392\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.5799999833106995, avg_cost: 0.8631522494554518\n",
            "**************************************************\n",
            "test_acc: 0.9262000036239624\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0014886772632598877\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.14289235840241107\n",
            "batch: 200, train_acc: 0.6399999856948853, avg_cost: 0.2834191178282101\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.42750014593203844\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5735405671596524\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7194738422830896\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.6000000238418579, avg_cost: 0.8636630873878791\n",
            "**************************************************\n",
            "test_acc: 0.9265000027418137\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.001521776815255483\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14512155344088878\n",
            "batch: 200, train_acc: 0.6899999976158142, avg_cost: 0.2892702978849411\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.43254771490891775\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5747078054149948\n",
            "batch: 500, train_acc: 0.6899999976158142, avg_cost: 0.715600510438284\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.5699999928474426, avg_cost: 0.8563343659043317\n",
            "**************************************************\n",
            "test_acc: 0.9255000001192093\n",
            "[0, 0.9254000020027161, 0.9265000033378601]\n",
            "[100, 0.883684594730536, 0.8833632872502013, 0.8766082748770708, 0.8730974697073298, 0.8696587119499827, 0.8635277869304016, 0.8627500519156451, 0.8577161649862929, 0.8577022924025857, 0.8563343659043317]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXqEPCqLLTou",
        "colab_type": "text"
      },
      "source": [
        "#Cont. : fc에 dropout 없앰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAJSLm49LVJh",
        "colab_type": "code",
        "outputId": "9de9af31-42e7-4f93-97ac-39b13ca2bcb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.00001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost_flatten.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc_flatten.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost_flatten.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 02:26:09.855189 140235330328448 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0815 02:26:09.888265 140235330328448 deprecation.py:323] From <ipython-input-3-4f14eef2ef83>:37: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W0815 02:26:17.466892 140235330328448 deprecation.py:506] From <ipython-input-3-4f14eef2ef83>:78: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ZMw-S7LXHF",
        "colab_type": "code",
        "outputId": "005a450c-c2c1-4b98-f872-a05d21b547c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:1, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0815 02:26:26.133930 140235330328448 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch: 0, train_acc: 1.0, avg_cost: 1.9868214481041e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 6.9953072889816725e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.626304685734917e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 2.0188967496300353e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.9212616543169028e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 3.175574337044291e-08\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 1.0, avg_cost: 3.568564394799497e-08\n",
            "**************************************************\n",
            "test_acc: 0.9307000029087067\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.9736428962082e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 6.772985368108664e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.2814785205576377e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.8751240078070594e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.195199231983897e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.8188441197526024e-08\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 1.0, avg_cost: 3.086468087665215e-08\n",
            "**************************************************\n",
            "test_acc: 0.930400003194809\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.973642526133858e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 5.247112169574444e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 8.477665892255484e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.5604007888025968e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.0429956114620387e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.3980376608149567e-08\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 1.0, avg_cost: 2.7659949494533216e-08\n",
            "**************************************************\n",
            "test_acc: 0.9305000030994415\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.960463974237958e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.0974186789508204e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 7.420614512495165e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.1994212015808973e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.8663795513888346e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.2178470441280845e-08\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 1.0, avg_cost: 2.5750759888495014e-08\n",
            "**************************************************\n",
            "test_acc: 0.9302000033855439\n",
            "batch: 0, train_acc: 1.0, avg_cost: 7.947285052267716e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 8.791413319923322e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.6142431163990173e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.8757083553033734e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.2657200122733185e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.949573332577946e-08\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 1.0, avg_cost: 3.2855435888601203e-08\n",
            "**************************************************\n",
            "test_acc: 0.9302000021934509\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.3644129138774586e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 4.0590423281787e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.0696781873598127e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.3229974363445286e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.7889033912347578e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.170569739538366e-08\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 1.0, avg_cost: 2.5657451887909693e-08\n",
            "**************************************************\n",
            "test_acc: 0.9301000028848648\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.370901259178936e-10\n",
            "batch: 100, train_acc: 1.0, avg_cost: 4.53190531657377e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 8.9366473462101e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.2409595896434314e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.0062576837990665e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.3380554358872404e-08\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 1.0, avg_cost: 2.6609132563285925e-08\n",
            "**************************************************\n",
            "test_acc: 0.9305000030994415\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.9868209300000218e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.4749273447598477e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 8.845228043711604e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.5618189304437517e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.9230218804381712e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.1701815017260945e-08\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 1.0, avg_cost: 2.497209338980783e-08\n",
            "**************************************************\n",
            "test_acc: 0.9305000030994415\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.7881389702267824e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.0517471908779e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.373699794615105e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.0218160483669674e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.4616946488003433e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.8896525557643434e-08\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 1.0, avg_cost: 2.236153118895247e-08\n",
            "**************************************************\n",
            "test_acc: 0.9307000029087067\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.9868214481041e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 5.844858453896148e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 9.967459326058284e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.3041060261675961e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.6202086962069388e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.265507502226724e-08\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 1.0, avg_cost: 2.9322732082818588e-08\n",
            "**************************************************\n",
            "test_acc: 0.9308000034093857\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.9126607555459333e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.149180951098287e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 9.767168902182397e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.3047365623464216e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.6818316345516383e-08\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 1.0, avg_cost: 2.012635757349854e-08\n",
            "**************************************************\n",
            "test_acc: 0.9308000028133392\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.5894568624238066e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.936505481739477e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.0017151788425482e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.5657356713753392e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.8083260949023712e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.134957973631833e-08\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 1.0, avg_cost: 2.5992671747558137e-08\n",
            "**************************************************\n",
            "test_acc: 0.9305000019073486\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.9736428962082e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.2371548639525217e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.468349334615001e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.033327174369811e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.0991028390779554e-06\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.1027327442525437e-06\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 1.0, avg_cost: 1.1051129523783245e-06\n",
            "**************************************************\n",
            "test_acc: 0.9307000023126603\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.967046921914668e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.4451316358966952e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 9.45711185066737e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.3013441955885473e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.7215526711596635e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.615919650598525e-08\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 1.0, avg_cost: 2.94612752253324e-08\n",
            "**************************************************\n",
            "test_acc: 0.9308000022172928\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.2119567571744484e-10\n",
            "batch: 100, train_acc: 1.0, avg_cost: 8.547121925032523e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.743912967699935e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 2.077498275003068e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.2984322569641323e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 3.147112055179585e-08\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 1.0, avg_cost: 3.991007936354749e-08\n",
            "**************************************************\n",
            "test_acc: 0.9308000022172928\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.3907749026505675e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 6.599991663932178e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 9.081523725621807e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.2562403816455514e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.6599581507866422e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.1075830954897506e-08\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 1.0, avg_cost: 2.6773955871991108e-08\n",
            "**************************************************\n",
            "test_acc: 0.931000002026558\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.0351767626796499e-08\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.4096878293567995e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.7226112944227865e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.9701687076212335e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.1434193189229226e-08\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 1.0, avg_cost: 2.3949482452397435e-08\n",
            "**************************************************\n",
            "test_acc: 0.9311000019311905\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 4.208071987162515e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.1394100471647425e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.5371652229238355e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.9494286623104433e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.3295022067992845e-08\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 1.0, avg_cost: 2.7614353451295884e-08\n",
            "**************************************************\n",
            "test_acc: 0.9309000021219254\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 6.0775168949274875e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 7.750418580254604e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.81226843002739e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.1778420484785953e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 3.4403640289948324e-08\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 1.0, avg_cost: 3.784280729853437e-08\n",
            "**************************************************\n",
            "test_acc: 0.9309000021219254\n",
            "batch: 0, train_acc: 1.0, avg_cost: 9.934106870446157e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.4510796823964757e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.41935179198268e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.0835958937915892e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.3587694709382704e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.543487046321939e-07\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 1.0, avg_cost: 1.5615869553527656e-07\n",
            "**************************************************\n",
            "test_acc: 0.931000002026558\n",
            "batch: 0, train_acc: 1.0, avg_cost: 7.947285052267716e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.2252338397847167e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.289532598752049e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 6.576357911569626e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 9.171131206316596e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.1299014494407341e-08\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 1.0, avg_cost: 1.393353366944501e-08\n",
            "**************************************************\n",
            "test_acc: 0.931000002026558\n",
            "batch: 0, train_acc: 1.0, avg_cost: 9.934104650000109e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 5.811253038038681e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 9.45107541783231e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.1920689052165794e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.3986977298596796e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.8991517180729425e-08\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 1.0, avg_cost: 2.064653731463177e-08\n",
            "**************************************************\n",
            "test_acc: 0.9311000019311905\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.9868214481041e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.4669774566046608e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 7.935298726631972e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.0720813910329136e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.2771208737586805e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.4982534188856563e-08\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 1.0, avg_cost: 1.7934930947779085e-08\n",
            "**************************************************\n",
            "test_acc: 0.9316000014543533\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.357695040525262e-10\n",
            "batch: 100, train_acc: 1.0, avg_cost: 4.714668103330174e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 9.969615225591263e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.3722652922136093e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 2.1704957867315285e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.4560011083751845e-08\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 1.0, avg_cost: 2.649119678732603e-08\n",
            "**************************************************\n",
            "test_acc: 0.9313000017404556\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.3682838575419144e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.839881656929867e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.271722664385169e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.045658235961552e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.6365068660576898e-08\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 1.0, avg_cost: 1.882272135415399e-08\n",
            "**************************************************\n",
            "test_acc: 0.9317000019550323\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.973642526133858e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.849089302259244e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 5.634604785716611e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 9.407532060764137e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.1982445533205903e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.41560242454079e-08\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 1.0, avg_cost: 1.5721637864896134e-08\n",
            "**************************************************\n",
            "test_acc: 0.9317000019550323\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.960463974237958e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.5901684110791365e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.119387896038235e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 9.038014600430226e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.3194318739806488e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.519107019028652e-08\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 1.0, avg_cost: 1.6357333713079596e-08\n",
            "**************************************************\n",
            "test_acc: 0.9311000025272369\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.5848479575281877e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.283578165509279e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 9.171083943382288e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.2167190214980772e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 2.1494013561958647e-08\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 1.0, avg_cost: 2.4047072336349144e-08\n",
            "**************************************************\n",
            "test_acc: 0.9309000027179718\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.781548621063242e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.2729168743425745e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.875645864451287e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 8.674426765405609e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.1708275421037004e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3353360677944233e-08\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 1.0, avg_cost: 1.5908404010038084e-08\n",
            "**************************************************\n",
            "test_acc: 0.9307000023126603\n",
            "batch: 0, train_acc: 1.0, avg_cost: 6.35781679155419e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.5967574753445642e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.019319896538437e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 5.6644052925935965e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 7.271741929715242e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 9.15127011798707e-09\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 1.0, avg_cost: 1.1376502338326477e-08\n",
            "**************************************************\n",
            "test_acc: 0.9309000027179718\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.986820485910812e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.8391548867467956e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.587545948675496e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.99294180250006e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.017446651670658e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3112938023936623e-08\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 1.0, avg_cost: 1.4094427140583712e-08\n",
            "**************************************************\n",
            "test_acc: 0.9309000021219254\n",
            "batch: 0, train_acc: 1.0, avg_cost: 9.934106870446157e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.139794981574165e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 5.654443507863148e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 8.163770469638017e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.1877076994866927e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.4072509430714454e-08\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 1.0, avg_cost: 1.638913613747722e-08\n",
            "**************************************************\n",
            "test_acc: 0.9311000019311905\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.5894568624238066e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 6.3218504046484256e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 7.929186706112652e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.0527920995725062e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.348229437980744e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.530420639069462e-08\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 1.0, avg_cost: 1.964719018846912e-08\n",
            "**************************************************\n",
            "test_acc: 0.9309000015258789\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.192092646817855e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.894783319717929e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 5.106110542903625e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.782345441039353e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.0065188812413695e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3611608283466606e-08\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 1.0, avg_cost: 1.689180063157992e-08\n",
            "**************************************************\n",
            "test_acc: 0.9312000012397766\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 7.032437988237679e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 1.2015065950684211e-08\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.4611828686265475e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.6084060859844496e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.933642118460513e-08\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 1.0, avg_cost: 2.0651695208743574e-08\n",
            "**************************************************\n",
            "test_acc: 0.9312000012397766\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.960463604163617e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.000725177013033e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.957739377706082e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.249881335269485e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 8.531379862194065e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.1404306320036052e-08\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 1.0, avg_cost: 1.3319591894248962e-08\n",
            "**************************************************\n",
            "test_acc: 0.9313000011444091\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.6623117899138293e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 5.07231716998883e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.585636132567683e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.171418535210004e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3118866207066192e-08\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 1.0, avg_cost: 1.695732705290591e-08\n",
            "**************************************************\n",
            "test_acc: 0.9310000014305114\n",
            "batch: 0, train_acc: 1.0, avg_cost: 9.93410539014879e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.6868049642256486e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.1987725331491147e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 4.267681859756085e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 6.155155951592656e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 8.187662619748198e-09\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 1.0, avg_cost: 1.6039249982243333e-08\n",
            "**************************************************\n",
            "test_acc: 0.9312000012397766\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.3907749026505675e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.7126314651635023e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.502750481517341e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.512104293431554e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 9.405539710405365e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.1426128889343216e-08\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 1.0, avg_cost: 1.2955979120284545e-08\n",
            "**************************************************\n",
            "test_acc: 0.9314000010490417\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.9868214481041e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.220582887782559e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 6.653706583851594e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 8.491509725742461e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.0295539522031672e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3835973110554195e-08\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 1.0, avg_cost: 2.008213931592453e-08\n",
            "**************************************************\n",
            "test_acc: 0.9312000006437302\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.9736428962082e-12\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.9285585205022588e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 5.88686477268988e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.289559017150293e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.4270957785130397e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.66193655093405e-08\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 1.0, avg_cost: 1.7835299462864976e-08\n",
            "**************************************************\n",
            "test_acc: 0.9309000015258789\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.1920924987881184e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.5437561673437013e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.1749345171435292e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 5.455797351302153e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 9.785013793672797e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.5884318358786805e-08\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 1.0, avg_cost: 1.8544660005022795e-08\n",
            "**************************************************\n",
            "test_acc: 0.9314000016450882\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.7881389702267824e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.750385031732786e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 4.668992048539877e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 6.747193689315974e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 7.744576323576171e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.0794271469623724e-08\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 1.0, avg_cost: 1.3389039388670808e-08\n",
            "**************************************************\n",
            "test_acc: 0.9314000016450882\n",
            "batch: 0, train_acc: 1.0, avg_cost: 5.960459385316123e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 5.020349376957018e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 8.006530488602337e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 1.0033079911320197e-08\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.1914594202725837e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3635177220638927e-08\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 1.0, avg_cost: 1.5282249271978823e-08\n",
            "**************************************************\n",
            "test_acc: 0.9312000018358231\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.3768630609719187e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.709365706446493e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 5.785584534049679e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 8.29292356489356e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 9.391633868742026e-09\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 1.0, avg_cost: 1.119566266953126e-08\n",
            "**************************************************\n",
            "test_acc: 0.9313000011444091\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.601375742750951e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.133209374543236e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 4.818025575170636e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 7.019417221180525e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 8.640658283691025e-09\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 1.0, avg_cost: 1.057978660308953e-08\n",
            "**************************************************\n",
            "test_acc: 0.9310000014305114\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.371003411070736e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.5358104636190956e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 5.000665861398799e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 7.653028009461472e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 8.900750610626081e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.0970998152171071e-08\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 1.0, avg_cost: 1.2407465929120698e-08\n",
            "**************************************************\n",
            "test_acc: 0.9305000007152557\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.8417703162576516e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.6954625737273222e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 8.757649533341904e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 1.121930799855935e-08\n",
            "batch: 500, train_acc: 1.0, avg_cost: 1.3214071320858773e-08\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 1.0, avg_cost: 1.5264462445892724e-08\n",
            "**************************************************\n",
            "test_acc: 0.9310000014305114\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.192092646817855e-11\n",
            "batch: 100, train_acc: 1.0, avg_cost: 2.2907952834660965e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 3.7948155366211255e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 6.433270465603749e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 7.66708495284855e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 9.65390058005334e-09\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 1.0, avg_cost: 1.1440048887830437e-08\n",
            "**************************************************\n",
            "test_acc: 0.9307000005245208\n",
            "batch: 0, train_acc: 1.0, avg_cost: 0.0\n",
            "batch: 100, train_acc: 1.0, avg_cost: 1.2576499201314807e-09\n",
            "batch: 200, train_acc: 1.0, avg_cost: 2.99015129064569e-09\n",
            "batch: 300, train_acc: 1.0, avg_cost: 4.309395122599942e-09\n",
            "batch: 400, train_acc: 1.0, avg_cost: 6.302167318746515e-09\n",
            "batch: 500, train_acc: 1.0, avg_cost: 9.051859952332226e-09\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 1.0, avg_cost: 1.0412831137340823e-08\n",
            "**************************************************\n",
            "test_acc: 0.9305000019073486\n",
            "[0, 0.9307000029087067, 0.9308000034093857, 0.931000002026558, 0.9311000019311905, 0.9316000014543533, 0.9317000019550323]\n",
            "[100, 3.568564394799497e-08, 3.086468087665215e-08, 2.7659949494533216e-08, 2.5750759888495014e-08, 2.5657451887909693e-08, 2.497209338980783e-08, 2.236153118895247e-08, 2.012635757349854e-08, 1.393353366944501e-08, 1.1376502338326477e-08, 1.119566266953126e-08, 1.057978660308953e-08, 1.0412831137340823e-08]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAWKo5C0DuTg",
        "colab_type": "text"
      },
      "source": [
        "# dropout을 적용한 ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chVYxnYcDy49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.00001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-TtqPgDy13",
        "colab_type": "code",
        "outputId": "022081c6-4609-4931-f8c8-32bd5838a805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.5, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0013672564427057903\n",
            "batch: 100, train_acc: 0.7099999785423279, avg_cost: 0.14226533919572823\n",
            "batch: 200, train_acc: 0.5600000023841858, avg_cost: 0.28245035350322734\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.42375343014796574\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.5656221961975098\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7051856792966523\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.6299999952316284, avg_cost: 0.8448312785228091\n",
            "**************************************************\n",
            "test_acc: 0.9285000032186508\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0016069595019022624\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.14254403491814932\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.28085944294929527\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4218203474084537\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.5624298495054249\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7024963088830314\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.5600000023841858, avg_cost: 0.8427510045965523\n",
            "**************************************************\n",
            "test_acc: 0.9291000032424926\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013335500160853069\n",
            "batch: 100, train_acc: 0.6600000262260437, avg_cost: 0.14295987764994306\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.2838984881838161\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4231294970711071\n",
            "batch: 400, train_acc: 0.6700000166893005, avg_cost: 0.5633259006341299\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7054305200775464\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.6399999856948853, avg_cost: 0.8431793001294137\n",
            "**************************************************\n",
            "test_acc: 0.9291000032424926\n",
            "batch: 0, train_acc: 0.5899999737739563, avg_cost: 0.00133525679508845\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.14170120129982627\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.28276032100121173\n",
            "batch: 300, train_acc: 0.6499999761581421, avg_cost: 0.42349551369746524\n",
            "batch: 400, train_acc: 0.6299999952316284, avg_cost: 0.5643402229746183\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7042912874619163\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.5699999928474426, avg_cost: 0.8429041653871534\n",
            "**************************************************\n",
            "test_acc: 0.9294000047445298\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0012710062662760417\n",
            "batch: 100, train_acc: 0.6499999761581421, avg_cost: 0.1392636596163114\n",
            "batch: 200, train_acc: 0.5799999833106995, avg_cost: 0.2789657194415729\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.41881292363007866\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5569523332516348\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.6964289487401648\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.47999998927116394, avg_cost: 0.8356412877639137\n",
            "**************************************************\n",
            "test_acc: 0.92980000436306\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.0014831594626108805\n",
            "batch: 100, train_acc: 0.5199999809265137, avg_cost: 0.13943809390068052\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.2790935831268626\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.41901081611712776\n",
            "batch: 400, train_acc: 0.6800000071525574, avg_cost: 0.5563218327363333\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.6969596994916599\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.5699999928474426, avg_cost: 0.8327766104539236\n",
            "**************************************************\n",
            "test_acc: 0.9294000047445298\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.001422849694887797\n",
            "batch: 100, train_acc: 0.46000000834465027, avg_cost: 0.1418589703241984\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.28306914518276843\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4215793243050576\n",
            "batch: 400, train_acc: 0.6700000166893005, avg_cost: 0.5621165506045023\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7012142206231754\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.5099999904632568, avg_cost: 0.8401187704006838\n",
            "**************************************************\n",
            "test_acc: 0.9297000044584274\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.001383263866106669\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.13881001283725106\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.2764974112311998\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.41759862105051687\n",
            "batch: 400, train_acc: 0.5199999809265137, avg_cost: 0.5584099448720614\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.698052970767021\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.6100000143051147, avg_cost: 0.8348436371485388\n",
            "**************************************************\n",
            "test_acc: 0.9296000039577484\n",
            "batch: 0, train_acc: 0.5400000214576721, avg_cost: 0.0014547119537989298\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.1398785469929377\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.28079079121351247\n",
            "batch: 300, train_acc: 0.6700000166893005, avg_cost: 0.4213828272620838\n",
            "batch: 400, train_acc: 0.6600000262260437, avg_cost: 0.5602619338035583\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.6993245168526967\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.699999988079071, avg_cost: 0.8367318901419643\n",
            "**************************************************\n",
            "test_acc: 0.9293000042438507\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014794390400250752\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.13936151335636773\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.27795129209756847\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.41588138202826175\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5563739885886511\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.6959944155812262\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.5600000023841858, avg_cost: 0.8347695638736087\n",
            "**************************************************\n",
            "test_acc: 0.9299000036716462\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0013123924533526103\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.1413994338115056\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.28215460558732336\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.42073989073435464\n",
            "batch: 400, train_acc: 0.6399999856948853, avg_cost: 0.5609417284528414\n",
            "batch: 500, train_acc: 0.6299999952316284, avg_cost: 0.6991321475307147\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.6000000238418579, avg_cost: 0.8370789701739946\n",
            "**************************************************\n",
            "test_acc: 0.929600003361702\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0011584867040316264\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.14358237942059832\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.28503305276234947\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4185497014721235\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5573256963491436\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.6969946121176082\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.5899999737739563, avg_cost: 0.8377949965000157\n",
            "**************************************************\n",
            "test_acc: 0.9301000034809113\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.001335676411787669\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.1420164174834887\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.28284535974264136\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.4219877005616823\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5590007913112637\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.6965803453326217\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.6299999952316284, avg_cost: 0.8351984744270635\n",
            "**************************************************\n",
            "test_acc: 0.9299000042676926\n",
            "batch: 0, train_acc: 0.5799999833106995, avg_cost: 0.0013189910848935445\n",
            "batch: 100, train_acc: 0.7099999785423279, avg_cost: 0.14173138002554567\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.2828505451480548\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4239245004455245\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5643375254670777\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.7052972710132595\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.6299999952316284, avg_cost: 0.8408264160156241\n",
            "**************************************************\n",
            "test_acc: 0.9300000041723251\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0014737746119499206\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.13856127858161923\n",
            "batch: 200, train_acc: 0.550000011920929, avg_cost: 0.2773377498984336\n",
            "batch: 300, train_acc: 0.5600000023841858, avg_cost: 0.4160932967066768\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.5540589110056561\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.6946167870362601\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.6000000238418579, avg_cost: 0.833405864238739\n",
            "**************************************************\n",
            "test_acc: 0.930100005865097\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0015678703784942627\n",
            "batch: 100, train_acc: 0.6200000047683716, avg_cost: 0.13860806544621784\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.2772591550151507\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.4131600177288056\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5542818226416905\n",
            "batch: 500, train_acc: 0.7300000190734863, avg_cost: 0.6905483784278234\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.6899999976158142, avg_cost: 0.8278015273809435\n",
            "**************************************************\n",
            "test_acc: 0.9295000040531158\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013416757186253865\n",
            "batch: 100, train_acc: 0.6399999856948853, avg_cost: 0.14137534836928048\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.2818913833300273\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.4203083007534348\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.5610273163517319\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.7032043029864637\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.6499999761581421, avg_cost: 0.8406450209021574\n",
            "**************************************************\n",
            "test_acc: 0.9306000053882599\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0014564281702041627\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.1414300445715586\n",
            "batch: 200, train_acc: 0.5299999713897705, avg_cost: 0.28211939762036004\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.42197694629430765\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5623952453335131\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.6992472486694652\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.47999998927116394, avg_cost: 0.8355338717500365\n",
            "**************************************************\n",
            "test_acc: 0.930800005197525\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0014713946978251139\n",
            "batch: 100, train_acc: 0.550000011920929, avg_cost: 0.14097147891918818\n",
            "batch: 200, train_acc: 0.6899999976158142, avg_cost: 0.2779426353176435\n",
            "batch: 300, train_acc: 0.6600000262260437, avg_cost: 0.41789634029070544\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5552022479971251\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.6928281711538634\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.7400000095367432, avg_cost: 0.8300839869181313\n",
            "**************************************************\n",
            "test_acc: 0.9300000041723251\n",
            "batch: 0, train_acc: 0.6899999976158142, avg_cost: 0.001208638350168864\n",
            "batch: 100, train_acc: 0.6000000238418579, avg_cost: 0.1417991716663042\n",
            "batch: 200, train_acc: 0.6800000071525574, avg_cost: 0.28187674721082034\n",
            "batch: 300, train_acc: 0.5099999904632568, avg_cost: 0.42159703294436135\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5609383681416513\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.7007815156380335\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.6000000238418579, avg_cost: 0.8398755640784898\n",
            "**************************************************\n",
            "test_acc: 0.9302000051736832\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013706952333450318\n",
            "batch: 100, train_acc: 0.6299999952316284, avg_cost: 0.14186313033103945\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.2811221754550933\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4211190673708916\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5603910566369696\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.700359347065291\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.6000000238418579, avg_cost: 0.8380993147691108\n",
            "**************************************************\n",
            "test_acc: 0.9310000032186508\n",
            "batch: 0, train_acc: 0.6200000047683716, avg_cost: 0.0013523351152737936\n",
            "batch: 100, train_acc: 0.5400000214576721, avg_cost: 0.1375744766990344\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.2774205224712691\n",
            "batch: 300, train_acc: 0.5, avg_cost: 0.4176729751626652\n",
            "batch: 400, train_acc: 0.5699999928474426, avg_cost: 0.5555569954713184\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.6963485268751779\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.5299999713897705, avg_cost: 0.8332971323529879\n",
            "**************************************************\n",
            "test_acc: 0.9309000039100647\n",
            "batch: 0, train_acc: 0.6399999856948853, avg_cost: 0.0012156692147254943\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.1405545106530189\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.2772405968109769\n",
            "batch: 300, train_acc: 0.6100000143051147, avg_cost: 0.41634515374898934\n",
            "batch: 400, train_acc: 0.5899999737739563, avg_cost: 0.5547256155808772\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.6907472451527918\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.5799999833106995, avg_cost: 0.8279029310743019\n",
            "**************************************************\n",
            "test_acc: 0.9307000041007996\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0015690943598747253\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.13885624319314957\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.2764004103342692\n",
            "batch: 300, train_acc: 0.5799999833106995, avg_cost: 0.4162758821249007\n",
            "batch: 400, train_acc: 0.6499999761581421, avg_cost: 0.5549867127339045\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.6925368281205496\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.6000000238418579, avg_cost: 0.8290791829427085\n",
            "**************************************************\n",
            "test_acc: 0.9309000039100647\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0013620797793070475\n",
            "batch: 100, train_acc: 0.5, avg_cost: 0.13854405303796133\n",
            "batch: 200, train_acc: 0.6200000047683716, avg_cost: 0.2786767154932023\n",
            "batch: 300, train_acc: 0.5699999928474426, avg_cost: 0.417972609003385\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5584600668152172\n",
            "batch: 500, train_acc: 0.6100000143051147, avg_cost: 0.6999145553509394\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.6000000238418579, avg_cost: 0.8360517748196922\n",
            "**************************************************\n",
            "test_acc: 0.9303000050783158\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.0012895063559214275\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.14121197829643892\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.28060011754433317\n",
            "batch: 300, train_acc: 0.5199999809265137, avg_cost: 0.42044672906398767\n",
            "batch: 400, train_acc: 0.5299999713897705, avg_cost: 0.560494703849157\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.7005876353383066\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.6200000047683716, avg_cost: 0.8380395354827244\n",
            "**************************************************\n",
            "test_acc: 0.9310000050067901\n",
            "batch: 0, train_acc: 0.550000011920929, avg_cost: 0.001488437553246816\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.13910740971565252\n",
            "batch: 200, train_acc: 0.5899999737739563, avg_cost: 0.2784723060329758\n",
            "batch: 300, train_acc: 0.550000011920929, avg_cost: 0.41672390043735513\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5561444122592611\n",
            "batch: 500, train_acc: 0.550000011920929, avg_cost: 0.6951111454764999\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.6000000238418579, avg_cost: 0.8329618628819783\n",
            "**************************************************\n",
            "test_acc: 0.9301000046730041\n",
            "batch: 0, train_acc: 0.5600000023841858, avg_cost: 0.0014525359869003295\n",
            "batch: 100, train_acc: 0.5899999737739563, avg_cost: 0.14066699594259263\n",
            "batch: 200, train_acc: 0.6000000238418579, avg_cost: 0.2795937153697015\n",
            "batch: 300, train_acc: 0.6000000238418579, avg_cost: 0.4195636280377709\n",
            "batch: 400, train_acc: 0.6000000238418579, avg_cost: 0.5571879580616953\n",
            "batch: 500, train_acc: 0.6399999856948853, avg_cost: 0.6951262949903808\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.6499999761581421, avg_cost: 0.8317440456151968\n",
            "**************************************************\n",
            "test_acc: 0.9307000041007996\n",
            "batch: 0, train_acc: 0.5699999928474426, avg_cost: 0.0014861192305882771\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14312922696272534\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.28059343288342165\n",
            "batch: 300, train_acc: 0.5899999737739563, avg_cost: 0.4178170914451284\n",
            "batch: 400, train_acc: 0.6200000047683716, avg_cost: 0.5552234139045084\n",
            "batch: 500, train_acc: 0.5799999833106995, avg_cost: 0.6951700826485956\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.5600000023841858, avg_cost: 0.8365549444158878\n",
            "**************************************************\n",
            "test_acc: 0.9301000034809113\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.00157282292842865\n",
            "batch: 100, train_acc: 0.5600000023841858, avg_cost: 0.139067321618398\n",
            "batch: 200, train_acc: 0.6299999952316284, avg_cost: 0.2777282614509265\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.4169798315564789\n",
            "batch: 400, train_acc: 0.5400000214576721, avg_cost: 0.5561938568949697\n",
            "batch: 500, train_acc: 0.5899999737739563, avg_cost: 0.6973327696323396\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.6299999952316284, avg_cost: 0.8350801319877308\n",
            "**************************************************\n",
            "test_acc: 0.9309000033140182\n",
            "batch: 0, train_acc: 0.6700000166893005, avg_cost: 0.001383836269378662\n",
            "batch: 100, train_acc: 0.5799999833106995, avg_cost: 0.13793882459402088\n",
            "batch: 200, train_acc: 0.6100000143051147, avg_cost: 0.27650153656800597\n",
            "batch: 300, train_acc: 0.6499999761581421, avg_cost: 0.41311750233173383\n",
            "batch: 400, train_acc: 0.5799999833106995, avg_cost: 0.554531002144019\n",
            "batch: 500, train_acc: 0.5699999928474426, avg_cost: 0.692744760811329\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.6499999761581421, avg_cost: 0.8293722161650656\n",
            "**************************************************\n",
            "test_acc: 0.930600004196167\n",
            "batch: 0, train_acc: 0.6000000238418579, avg_cost: 0.0015217010180155436\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.13950539489587155\n",
            "batch: 200, train_acc: 0.6700000166893005, avg_cost: 0.27810654709736504\n",
            "batch: 300, train_acc: 0.6200000047683716, avg_cost: 0.4156831243634223\n",
            "batch: 400, train_acc: 0.5, avg_cost: 0.5577891858418784\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.6986910791198421\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.550000011920929, avg_cost: 0.8329478676120451\n",
            "**************************************************\n",
            "test_acc: 0.9302000057697296\n",
            "batch: 0, train_acc: 0.6100000143051147, avg_cost: 0.001386648416519165\n",
            "batch: 100, train_acc: 0.5699999928474426, avg_cost: 0.14143864403168363\n",
            "batch: 200, train_acc: 0.6499999761581421, avg_cost: 0.280634866654873\n",
            "batch: 300, train_acc: 0.6299999952316284, avg_cost: 0.41888767083485945\n",
            "batch: 400, train_acc: 0.550000011920929, avg_cost: 0.5561987511316936\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.6945773038268088\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.7200000286102295, avg_cost: 0.8319690011938409\n",
            "**************************************************\n",
            "test_acc: 0.9306000053882599\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0012002759178479513\n",
            "batch: 100, train_acc: 0.6100000143051147, avg_cost: 0.14102356523275378\n",
            "batch: 200, train_acc: 0.5699999928474426, avg_cost: 0.28221107433239606\n",
            "batch: 300, train_acc: 0.6899999976158142, avg_cost: 0.4216132950782775\n",
            "batch: 400, train_acc: 0.5600000023841858, avg_cost: 0.5601835405826571\n",
            "batch: 500, train_acc: 0.6200000047683716, avg_cost: 0.6994246852397926\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.6499999761581421, avg_cost: 0.8373811306556078\n",
            "**************************************************\n",
            "test_acc: 0.930700004696846\n",
            "batch: 0, train_acc: 0.5199999809265137, avg_cost: 0.0015122535824775695\n",
            "batch: 100, train_acc: 0.5299999713897705, avg_cost: 0.14076532661914823\n",
            "batch: 200, train_acc: 0.6800000071525574, avg_cost: 0.278725341061751\n",
            "batch: 300, train_acc: 0.6899999976158142, avg_cost: 0.41666840632756547\n",
            "batch: 400, train_acc: 0.6100000143051147, avg_cost: 0.5523677468299867\n",
            "batch: 500, train_acc: 0.6000000238418579, avg_cost: 0.691430496076743\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.6100000143051147, avg_cost: 0.8284126911560697\n",
            "**************************************************\n",
            "test_acc: 0.9307000041007996\n",
            "batch: 0, train_acc: 0.6299999952316284, avg_cost: 0.0013708208998044332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f8207d8094eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQp8jc0XcK-M",
        "colab_type": "text"
      },
      "source": [
        "# dropout 0.9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWmTbVJIcNco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.00001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0814_BN_54l_bst_cost_dr0.9.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipsH3qtKcNGL",
        "colab_type": "code",
        "outputId": "3aea2995-39dd-4978-cfd5-73d55097c030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:1, keep_prob_flatten:0.9, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00014928001910448075\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009403084375274678\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.01897650552292665\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.02932942595022421\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.03915649446037911\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04916275852359831\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.9800000190734863, avg_cost: 0.059113255959625036\n",
            "**************************************************\n",
            "test_acc: 0.9300000059604645\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 9.919105097651482e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009492335151880972\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.018880791865910088\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.028198941253746543\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.03768103645183146\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.04766101322447258\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.9900000095367432, avg_cost: 0.05766912842945502\n",
            "**************************************************\n",
            "test_acc: 0.9305000054836273\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0001389828324317932\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.00983013480746498\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.019570414142993597\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.02954117896500975\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03958133976906541\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.04849235925823453\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.949999988079071, avg_cost: 0.0579180879316603\n",
            "**************************************************\n",
            "test_acc: 0.931500004529953\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 7.633623977502187e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009382563672649365\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.018712195111438634\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.02818465483685334\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.03791323501151056\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04693421407602729\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.9599999785423279, avg_cost: 0.05631571204091115\n",
            "**************************************************\n",
            "test_acc: 0.931500004529953\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 6.542352959513664e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009428014949662614\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.01901573288603686\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.028652451310384387\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03820712755473019\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.047794715954417664\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.9900000095367432, avg_cost: 0.05678808425378523\n",
            "**************************************************\n",
            "test_acc: 0.930800005197525\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 6.559622784455617e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009257572836553058\n",
            "batch: 200, train_acc: 0.9200000166893005, avg_cost: 0.01929888947866858\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.02868897147476675\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.038353748965698015\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.04804671939045272\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9599999785423279, avg_cost: 0.057493815968919136\n",
            "**************************************************\n",
            "test_acc: 0.9311000049114228\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 8.852357044816018e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009094128715029608\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.018592883516879145\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.028292442883830538\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.038159153976012036\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04739689339340354\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.9599999785423279, avg_cost: 0.05707285156085464\n",
            "**************************************************\n",
            "test_acc: 0.9316000044345856\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 8.018272618452707e-05\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.009200444218392173\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.018208225836666928\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.02775527926937987\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.037880927802374\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04769388058378058\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9599999785423279, avg_cost: 0.05729752053681303\n",
            "**************************************************\n",
            "test_acc: 0.9312000048160552\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 8.141221478581428e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009265107740648093\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018878091146859042\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.02858480744451907\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.037616696064457465\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.046987138194284274\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9599999785423279, avg_cost: 0.05610612929323303\n",
            "**************************************************\n",
            "test_acc: 0.931500004529953\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00015708229194084803\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009616620998131108\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.019260053823624425\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.02889515018827903\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.038052119058168805\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04769692090456374\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.9900000095367432, avg_cost: 0.057038240151402235\n",
            "**************************************************\n",
            "test_acc: 0.931500004529953\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00014873708287874856\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.00962214071303606\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.018708314956747945\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.028040650866266052\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03758100616338198\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04693584957373484\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9599999785423279, avg_cost: 0.05600460941806279\n",
            "**************************************************\n",
            "test_acc: 0.9305000042915345\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 0.00010841317474842071\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.008872171208883324\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.018419554645661272\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.027879071518934023\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03672602487650389\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.04627972541920221\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 1.0, avg_cost: 0.05577329588481613\n",
            "**************************************************\n",
            "test_acc: 0.9317000037431717\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.856738607088725e-05\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.00985577719906966\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.019500721807902074\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.02857477565140773\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.037718421085737655\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04618836046662182\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.9700000286102295, avg_cost: 0.05555727476157095\n",
            "**************************************************\n",
            "test_acc: 0.9308000040054322\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 9.183398758371671e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009227900138745708\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018321553997908892\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.02777106707285083\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03698236217906623\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.046899888518867894\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.949999988079071, avg_cost: 0.056441572311935834\n",
            "**************************************************\n",
            "test_acc: 0.9312000048160552\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.00016711734235286713\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009114953309763226\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.018114423758840228\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.027096923092249196\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.036103147231842735\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04475946734118527\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.9800000190734863, avg_cost: 0.0536863943277907\n",
            "**************************************************\n",
            "test_acc: 0.9314000052213669\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 7.809512317180634e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009123875428534425\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.01805993826167348\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.026873347897004966\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0365312875555052\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.045365719936089566\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.9900000095367432, avg_cost: 0.05503028518947148\n",
            "**************************************************\n",
            "test_acc: 0.9310000050067901\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0001006979929904143\n",
            "batch: 100, train_acc: 0.9100000262260437, avg_cost: 0.009406763523196181\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.018958119063948585\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.028217074309165285\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03740000599995257\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.04665928896671785\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.9599999785423279, avg_cost: 0.05569198643691687\n",
            "**************************************************\n",
            "test_acc: 0.9308000046014786\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00012472086896498998\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009355264829161264\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018483227523975073\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.028117392843899625\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.03704703758237884\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.04595775898623589\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9700000286102295, avg_cost: 0.0552256815554574\n",
            "**************************************************\n",
            "test_acc: 0.9308000034093857\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 7.650880143046379e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.00974512274609879\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.019283561033662404\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.02821312630393852\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03748140502633762\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04676517079467886\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.949999988079071, avg_cost: 0.05616749541872801\n",
            "**************************************************\n",
            "test_acc: 0.9309000021219254\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00010444308320681254\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009634244168798128\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.01815150920456897\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.02685559662214171\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.035872006447364886\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.04469435015072423\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.9800000190734863, avg_cost: 0.05357017296366394\n",
            "**************************************************\n",
            "test_acc: 0.9301000040769577\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 8.054656907916069e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.008932917783968146\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.018300418575915203\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.027590649865257253\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.03672950933765\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.04605786043917761\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.949999988079071, avg_cost: 0.055007258556627976\n",
            "**************************************************\n",
            "test_acc: 0.9306000036001205\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00011781306316455206\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.009782691633639236\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.019616318577124443\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.02842084843548948\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.037631700039298\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04668972810613925\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.9599999785423279, avg_cost: 0.05559598048101183\n",
            "**************************************************\n",
            "test_acc: 0.9312000042200088\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 8.890132730205854e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.008990540748151636\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.01816423296385134\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.027139800852940717\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03648741558892654\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.044951823985126524\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9599999785423279, avg_cost: 0.05413518067109788\n",
            "**************************************************\n",
            "test_acc: 0.9312000048160552\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.7280897075931234e-05\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.009423760499921626\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.018061474201385864\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.027246665246590657\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03582934303345\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04473785343094882\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9900000095367432, avg_cost: 0.053555446693790136\n",
            "**************************************************\n",
            "test_acc: 0.9314000034332275\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 5.563239877422651e-05\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.009397876306126514\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.019051226756225025\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.028133341971163947\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.037334252023914195\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.04658132917170101\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.9599999785423279, avg_cost: 0.056166152940907814\n",
            "**************************************************\n",
            "test_acc: 0.9308000040054322\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 9.532894318302473e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009430854174618924\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.01872570984531194\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.0284496265494575\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.03729906793218106\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.046307195397093895\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.949999988079071, avg_cost: 0.05501195156869167\n",
            "**************************************************\n",
            "test_acc: 0.9310000038146973\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 4.157307247320811e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009600858835813901\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018840124442067462\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.027318347155038895\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03596995697938838\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04538080760083783\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9800000190734863, avg_cost: 0.054503706208973504\n",
            "**************************************************\n",
            "test_acc: 0.9315000027418137\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 9.95557631055514e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.008847583301054935\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.01839517617753397\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.026840726590404904\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.036414731898500266\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04526072356151414\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9599999785423279, avg_cost: 0.05355124653860305\n",
            "**************************************************\n",
            "test_acc: 0.9314000034332275\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.33157387872537e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009220567295172564\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018712394893324612\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.027894997467131654\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.036482985655311474\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.04543153169021627\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.9700000286102295, avg_cost: 0.054090153476378564\n",
            "**************************************************\n",
            "test_acc: 0.9313000029325486\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.9940806950132052e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.008531884312008822\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.017746125406896077\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.026689724352521198\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.035913489507220224\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.04520248861721485\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9800000190734863, avg_cost: 0.05400904846426176\n",
            "**************************************************\n",
            "test_acc: 0.9307000017166138\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0001245976984500885\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.008919800077565014\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.018339104504945376\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.027297245110385112\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.036734911734238235\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04516279306456755\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9700000286102295, avg_cost: 0.054078040277042105\n",
            "**************************************************\n",
            "test_acc: 0.9305000030994415\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 9.938798223932585e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009182058469208035\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.017961554667417657\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.02701635171945479\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.036174911296063005\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.044797469287247316\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.9700000286102295, avg_cost: 0.05370287844964573\n",
            "**************************************************\n",
            "test_acc: 0.9299000018835067\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00013625089079141617\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009175270743823303\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.018220215342783688\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.02767615501458448\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.037013895572163204\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.046093583873783615\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.9599999785423279, avg_cost: 0.05505593378096825\n",
            "**************************************************\n",
            "test_acc: 0.9306000024080276\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00011501396695772807\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.008988824929110704\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.018257198465677596\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.026638343972153956\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.035526104955934\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.0448283314503109\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.949999988079071, avg_cost: 0.053550395224398695\n",
            "**************************************************\n",
            "test_acc: 0.9301000022888184\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 8.329593886931737e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.0098688288545236\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.01847891520010308\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.02730172727511672\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03608707013173747\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04538574289474731\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.949999988079071, avg_cost: 0.05459735809148695\n",
            "**************************************************\n",
            "test_acc: 0.9300000017881394\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00012356063971916833\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009023347531086376\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018064075744041487\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.02763517751896871\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.03701870511979602\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.045549241744398984\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9599999785423279, avg_cost: 0.054457704084100715\n",
            "**************************************************\n",
            "test_acc: 0.9306000018119812\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00014815890540679296\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.008755935097578917\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.017877905771601955\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.027878331972751778\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03680532995844262\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04581131340858221\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.9599999785423279, avg_cost: 0.05459754616565386\n",
            "**************************************************\n",
            "test_acc: 0.9306000012159348\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0001499508321285248\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.009241316132247447\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.018486923798918732\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.027135881165352958\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.03641139403946\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04531501904159086\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.9700000286102295, avg_cost: 0.05428563745333426\n",
            "**************************************************\n",
            "test_acc: 0.9306000012159348\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 7.897629713018735e-05\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.00914641194511205\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.01836906610444809\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.027341244105094423\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.03653880714584376\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.04537720395785681\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 0.949999988079071, avg_cost: 0.05404658269748324\n",
            "**************************************************\n",
            "test_acc: 0.9300000023841858\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 9.474091231822967e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009149492094293235\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.017696448679392532\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.026808083459424488\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03612600371552012\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04513336262743299\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.9800000190734863, avg_cost: 0.053510644828590255\n",
            "**************************************************\n",
            "test_acc: 0.9312000018358231\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00010674916207790374\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.00925327790658533\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.017986294360889582\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.026795452990627376\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.035580340388599606\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.044585414064640595\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 0.9700000286102295, avg_cost: 0.05409431493365744\n",
            "**************************************************\n",
            "test_acc: 0.9309000021219254\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.29589863618215e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.009501376956080396\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.01859742111836871\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.02737864605582824\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.036602966542947514\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.04539685537849437\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 0.9700000286102295, avg_cost: 0.05483098652490301\n",
            "**************************************************\n",
            "test_acc: 0.9309000039100647\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00015928251047929128\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.009195722850660485\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.01786456051903467\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.02707335410794864\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.03629613840642075\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04600820286665109\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 0.9700000286102295, avg_cost: 0.053513511825973734\n",
            "**************************************************\n",
            "test_acc: 0.9306000036001205\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0001495683069030444\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.008834210125787651\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018027067418597647\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.027130645686274855\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.036185687897241206\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.04500159541655192\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 0.9900000095367432, avg_cost: 0.05416228105591473\n",
            "**************************************************\n",
            "test_acc: 0.9309000021219254\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 5.997379620869955e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.00898540154875567\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.018596546019737918\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.027779235237588483\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.037031336703027266\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.04547922232576334\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 0.9399999976158142, avg_cost: 0.05444147386355324\n",
            "**************************************************\n",
            "test_acc: 0.9306000018119812\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 3.490829529861609e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.008698700401000677\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.017277488416681683\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.026005117614392153\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.03471424485613776\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.04356399465874951\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 0.9599999785423279, avg_cost: 0.05294589618210009\n",
            "**************************************************\n",
            "test_acc: 0.9307000005245208\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00013445701450109481\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.008998856307783474\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.017630205696914335\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.026320639785844837\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.03485207702498883\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.043960097064264125\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 0.949999988079071, avg_cost: 0.053122241501696435\n",
            "**************************************************\n",
            "test_acc: 0.930200001001358\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 8.402345702052116e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009212607268709691\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.018142755286923298\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.027303781216808893\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.036070100266952065\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.04549354510925089\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 0.9700000286102295, avg_cost: 0.053637067008918786\n",
            "**************************************************\n",
            "test_acc: 0.9307000011205673\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 5.3474909315506617e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.008816852931243681\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.0174080532308047\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.02620398194529117\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.03504715779175361\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.043241585909078506\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 0.9700000286102295, avg_cost: 0.05195838867376247\n",
            "**************************************************\n",
            "test_acc: 0.9309000015258789\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00012935904165109\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.008736480212149523\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.017406504222502315\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.02648071823641657\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.03554488929842287\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.04444314369776595\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 0.949999988079071, avg_cost: 0.0529347041264797\n",
            "**************************************************\n",
            "test_acc: 0.9313000005483627\n",
            "[0, 0.9300000059604645, 0.9305000054836273, 0.931500004529953, 0.9316000044345856, 0.9317000037431717]\n",
            "[100, 0.059113255959625036, 0.05766912842945502, 0.05631571204091115, 0.05610612929323303, 0.05600460941806279, 0.05577329588481613, 0.05555727476157095, 0.0536863943277907, 0.05357017296366394, 0.053555446693790136, 0.05355124653860305, 0.053550395224398695, 0.053510644828590255, 0.05294589618210009, 0.05195838867376247]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nym-AVeBIf66",
        "colab_type": "text"
      },
      "source": [
        "# 32후 maxpool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qVAWdRFIiXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0815_BN_54l_bst_cost_3.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0815_BN_54l_bst_acc_3.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0815_BN_54l_bst_cost_3.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1 = make_conv_once_wo_pooling(h1, 16, keep_prob, '1b', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "h1 = make_conv_once_wo_pooling(h1, 16, keep_prob, '1c', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[64,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZScdqvImImqS",
        "colab_type": "code",
        "outputId": "023b2905-7e22-44d2-aff9-90bf85532f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:1, keep_prob_flatten:1, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.00038107750316460927\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.03238856510569651\n",
            "batch: 200, train_acc: 0.8600000143051147, avg_cost: 0.0672397422045469\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.10062435284256925\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.13650368046015499\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.17484475293507182\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.8799999952316284, avg_cost: 0.21053836705784007\n",
            "**************************************************\n",
            "test_acc: 0.8745999991893768\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0004257612923781077\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.02984030584494274\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.060500100540618114\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.09061081683884063\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.12004650690903265\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.14988193965206534\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.949999988079071, avg_cost: 0.18328652394314598\n",
            "**************************************************\n",
            "test_acc: 0.879400001168251\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00014224978784720102\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.024109929315745838\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.04907784088204306\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.07387151135131714\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.09792119029909366\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.12297073656693092\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.949999988079071, avg_cost: 0.15055851144095242\n",
            "**************************************************\n",
            "test_acc: 0.8871999996900558\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00016537810365358988\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.021156880315393208\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.042236312180757525\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.0639450622846683\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.0883381060262521\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.11147527972236274\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.8799999952316284, avg_cost: 0.13831500938162208\n",
            "**************************************************\n",
            "test_acc: 0.8819999980926514\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00031446787218252815\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.018627088007827586\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.03828264259422813\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.05846460146208596\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.07838609157750988\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.1002596197277306\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.9399999976158142, avg_cost: 0.11943527496109387\n",
            "**************************************************\n",
            "test_acc: 0.8859000009298325\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.0001722188914815585\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.016643532607704403\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.0348685749620199\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.0526572963098685\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.07210091243808468\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.0909130875890454\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9300000071525574, avg_cost: 0.11125683295850952\n",
            "**************************************************\n",
            "test_acc: 0.8834000015258789\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.0001052672415971756\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.01388204430229962\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.029776461860164992\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.04531318486978609\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.060577264962097\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.07882693639025085\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.9599999785423279, avg_cost: 0.0974181589049596\n",
            "**************************************************\n",
            "test_acc: 0.8864000004529953\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00011546142399311065\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.011378458919934924\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.024591152595045674\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.03565603142138572\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.049782464269859106\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.06674001755658539\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9599999785423279, avg_cost: 0.08096236084122212\n",
            "**************************************************\n",
            "test_acc: 0.8792000019550323\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00014334907134373983\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.010420512820904454\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.021929966607131063\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.034447350041009496\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.04575830531461785\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.058085924823147554\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9700000286102295, avg_cost: 0.07082312068125853\n",
            "**************************************************\n",
            "test_acc: 0.8854000002145768\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00010565593838691711\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.009065908162544172\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.018233284327822424\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.03125177955410133\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.04158743554105362\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.05339058114836616\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.9900000095367432, avg_cost: 0.06904902343017365\n",
            "**************************************************\n",
            "test_acc: 0.866499999165535\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00018742612252632777\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0105509376556923\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.021657542952646807\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.030338468799988423\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.039437598764585975\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.05058739346296842\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9700000286102295, avg_cost: 0.06015191127546139\n",
            "**************************************************\n",
            "test_acc: 0.8782000005245209\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0001592319334546725\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.007430176169145851\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.014128796085715293\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.021786662599382296\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.03131886902420472\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.04305047063933063\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.9399999976158142, avg_cost: 0.05647876561464119\n",
            "**************************************************\n",
            "test_acc: 0.8829000002145767\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 7.254208748539288e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.007091523744165898\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.014325827316691478\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.021225911469664422\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.030500065623782567\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.04049505006987599\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.9399999976158142, avg_cost: 0.04891214165681347\n",
            "**************************************************\n",
            "test_acc: 0.8710000002384186\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 7.903192192316056e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0058875011334506185\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011062767965874326\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.01666782077945148\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.023978575721072648\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.03167950610940653\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.949999988079071, avg_cost: 0.04227107263403009\n",
            "**************************************************\n",
            "test_acc: 0.8771000003814697\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 9.504899382591248e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.006836559126774474\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.01343261071558421\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.020365588066633793\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.02927389784948901\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.03985110839751237\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.9800000190734863, avg_cost: 0.04848898547546315\n",
            "**************************************************\n",
            "test_acc: 0.8881000012159348\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.4340303304294746e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.0051140153501182785\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.010607892883320648\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.016581802854780114\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.022014137215058644\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.027559917217974267\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.9800000190734863, avg_cost: 0.0371962952462491\n",
            "**************************************************\n",
            "test_acc: 0.8839000004529953\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 7.345107694466909e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.005167191434884445\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.009482188530964776\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.015413016779736296\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.021775187428186946\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.02983573289626897\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.9800000190734863, avg_cost: 0.03657073139251831\n",
            "**************************************************\n",
            "test_acc: 0.8813000011444092\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.327912487089634e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.00424531622013698\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008385785227874296\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.015456380263203751\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.021626602938243495\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.029568701457077014\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9599999785423279, avg_cost: 0.03622540264079968\n",
            "**************************************************\n",
            "test_acc: 0.8813999998569488\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00010001922647158304\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.007440652648995942\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.012103204961167648\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.016132360199311125\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01996787842789975\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.025231671729125084\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9800000190734863, avg_cost: 0.03221385271560091\n",
            "**************************************************\n",
            "test_acc: 0.8841999995708466\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 6.98489509522915e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.006014985655977702\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011228031190888341\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.018888984301981195\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.02470960038675304\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.030046081897259356\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.9800000190734863, avg_cost: 0.036412824958485204\n",
            "**************************************************\n",
            "test_acc: 0.8813000017404556\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 4.5245053867499035e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.00553325682024782\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.011357872524919611\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.015543785962897048\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.021034979114774616\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02635612570680676\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.9800000190734863, avg_cost: 0.03134033342357724\n",
            "**************************************************\n",
            "test_acc: 0.8869000017642975\n",
            "batch: 0, train_acc: 1.0, avg_cost: 9.764227240035931e-06\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.005274053824832663\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.011834274755868443\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.016582038932441118\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.021701310865852676\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.026240798065555283\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.9900000095367432, avg_cost: 0.033403490594937454\n",
            "**************************************************\n",
            "test_acc: 0.8872000008821488\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.0585828460752966e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.003988461093783067\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.0070892177980082715\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.012185557427777292\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0184430659688466\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.022806101545769102\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9900000095367432, avg_cost: 0.028117444276188824\n",
            "**************************************************\n",
            "test_acc: 0.8809000027179718\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 3.9633242413401606e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0034548288302903535\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.007870871826162328\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.012006974186757965\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.015729519191518188\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.02123968947431421\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9900000095367432, avg_cost: 0.025917983236965175\n",
            "**************************************************\n",
            "test_acc: 0.8889000004529953\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.04781372162203e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0036486492267188923\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006564967215526846\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.01083607524012526\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01847158657930171\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.024863794172803565\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.9599999785423279, avg_cost: 0.03072409808713321\n",
            "**************************************************\n",
            "test_acc: 0.8861000007390976\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.4531450364738704e-06\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.00521481141525631\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008384146535730296\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.013301511433479981\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.01982657187695924\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.02648193724647474\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 1.0, avg_cost: 0.030864514485777657\n",
            "**************************************************\n",
            "test_acc: 0.8866000002622605\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.9905973846713702e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.0025390954919081816\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0051962681764659195\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.007700191473316716\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.010806994220668768\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.013998150365659975\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9900000095367432, avg_cost: 0.01875438766020429\n",
            "**************************************************\n",
            "test_acc: 0.8818999993801117\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.80432998140653e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002925940669859605\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005285712355883638\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.011001805133322096\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.017080815737814795\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.022695334865420597\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9700000286102295, avg_cost: 0.029744143883484273\n",
            "**************************************************\n",
            "test_acc: 0.884400001168251\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.128010964952409e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0051979012517646556\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008979512798153639\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.013071904359288348\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01834500866389134\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.022374999581467513\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 1.0, avg_cost: 0.026601246890204478\n",
            "**************************************************\n",
            "test_acc: 0.8892999994754791\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.5462883058935405e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0038112075732594044\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.007374919600260907\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.010709638957438674\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.013124213191137333\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.016301282448500085\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9900000095367432, avg_cost: 0.02035319202792987\n",
            "**************************************************\n",
            "test_acc: 0.8832999980449676\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 5.2963880201180776e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003664559535779213\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.007622815267532131\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.01214655940207497\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.0216169663578815\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.027959708641671258\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9900000095367432, avg_cost: 0.03379212103784083\n",
            "**************************************************\n",
            "test_acc: 0.8848000001907349\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 4.046007680396239e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.002666594684512043\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.005602381501521445\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.00883652178745251\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.012632186596844501\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.016517820713440108\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 1.0, avg_cost: 0.02072464289880977\n",
            "**************************************************\n",
            "test_acc: 0.8889000022411346\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00012936779608329138\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.00302628120019411\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005094639908250728\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.006723076169388751\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.008845012504267889\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.013157924106781129\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.9900000095367432, avg_cost: 0.01888979667681269\n",
            "**************************************************\n",
            "test_acc: 0.8809999984502792\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.2625294600923856e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0029216827086929703\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005889416965025397\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.009758271853982772\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.017152335353118046\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.023405583875525417\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 1.0, avg_cost: 0.027887555203439367\n",
            "**************************************************\n",
            "test_acc: 0.8848999983072281\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.358808639148871e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0037014554438064805\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006178744324424768\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.009692427333211524\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.012939229734862837\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.016222294171069136\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.9900000095367432, avg_cost: 0.01990252448798858\n",
            "**************************************************\n",
            "test_acc: 0.8892000007629395\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.7891042797515788e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0021437870873584567\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.004160202335381958\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.007622905118963297\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.011858757207470873\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.016547603926301245\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9900000095367432, avg_cost: 0.021870765174147305\n",
            "**************************************************\n",
            "test_acc: 0.8832000017166137\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 6.163494661450386e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.004244823116653909\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.008667047257185907\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.013021166746912052\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.01717108325169343\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.019744198805904785\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 1.0, avg_cost: 0.023605335643126925\n",
            "**************************************************\n",
            "test_acc: 0.8879999977350235\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 1.787558818856875e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.003031601422165598\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.0059896125427621886\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.008559142989154984\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.012120483320322825\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01436500664800406\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 1.0, avg_cost: 0.017786876871056543\n",
            "**************************************************\n",
            "test_acc: 0.8834000015258789\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.2554846170047918e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003232324352720753\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.005929559389575542\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.008972304056078429\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.01257885710932896\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.01609773661189439\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 0.9900000095367432, avg_cost: 0.018435596474106773\n",
            "**************************************************\n",
            "test_acc: 0.8900999975204468\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.9688743244235715e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0013482565585582048\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.0050171141571384685\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.008585291202519631\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.011685107623682902\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.018492876355643\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.9700000286102295, avg_cost: 0.025754224992851953\n",
            "**************************************************\n",
            "test_acc: 0.8872000014781952\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 8.345502118269603e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.004537042370454098\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.008202652369897501\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.011024902211114141\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.014059999509336191\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.0168819671710662\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 0.9900000095367432, avg_cost: 0.020172630909558703\n",
            "**************************************************\n",
            "test_acc: 0.8865000027418136\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.489102577480177e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0019728145379485787\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.003957196836393751\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.006682216409802392\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.009576521127867936\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.01213072995439385\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 0.9800000190734863, avg_cost: 0.015041635733578002\n",
            "**************************************************\n",
            "test_acc: 0.8916000020503998\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 1.970556719849507e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0020011998702830175\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.003565518370038869\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.005718472773175258\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.008806728115135531\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.013018550136015014\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 0.9900000095367432, avg_cost: 0.01768481991595764\n",
            "**************************************************\n",
            "test_acc: 0.8881999987363816\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.1297762393951415e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.004235267731564819\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.007166483061931407\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.010056071743310897\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.013750074700704622\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.020774269313842526\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 0.9900000095367432, avg_cost: 0.02694065256383814\n",
            "**************************************************\n",
            "test_acc: 0.8851999992132187\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.3597055027882259e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002926456072988609\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0047724333065464935\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.006340386704444733\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.008019289606454549\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.009655274359150395\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 1.0, avg_cost: 0.012103525173370131\n",
            "**************************************************\n",
            "test_acc: 0.8874999994039535\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.9397314948340256e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.001692194225688581\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.0037911759189106897\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.006297686445492828\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.008484469532437896\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.012490930534137678\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 0.9900000095367432, avg_cost: 0.017299374554107384\n",
            "**************************************************\n",
            "test_acc: 0.8910000044107437\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.8722321838140486e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.003534140568663132\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006191108091249285\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.008179525793190501\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.012924981497587093\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.01868687443760186\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 1.0, avg_cost: 0.022477819864628466\n",
            "**************************************************\n",
            "test_acc: 0.886700000166893\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.2134717932591836e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.002092433559979933\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.004579645262371437\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.007469876134515899\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.01050681366362067\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.01362527722356997\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 0.9900000095367432, avg_cost: 0.015755020336752454\n",
            "**************************************************\n",
            "test_acc: 0.8895000004768372\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.6475377380847932e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0011940506398483795\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.002677012763609429\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.004790420469495679\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.007859843709144719\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.012019998317688687\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 0.9800000190734863, avg_cost: 0.01660351201745167\n",
            "**************************************************\n",
            "test_acc: 0.8894000005722046\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 2.9538339003920554e-05\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.0037390542883319246\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.006204205886436592\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.009051737191596965\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0115384792675953\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.013311815033691056\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 1.0, avg_cost: 0.016530136954558364\n",
            "**************************************************\n",
            "test_acc: 0.8886999970674515\n",
            "[0, 0.8745999991893768, 0.879400001168251, 0.8871999996900558, 0.8881000012159348, 0.8889000004529953, 0.8892999994754791, 0.8900999975204468, 0.8916000020503998]\n",
            "[100, 0.21053836705784007, 0.18328652394314598, 0.15055851144095242, 0.13831500938162208, 0.11943527496109387, 0.11125683295850952, 0.0974181589049596, 0.08096236084122212, 0.07082312068125853, 0.06904902343017365, 0.06015191127546139, 0.05647876561464119, 0.04891214165681347, 0.04227107263403009, 0.0371962952462491, 0.03657073139251831, 0.03622540264079968, 0.03221385271560091, 0.03134033342357724, 0.028117444276188824, 0.025917983236965175, 0.01875438766020429, 0.017786876871056543, 0.015041635733578002, 0.012103525173370131]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzMMSzKUjjAU",
        "colab_type": "text"
      },
      "source": [
        "# Broad Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibmglPLgjjlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 32 channel 단층으로 전환\n",
        "2. #filter를 32-64-128로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0815_BN_54l_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0815_BN_54l_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0815_BN_54l_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 32, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [32,32,32]\n",
        "iList1_1 = [32,32,64]\n",
        "iList2 = [64,64,64]\n",
        "iList2_1 = [64,64,128]\n",
        "iList3 = [128,128,128]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):0\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "h19 = tf.nn.max_pool(h19, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 55)\n",
        "h55 = tf.nn.max_pool(h55, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h55)\n",
        "  \n",
        "h56 = tf.nn.avg_pool(h55, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h56_flat = tf.reshape(h56, [-1,128])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[128,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h56_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVHoapsQjiv6",
        "colab_type": "code",
        "outputId": "e8e684e8-cb47-4ee5-e8da-1d8312274868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:1, keep_prob_flatten:0.9, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.14000000059604645, avg_cost: 0.016808791160583494\n",
            "batch: 100, train_acc: 0.7599999904632568, avg_cost: 0.25777745485305803\n",
            "batch: 200, train_acc: 0.7599999904632568, avg_cost: 0.3869866437713308\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 0.5061947311460975\n",
            "batch: 400, train_acc: 0.7599999904632568, avg_cost: 0.6131939578553041\n",
            "batch: 500, train_acc: 0.7900000214576721, avg_cost: 0.7092880363762378\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.8199999928474426, avg_cost: 0.804229662021002\n",
            "**************************************************\n",
            "test_acc: 0.8224999976158142\n",
            "batch: 0, train_acc: 0.7599999904632568, avg_cost: 0.0012449613213539124\n",
            "batch: 100, train_acc: 0.7599999904632568, avg_cost: 0.09131325448552764\n",
            "batch: 200, train_acc: 0.8799999952316284, avg_cost: 0.17208044076959295\n",
            "batch: 300, train_acc: 0.8600000143051147, avg_cost: 0.24883734159171586\n",
            "batch: 400, train_acc: 0.7799999713897705, avg_cost: 0.32841692345837775\n",
            "batch: 500, train_acc: 0.8100000023841858, avg_cost: 0.4083035135020814\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.8600000143051147, avg_cost: 0.48614370430509274\n",
            "**************************************************\n",
            "test_acc: 0.8788999980688095\n",
            "batch: 0, train_acc: 0.8100000023841858, avg_cost: 0.0007404556373755137\n",
            "batch: 100, train_acc: 0.8299999833106995, avg_cost: 0.07043356992304327\n",
            "batch: 200, train_acc: 0.8299999833106995, avg_cost: 0.14111444711685192\n",
            "batch: 300, train_acc: 0.7900000214576721, avg_cost: 0.2099496641010047\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.2793827542662622\n",
            "batch: 500, train_acc: 0.8199999928474426, avg_cost: 0.34956166043877634\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.8999999761581421, avg_cost: 0.4147733908146623\n",
            "**************************************************\n",
            "test_acc: 0.8910000002384186\n",
            "batch: 0, train_acc: 0.8799999952316284, avg_cost: 0.0005593023200829824\n",
            "batch: 100, train_acc: 0.8600000143051147, avg_cost: 0.05844708065191906\n",
            "batch: 200, train_acc: 0.8299999833106995, avg_cost: 0.11822968669235702\n",
            "batch: 300, train_acc: 0.8399999737739563, avg_cost: 0.17489982513089977\n",
            "batch: 400, train_acc: 0.8299999833106995, avg_cost: 0.23449354271094\n",
            "batch: 500, train_acc: 0.8600000143051147, avg_cost: 0.29415406552453827\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.8899999856948853, avg_cost: 0.355751197511951\n",
            "**************************************************\n",
            "test_acc: 0.887700001001358\n",
            "batch: 0, train_acc: 0.8700000047683716, avg_cost: 0.0006963068246841431\n",
            "batch: 100, train_acc: 0.8799999952316284, avg_cost: 0.05680500673751035\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.1088014705230792\n",
            "batch: 300, train_acc: 0.8500000238418579, avg_cost: 0.16337022356688985\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.2182013685132067\n",
            "batch: 500, train_acc: 0.8299999833106995, avg_cost: 0.2814566906665763\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.8100000023841858, avg_cost: 0.33643747678647457\n",
            "**************************************************\n",
            "test_acc: 0.8968000000715256\n",
            "batch: 0, train_acc: 0.8999999761581421, avg_cost: 0.0005305083096027374\n",
            "batch: 100, train_acc: 0.8100000023841858, avg_cost: 0.05280222517748674\n",
            "batch: 200, train_acc: 0.8899999856948853, avg_cost: 0.10271331484119099\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.15313115509847805\n",
            "batch: 400, train_acc: 0.800000011920929, avg_cost: 0.20616825886070722\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.2595471096783876\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 0.9300000071525574, avg_cost: 0.30815361256400753\n",
            "**************************************************\n",
            "test_acc: 0.8985000032186509\n",
            "batch: 0, train_acc: 0.8100000023841858, avg_cost: 0.0007302347322305044\n",
            "batch: 100, train_acc: 0.8399999737739563, avg_cost: 0.041913700513541714\n",
            "batch: 200, train_acc: 0.8700000047683716, avg_cost: 0.08807553718487425\n",
            "batch: 300, train_acc: 0.8899999856948853, avg_cost: 0.13280865467463931\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.17714910332734374\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.2288382034935057\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 0.9200000166893005, avg_cost: 0.274462412465364\n",
            "**************************************************\n",
            "test_acc: 0.8959000009298325\n",
            "batch: 0, train_acc: 0.8500000238418579, avg_cost: 0.0005124372243881226\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.03988743145018816\n",
            "batch: 200, train_acc: 0.8899999856948853, avg_cost: 0.08338509865105152\n",
            "batch: 300, train_acc: 0.8700000047683716, avg_cost: 0.1281410790483157\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.17184336407730996\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.21682096069057802\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 0.9399999976158142, avg_cost: 0.2650184204305215\n",
            "**************************************************\n",
            "test_acc: 0.9005000001192093\n",
            "batch: 0, train_acc: 0.8700000047683716, avg_cost: 0.00042441248893737793\n",
            "batch: 100, train_acc: 0.9100000262260437, avg_cost: 0.03480921698113283\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.07245397032548992\n",
            "batch: 300, train_acc: 0.8799999952316284, avg_cost: 0.11389876302331697\n",
            "batch: 400, train_acc: 0.8700000047683716, avg_cost: 0.15273578159511098\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.1927084073051812\n",
            "**************************************************\n",
            "epoch: 8, train_acc: 0.9300000071525574, avg_cost: 0.22922350328415667\n",
            "**************************************************\n",
            "test_acc: 0.9089000022411347\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0002949179708957672\n",
            "batch: 100, train_acc: 0.9100000262260437, avg_cost: 0.0324152114863197\n",
            "batch: 200, train_acc: 0.9200000166893005, avg_cost: 0.062493990249931776\n",
            "batch: 300, train_acc: 0.8600000143051147, avg_cost: 0.09565582446753976\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.1281409523636102\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.16657656542956828\n",
            "**************************************************\n",
            "epoch: 9, train_acc: 0.8999999761581421, avg_cost: 0.20349514762560522\n",
            "**************************************************\n",
            "test_acc: 0.9058000016212463\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.0003032950311899185\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.029008971961836005\n",
            "batch: 200, train_acc: 0.9100000262260437, avg_cost: 0.06184627739091712\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.09572105389088387\n",
            "batch: 400, train_acc: 0.8899999856948853, avg_cost: 0.13067916826655465\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.1643460517252485\n",
            "**************************************************\n",
            "epoch: 10, train_acc: 0.9200000166893005, avg_cost: 0.19848077864696598\n",
            "**************************************************\n",
            "test_acc: 0.9023000031709671\n",
            "batch: 0, train_acc: 0.8999999761581421, avg_cost: 0.0003205957512060801\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.027328572968641923\n",
            "batch: 200, train_acc: 0.8899999856948853, avg_cost: 0.055536346634228996\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.0847055534645915\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.11442607135822376\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.1425667462560038\n",
            "**************************************************\n",
            "epoch: 11, train_acc: 0.8999999761581421, avg_cost: 0.17320114228874425\n",
            "**************************************************\n",
            "test_acc: 0.9064999997615815\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.0002618740995724996\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.02258063207070033\n",
            "batch: 200, train_acc: 0.9200000166893005, avg_cost: 0.04876330627128482\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.07636134898290044\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.10539673687890176\n",
            "batch: 500, train_acc: 0.8899999856948853, avg_cost: 0.1343415721630058\n",
            "**************************************************\n",
            "epoch: 12, train_acc: 0.9599999785423279, avg_cost: 0.16654243931174287\n",
            "**************************************************\n",
            "test_acc: 0.9077999991178513\n",
            "batch: 0, train_acc: 0.9100000262260437, avg_cost: 0.00020645743856827418\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.02097220222776134\n",
            "batch: 200, train_acc: 0.9100000262260437, avg_cost: 0.0436201353619496\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.06789623793835439\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.09232738563790913\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.12031444461395338\n",
            "**************************************************\n",
            "epoch: 13, train_acc: 0.9399999976158142, avg_cost: 0.14720057868088285\n",
            "**************************************************\n",
            "test_acc: 0.9058000034093857\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.0002764531970024109\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.020279100450376664\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.041907542062302426\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.06532636171517273\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.09046400555409492\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.11733324872019395\n",
            "**************************************************\n",
            "epoch: 14, train_acc: 0.9300000071525574, avg_cost: 0.14536132596122728\n",
            "**************************************************\n",
            "test_acc: 0.9039000004529953\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.0001728226989507675\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.021173507949958238\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.04170462472985188\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.06089192956686019\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.08184509545564654\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.10475192022820314\n",
            "**************************************************\n",
            "epoch: 15, train_acc: 0.949999988079071, avg_cost: 0.1268017622393866\n",
            "**************************************************\n",
            "test_acc: 0.9136000037193298\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00017982634405295055\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.018132667876780036\n",
            "batch: 200, train_acc: 0.9200000166893005, avg_cost: 0.036638286455223955\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.058104278954366866\n",
            "batch: 400, train_acc: 0.9100000262260437, avg_cost: 0.08224207546561967\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.10433674788723407\n",
            "**************************************************\n",
            "epoch: 16, train_acc: 0.949999988079071, avg_cost: 0.12557169475903135\n",
            "**************************************************\n",
            "test_acc: 0.9032000005245209\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00018893714994192124\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.016905560425172245\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.034417088019351165\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.05096616965718568\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.07136373427075642\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.09213681813019016\n",
            "**************************************************\n",
            "epoch: 17, train_acc: 0.9300000071525574, avg_cost: 0.11387653847845894\n",
            "**************************************************\n",
            "test_acc: 0.913000003695488\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 0.00013637671868006388\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.018733329679816963\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.03596828345519801\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.05519776719622311\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.0736623610882088\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.09715379686560478\n",
            "**************************************************\n",
            "epoch: 18, train_acc: 0.9200000166893005, avg_cost: 0.11976451405168818\n",
            "**************************************************\n",
            "test_acc: 0.9063000023365021\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.00016329040129979452\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.020634911749511962\n",
            "batch: 200, train_acc: 0.9300000071525574, avg_cost: 0.042188773627082524\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.06253323782856268\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.08470280670250457\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.10724043158814314\n",
            "**************************************************\n",
            "epoch: 19, train_acc: 0.949999988079071, avg_cost: 0.13060815119184552\n",
            "**************************************************\n",
            "test_acc: 0.9068999999761581\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00021783644954363504\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.018685024045407773\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.03403913415968418\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.04999753694981335\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.07082100399769845\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.08983090348852175\n",
            "**************************************************\n",
            "epoch: 20, train_acc: 0.949999988079071, avg_cost: 0.11138089534826576\n",
            "**************************************************\n",
            "test_acc: 0.9099000012874603\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0003119797756274541\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.015032507671664159\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.029478837895827992\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.045919792389807604\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.0620869995793328\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.07800277448724961\n",
            "**************************************************\n",
            "epoch: 21, train_acc: 0.9300000071525574, avg_cost: 0.0955756154827153\n",
            "**************************************************\n",
            "test_acc: 0.9098000019788742\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 9.603814532359442e-05\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.013910306375473734\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02917553582539162\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.04521487400556606\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.0658093907063206\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.0843906043780346\n",
            "**************************************************\n",
            "epoch: 22, train_acc: 0.9599999785423279, avg_cost: 0.10288845889270307\n",
            "**************************************************\n",
            "test_acc: 0.9099000036716461\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00020962449411551158\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.013999230647459627\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.029001009136748798\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.04532007272820923\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.0630873474432156\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.08114871982019399\n",
            "**************************************************\n",
            "epoch: 23, train_acc: 0.9200000166893005, avg_cost: 0.09811712459195407\n",
            "**************************************************\n",
            "test_acc: 0.9111999976634979\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00013094693422317506\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.01652576091388861\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.03432406800178191\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.05057120882595583\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.06601713918149475\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.08494368523980184\n",
            "**************************************************\n",
            "epoch: 24, train_acc: 0.9599999785423279, avg_cost: 0.10535791548589873\n",
            "**************************************************\n",
            "test_acc: 0.9140000009536743\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 9.253023192286492e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.01490021909587085\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.03176643822031718\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.05019105567286416\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.06704248195824522\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.08335014111672835\n",
            "**************************************************\n",
            "epoch: 25, train_acc: 0.9300000071525574, avg_cost: 0.10087748286314303\n",
            "**************************************************\n",
            "test_acc: 0.9118999993801117\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00011814240366220474\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.01475087697617709\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.029572362455849828\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.04412300473389526\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.05965147550838686\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.07722507760860023\n",
            "**************************************************\n",
            "epoch: 26, train_acc: 0.9700000286102295, avg_cost: 0.09292250393889849\n",
            "**************************************************\n",
            "test_acc: 0.9099000030755997\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 9.823396801948547e-05\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.015420094244182113\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.02994596814736722\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.04363859663407004\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.05907646651069318\n",
            "batch: 500, train_acc: 0.9900000095367432, avg_cost: 0.07941141172156972\n",
            "**************************************************\n",
            "epoch: 27, train_acc: 0.9700000286102295, avg_cost: 0.09820710387546559\n",
            "**************************************************\n",
            "test_acc: 0.9124000024795532\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00013299499948819478\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.01591813409390548\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.030341696366667732\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.04631199731181063\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.06380413181924569\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.07961269447735195\n",
            "**************************************************\n",
            "epoch: 28, train_acc: 0.9700000286102295, avg_cost: 0.09654784136296553\n",
            "**************************************************\n",
            "test_acc: 0.9081000012159347\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.00019769766678412755\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.014802636271342637\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.031836922426397594\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.048045186006153644\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.06298022169309361\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.07674539007556931\n",
            "**************************************************\n",
            "epoch: 29, train_acc: 0.9800000190734863, avg_cost: 0.09326113940216597\n",
            "**************************************************\n",
            "test_acc: 0.9144000017642975\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00016127251088619233\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.01628031053269903\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.03174045279311638\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.04672646701025464\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.06038380181416869\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.07491759245128682\n",
            "**************************************************\n",
            "epoch: 30, train_acc: 0.9800000190734863, avg_cost: 0.08924741643325738\n",
            "**************************************************\n",
            "test_acc: 0.9151000046730041\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 8.356603483359019e-05\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.013306609966481727\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.026351047369341043\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.03952108688962958\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.053303740865861426\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.07182622122888767\n",
            "**************************************************\n",
            "epoch: 31, train_acc: 0.9599999785423279, avg_cost: 0.09596205713848277\n",
            "**************************************************\n",
            "test_acc: 0.9058999991416932\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.0003319681187470754\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.015764815645913286\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.030335320591305698\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.04751200573208434\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.062080523210267226\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.07533663609996441\n",
            "**************************************************\n",
            "epoch: 32, train_acc: 0.9200000166893005, avg_cost: 0.08914034272233647\n",
            "**************************************************\n",
            "test_acc: 0.9170000004768372\n",
            "batch: 0, train_acc: 0.9800000190734863, avg_cost: 2.445517728726069e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.011585565937372545\n",
            "batch: 200, train_acc: 0.9399999976158142, avg_cost: 0.023953771111555385\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.0365716730135803\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.051759936215045495\n",
            "batch: 500, train_acc: 0.9399999976158142, avg_cost: 0.06682239360020806\n",
            "**************************************************\n",
            "epoch: 33, train_acc: 0.9800000190734863, avg_cost: 0.08223490150800593\n",
            "**************************************************\n",
            "test_acc: 0.9116000056266784\n",
            "batch: 0, train_acc: 0.9900000095367432, avg_cost: 8.346710974971453e-05\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.014120699134655296\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.029231747455584517\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.045229251560134166\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.06119609259534625\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.07622797182605917\n",
            "**************************************************\n",
            "epoch: 34, train_acc: 0.949999988079071, avg_cost: 0.09301377564202995\n",
            "**************************************************\n",
            "test_acc: 0.9085999995470047\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.000214487686753273\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.01335704493181159\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02765705121525874\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.04408712428218374\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.0598672911695515\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.07670866032596677\n",
            "**************************************************\n",
            "epoch: 35, train_acc: 0.9200000166893005, avg_cost: 0.09056335082432879\n",
            "**************************************************\n",
            "test_acc: 0.9132000017166138\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00021989251176516214\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.011745962953815857\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.023942303378134976\n",
            "batch: 300, train_acc: 0.9800000190734863, avg_cost: 0.037454429967328895\n",
            "batch: 400, train_acc: 0.9100000262260437, avg_cost: 0.054778509705017006\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.07206884708565965\n",
            "**************************************************\n",
            "epoch: 36, train_acc: 0.9599999785423279, avg_cost: 0.08713205956233039\n",
            "**************************************************\n",
            "test_acc: 0.9191000026464462\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 9.323511272668839e-05\n",
            "batch: 100, train_acc: 0.8999999761581421, avg_cost: 0.014130344502627847\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.026637434498406953\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.039205497397730736\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.0534926681158443\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.06969043075262253\n",
            "**************************************************\n",
            "epoch: 37, train_acc: 0.9900000095367432, avg_cost: 0.08328565154069405\n",
            "**************************************************\n",
            "test_acc: 0.920400003194809\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.0001221587136387825\n",
            "batch: 100, train_acc: 0.949999988079071, avg_cost: 0.012468435058059791\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.02744399271905422\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.04071414668423432\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.05483737512181197\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.06703095225927726\n",
            "**************************************************\n",
            "epoch: 38, train_acc: 0.9700000286102295, avg_cost: 0.07956762081322569\n",
            "**************************************************\n",
            "test_acc: 0.9169999992847443\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.00016762124995390573\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.014874507592370113\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02784652411472053\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.04215176810355236\n",
            "batch: 400, train_acc: 0.949999988079071, avg_cost: 0.055246961320129524\n",
            "batch: 500, train_acc: 0.9800000190734863, avg_cost: 0.06858338813918335\n",
            "**************************************************\n",
            "epoch: 39, train_acc: 0.9800000190734863, avg_cost: 0.08268896223666779\n",
            "**************************************************\n",
            "test_acc: 0.9139000046253204\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 7.170787702004115e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.012579297002715364\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.027248455410299355\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.0411112319927391\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.05512006983791555\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.07065959016181295\n",
            "**************************************************\n",
            "epoch: 40, train_acc: 0.9900000095367432, avg_cost: 0.08353538184746864\n",
            "**************************************************\n",
            "test_acc: 0.9171000057458878\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 0.00016828672339518866\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.012626019818708298\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.026245099197452274\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.040724902106448996\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.055826358820001254\n",
            "batch: 500, train_acc: 0.9100000262260437, avg_cost: 0.07047166349366306\n",
            "**************************************************\n",
            "epoch: 41, train_acc: 0.9700000286102295, avg_cost: 0.08646426787910362\n",
            "**************************************************\n",
            "test_acc: 0.9169000035524368\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00015706685682137807\n",
            "batch: 100, train_acc: 0.9800000190734863, avg_cost: 0.013318718069543437\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.026031386451795684\n",
            "batch: 300, train_acc: 0.949999988079071, avg_cost: 0.04035798966574173\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.052761480480742966\n",
            "batch: 500, train_acc: 0.9599999785423279, avg_cost: 0.06496593013405802\n",
            "**************************************************\n",
            "epoch: 42, train_acc: 0.9700000286102295, avg_cost: 0.07865329140797259\n",
            "**************************************************\n",
            "test_acc: 0.9168000018596649\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 8.522943904002507e-05\n",
            "batch: 100, train_acc: 0.9700000286102295, avg_cost: 0.013564262160410486\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.02545529728677746\n",
            "batch: 300, train_acc: 0.9399999976158142, avg_cost: 0.038703322912721626\n",
            "batch: 400, train_acc: 0.9599999785423279, avg_cost: 0.05400312136470655\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.06933469483551268\n",
            "**************************************************\n",
            "epoch: 43, train_acc: 0.9300000071525574, avg_cost: 0.0835513179679402\n",
            "**************************************************\n",
            "test_acc: 0.916600005030632\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 7.935453827182452e-05\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.012206961546714105\n",
            "batch: 200, train_acc: 0.9800000190734863, avg_cost: 0.023626461809811487\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.038866765333029095\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.05132532163057473\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.06641181086190044\n",
            "**************************************************\n",
            "epoch: 44, train_acc: 0.9399999976158142, avg_cost: 0.08209754488120471\n",
            "**************************************************\n",
            "test_acc: 0.9161000037193299\n",
            "batch: 0, train_acc: 0.9700000286102295, avg_cost: 5.346146722634633e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.015998411209632944\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.03142108008110275\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.04568622517865152\n",
            "batch: 400, train_acc: 0.9300000071525574, avg_cost: 0.05884946417737717\n",
            "batch: 500, train_acc: 0.9700000286102295, avg_cost: 0.07351241179174396\n",
            "**************************************************\n",
            "epoch: 45, train_acc: 0.9599999785423279, avg_cost: 0.08794830196590442\n",
            "**************************************************\n",
            "test_acc: 0.9192000037431717\n",
            "batch: 0, train_acc: 0.9300000071525574, avg_cost: 0.00014559373259544371\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.011976094200896726\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02382198907434941\n",
            "batch: 300, train_acc: 0.9700000286102295, avg_cost: 0.03610836973879486\n",
            "batch: 400, train_acc: 0.9800000190734863, avg_cost: 0.04875323711584013\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.061093639836957085\n",
            "**************************************************\n",
            "epoch: 46, train_acc: 0.9700000286102295, avg_cost: 0.07401435345721737\n",
            "**************************************************\n",
            "test_acc: 0.9156000030040741\n",
            "batch: 0, train_acc: 0.949999988079071, avg_cost: 0.00016302240391572316\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.011220414444493752\n",
            "batch: 200, train_acc: 0.949999988079071, avg_cost: 0.023011758446276273\n",
            "batch: 300, train_acc: 0.9200000166893005, avg_cost: 0.03464477591299025\n",
            "batch: 400, train_acc: 0.9700000286102295, avg_cost: 0.04609722242462642\n",
            "batch: 500, train_acc: 0.949999988079071, avg_cost: 0.0604904464159335\n",
            "**************************************************\n",
            "epoch: 47, train_acc: 0.9800000190734863, avg_cost: 0.07627149252769107\n",
            "**************************************************\n",
            "test_acc: 0.9156000018119812\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 0.0001233972360690435\n",
            "batch: 100, train_acc: 0.9399999976158142, avg_cost: 0.012791075706481936\n",
            "batch: 200, train_acc: 0.9700000286102295, avg_cost: 0.027197823673486714\n",
            "batch: 300, train_acc: 0.9599999785423279, avg_cost: 0.04069391717823844\n",
            "batch: 400, train_acc: 0.9399999976158142, avg_cost: 0.054868212208772735\n",
            "batch: 500, train_acc: 0.9300000071525574, avg_cost: 0.06895826925911633\n",
            "**************************************************\n",
            "epoch: 48, train_acc: 0.9599999785423279, avg_cost: 0.08202489129345247\n",
            "**************************************************\n",
            "test_acc: 0.9144000041484833\n",
            "batch: 0, train_acc: 0.9399999976158142, avg_cost: 0.00032006680965423584\n",
            "batch: 100, train_acc: 0.9300000071525574, avg_cost: 0.012613410266737143\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.02526500830426813\n",
            "batch: 300, train_acc: 0.9300000071525574, avg_cost: 0.03956916639891767\n",
            "batch: 400, train_acc: 0.9200000166893005, avg_cost: 0.05093394112152359\n",
            "batch: 500, train_acc: 0.8700000047683716, avg_cost: 0.06346362532116474\n",
            "**************************************************\n",
            "epoch: 49, train_acc: 0.9300000071525574, avg_cost: 0.07669701000209901\n",
            "**************************************************\n",
            "test_acc: 0.9163000029325485\n",
            "[0, 0.8224999976158142, 0.8788999980688095, 0.8910000002384186, 0.8968000000715256, 0.8985000032186509, 0.9005000001192093, 0.9089000022411347, 0.9136000037193298, 0.9140000009536743, 0.9144000017642975, 0.9151000046730041, 0.9170000004768372, 0.9191000026464462, 0.920400003194809]\n",
            "[100, 0.804229662021002, 0.48614370430509274, 0.4147733908146623, 0.355751197511951, 0.33643747678647457, 0.30815361256400753, 0.274462412465364, 0.2650184204305215, 0.22922350328415667, 0.20349514762560522, 0.19848077864696598, 0.17320114228874425, 0.16654243931174287, 0.14720057868088285, 0.14536132596122728, 0.1268017622393866, 0.12557169475903135, 0.11387653847845894, 0.11138089534826576, 0.0955756154827153, 0.09292250393889849, 0.08924741643325738, 0.08914034272233647, 0.08223490150800593, 0.07956762081322569, 0.07865329140797259, 0.07401435345721737]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upMeXV6_8k8j",
        "colab_type": "text"
      },
      "source": [
        "# 152"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xxPXsaCDbIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 18*2 / 18*2 / 18*2 resnet layer\n",
        "1. 맨 처음 conv layer를 16 channel 단층으로 전환\n",
        "2. #filter를 16-32-64로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.00001\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_ResNet_0816_BN_152_bst_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_ResNet_0816_BN_152_bst_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_ResNet_0816_BN_152_bst_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 16, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [16,16,16]\n",
        "iList1_1 = [16,16,32]\n",
        "iList2 = [32,32,32]\n",
        "iList2_1 = [32,32,64]\n",
        "iList3 = [64,64,64]\n",
        "iList3_1 = [64,64,128]\n",
        "iList4 = [128,128,128]\n",
        "iList4_1 = [128,128,256]\n",
        "iList5 = [256,256,256]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,19):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h19 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 19)\n",
        "hidden_layer_list.append(h19)\n",
        "\n",
        "for i in range(20,37):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h37 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 37)\n",
        "h37 = tf.nn.max_pool(h37, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h37)\n",
        "\n",
        "for i in range(38,55):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h55 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3_1, 55)\n",
        "hidden_layer_list.append(h55)\n",
        "\n",
        "for i in range(56,73):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList4, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "  \n",
        "h73 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList4_1, 73)\n",
        "h73 = tf.nn.max_pool(h73, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h73)\n",
        "\n",
        "for i in range(74,91):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList5, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "  \n",
        "h91 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList4_1, 91)\n",
        "hidden_layer_list.append(h91)\n",
        "  \n",
        "h91 = tf.nn.avg_pool(h91, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h91_flat = tf.reshape(h91, [-1,256])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h91_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGIBzdP-8zw1",
        "colab_type": "code",
        "outputId": "c5fffdd4-36d3-4f91-e25a-bccefc472359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:1, keep_prob_flatten:1, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)  \n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(100):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 100\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 1.0, avg_cost: 1.012995218237241e-05\n",
            "batch: 100, train_acc: 0.9900000095367432, avg_cost: 0.0036641801839383944\n",
            "batch: 200, train_acc: 0.9900000095367432, avg_cost: 0.006155472416721749\n",
            "batch: 300, train_acc: 0.9900000095367432, avg_cost: 0.008049827112893884\n",
            "batch: 400, train_acc: 0.9900000095367432, avg_cost: 0.009994200265694713\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.011806211970251746\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 1.0, avg_cost: 0.013510935285400283\n",
            "**************************************************\n",
            "test_acc: 0.9114000028371811\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.074880223721266e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.000763924408723445\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.001536541542567041\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0022444882391088566\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.00279537334189323\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.0035273198499635294\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 1.0, avg_cost: 0.004097304790435975\n",
            "**************************************************\n",
            "test_acc: 0.9127000010013581\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.7297305253644782e-06\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.0002915978182378848\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0006579504289523664\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0009461743692008901\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0013759479918492921\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.0016972409647254016\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 1.0, avg_cost: 0.001956433087761981\n",
            "**************************************************\n",
            "test_acc: 0.9132000017166138\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.7209241428257275e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.00023240481621542125\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.000393481664086721\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0005731828105429785\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0007347569744100223\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.0009337043650581706\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 1.0, avg_cost: 0.0011430736501461065\n",
            "**************************************************\n",
            "test_acc: 0.9117000013589859\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.655312224912147e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.00012947648350746023\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0002781456828476317\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.00045118614806293996\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0005636303714103029\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.0006839618630813981\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 1.0, avg_cost: 0.0007988118691537244\n",
            "**************************************************\n",
            "test_acc: 0.9136000037193298\n",
            "batch: 0, train_acc: 1.0, avg_cost: 3.3502091052165875e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 0.00011731052812744262\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00020421217786861228\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.00030161076007667934\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.00037469997031318276\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.0004902497788043547\n",
            "**************************************************\n",
            "epoch: 5, train_acc: 1.0, avg_cost: 0.0006092646604671849\n",
            "**************************************************\n",
            "test_acc: 0.9134000009298324\n",
            "batch: 0, train_acc: 1.0, avg_cost: 2.167096681660041e-07\n",
            "batch: 100, train_acc: 1.0, avg_cost: 5.611027078884945e-05\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.00014529330675638142\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.00021156787673438273\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0002788745668264405\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.00034001616708943984\n",
            "**************************************************\n",
            "epoch: 6, train_acc: 1.0, avg_cost: 0.000385569929885605\n",
            "**************************************************\n",
            "test_acc: 0.9134000033140183\n",
            "batch: 0, train_acc: 1.0, avg_cost: 7.519653687874475e-08\n",
            "batch: 100, train_acc: 1.0, avg_cost: 3.1235455559605415e-05\n",
            "batch: 200, train_acc: 1.0, avg_cost: 0.0001349001872858935\n",
            "batch: 300, train_acc: 1.0, avg_cost: 0.0001842290345363533\n",
            "batch: 400, train_acc: 1.0, avg_cost: 0.0002364670132298367\n",
            "batch: 500, train_acc: 1.0, avg_cost: 0.00028977334461008765\n",
            "**************************************************\n",
            "epoch: 7, train_acc: 1.0, avg_cost: 0.00039682978419023127\n",
            "**************************************************\n",
            "test_acc: 0.9132000023126602\n",
            "batch: 0, train_acc: 1.0, avg_cost: 4.8393142909238426e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-11474f5a3554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UigEtQCuNof2",
        "colab_type": "text"
      },
      "source": [
        "# WRN : for 94% (too much big scale)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxU25nxS_rbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 28*2 / 28*2 / 28*2 wide resnet layer\n",
        "1. 맨 처음 conv layer를 160 channel 단층으로 전환\n",
        "2. #filter를 160-320-640로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "5. 1epoch당 60000/50 = 1200iteration\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 50\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_WRN_0816_BN_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_WRN_0816_BN_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_WRN_0816_BN_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 160, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [160,160,160]\n",
        "iList1_1 = [160,160,320]\n",
        "iList2 = [320,320,320]\n",
        "iList2_1 = [320,320,640]\n",
        "iList3 = [640,640,640]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,29):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h29 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 29)\n",
        "h29 = tf.nn.max_pool(h29, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h29)\n",
        "\n",
        "for i in range(30,57):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h57 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 57)\n",
        "h57 = tf.nn.max_pool(h57, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h57)\n",
        "\n",
        "for i in range(58,85):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h85 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 85)\n",
        "h85 = tf.nn.max_pool(h85, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h85)\n",
        "  \n",
        "h86 = tf.nn.avg_pool(h85, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h86_flat = tf.reshape(h86, [-1,640])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[640,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h86_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84_Xj9R-N_BM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd5da2b6-3fa8-46a3-a47c-8d4d42b065b0"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:1, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(200):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[50*num:50*(num+1),:,:],y:test_labels[50*num:50*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 200\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.7400000095367432, avg_cost: 0.0021068376302719118\n",
            "batch: 100, train_acc: 0.7400000095367432, avg_cost: 0.07177234232425686\n",
            "batch: 200, train_acc: 0.7799999713897705, avg_cost: 0.13477707090477156\n",
            "batch: 300, train_acc: 0.7799999713897705, avg_cost: 0.1968955680976312\n",
            "batch: 400, train_acc: 0.800000011920929, avg_cost: 0.2590624453375737\n",
            "batch: 500, train_acc: 0.800000011920929, avg_cost: 0.3190614515294632\n",
            "batch: 600, train_acc: 0.7400000095367432, avg_cost: 0.37459514717261005\n",
            "batch: 700, train_acc: 0.6800000071525574, avg_cost: 0.43008347928524004\n",
            "batch: 800, train_acc: 0.7599999904632568, avg_cost: 0.4855405419816572\n",
            "batch: 900, train_acc: 0.7200000286102295, avg_cost: 0.5427726601312554\n",
            "batch: 1000, train_acc: 0.7799999713897705, avg_cost: 0.5996599838634328\n",
            "batch: 1100, train_acc: 0.7599999904632568, avg_cost: 0.6555540790408846\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.7799999713897705, avg_cost: 0.7125723030169802\n",
            "**************************************************\n",
            "test_acc: 0.5656000021100044\n",
            "batch: 0, train_acc: 0.7599999904632568, avg_cost: 0.0004093986501296361\n",
            "batch: 100, train_acc: 0.800000011920929, avg_cost: 0.05409603406985599\n",
            "batch: 200, train_acc: 0.699999988079071, avg_cost: 0.10915131129324437\n",
            "batch: 300, train_acc: 0.7799999713897705, avg_cost: 0.16410365956525022\n",
            "batch: 400, train_acc: 0.8199999928474426, avg_cost: 0.21608330967525652\n",
            "batch: 500, train_acc: 0.7200000286102295, avg_cost: 0.2698367111633222\n",
            "batch: 600, train_acc: 0.7599999904632568, avg_cost: 0.3229343859106304\n",
            "batch: 700, train_acc: 0.8399999737739563, avg_cost: 0.37888753260175434\n",
            "batch: 800, train_acc: 0.6600000262260437, avg_cost: 0.43923295932511547\n",
            "batch: 900, train_acc: 0.6800000071525574, avg_cost: 0.4946442457536866\n",
            "batch: 1000, train_acc: 0.7599999904632568, avg_cost: 0.5495706814279169\n",
            "batch: 1100, train_acc: 0.8600000143051147, avg_cost: 0.6004086257268995\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.7599999904632568, avg_cost: 0.6528710013379664\n",
            "**************************************************\n",
            "test_acc: 0.5514000025391579\n",
            "batch: 0, train_acc: 0.8199999928474426, avg_cost: 0.0003849842647711436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-04872d680136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mmyfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pimnJKQ4VR69",
        "colab_type": "text"
      },
      "source": [
        "# Modified WRN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkAliSZGS5eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 28*2 / 28*2 / 28*2 wide resnet layer\n",
        "1. 맨 처음 conv layer를 160 channel 단층으로 전환\n",
        "2. #filter를 160-320-640로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "5. 1epoch당 60000/50 = 1200iteration\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 50\n",
        "batch_size = 50\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_WRN_revised_0816_BN_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_WRN_revised_0816_BN_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_WRN_revised_0816_BN_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 160, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [160,160,160]\n",
        "iList1_1 = [160,160,320]\n",
        "iList2 = [320,320,320]\n",
        "iList2_1 = [320,320,640]\n",
        "iList3 = [640,640,640]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,5):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h5 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h5)\n",
        "\n",
        "for i in range(6,9):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h9 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 9)\n",
        "h9 = tf.nn.max_pool(h9, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h9)\n",
        "\n",
        "for i in range(10,13):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h13 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 13)\n",
        "h13 = tf.nn.max_pool(h13, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h13)\n",
        "  \n",
        "h14 = tf.nn.avg_pool(h13, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h14_flat = tf.reshape(h14, [-1,640])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[640,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h14_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfG2MqRzVa6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "155a6196-3b6a-4377-cd73-f2a845f190a3"
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "#   saver.restore(sess, load_file)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:1, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(200):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[50*num:50*(num+1),:,:],y:test_labels[50*num:50*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 200\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start\n",
            "batch: 0, train_acc: 0.11999999731779099, avg_cost: 0.0032632982730865477\n",
            "batch: 100, train_acc: 0.7200000286102295, avg_cost: 1.1852482240895434\n",
            "batch: 200, train_acc: 0.7599999904632568, avg_cost: 1.2946564383804804\n",
            "batch: 300, train_acc: 0.6399999856948853, avg_cost: 1.495631261120241\n",
            "batch: 400, train_acc: 0.7799999713897705, avg_cost: 1.5919538464893908\n",
            "batch: 500, train_acc: 0.699999988079071, avg_cost: 1.6672483658045536\n",
            "batch: 600, train_acc: 0.7799999713897705, avg_cost: 1.737454542542498\n",
            "batch: 700, train_acc: 0.7400000095367432, avg_cost: 1.8170557390029227\n",
            "batch: 800, train_acc: 0.7799999713897705, avg_cost: 1.8789849527304368\n",
            "batch: 900, train_acc: 0.800000011920929, avg_cost: 1.9407696837559345\n",
            "batch: 1000, train_acc: 0.7400000095367432, avg_cost: 1.9960565011699996\n",
            "batch: 1100, train_acc: 0.8199999928474426, avg_cost: 2.0419485336293786\n",
            "**************************************************\n",
            "epoch: 0, train_acc: 0.8600000143051147, avg_cost: 2.089116011286775\n",
            "**************************************************\n",
            "test_acc: 0.8429999938607216\n",
            "batch: 0, train_acc: 0.8199999928474426, avg_cost: 0.0004028754432996114\n",
            "batch: 100, train_acc: 0.7200000286102295, avg_cost: 0.054179277407626326\n",
            "batch: 200, train_acc: 0.8999999761581421, avg_cost: 0.10803080644458539\n",
            "batch: 300, train_acc: 0.8600000143051147, avg_cost: 0.15303248606622216\n",
            "batch: 400, train_acc: 0.8600000143051147, avg_cost: 0.19222817808389672\n",
            "batch: 500, train_acc: 0.8799999952316284, avg_cost: 0.23285232887292906\n",
            "batch: 600, train_acc: 0.8999999761581421, avg_cost: 0.27678555459405\n",
            "batch: 700, train_acc: 0.8600000143051147, avg_cost: 0.3171909195246798\n",
            "batch: 800, train_acc: 0.8399999737739563, avg_cost: 0.35735857123509074\n",
            "batch: 900, train_acc: 0.8600000143051147, avg_cost: 0.39270498241608376\n",
            "batch: 1000, train_acc: 0.9399999976158142, avg_cost: 0.42315711420029417\n",
            "batch: 1100, train_acc: 0.8399999737739563, avg_cost: 0.45688875465964274\n",
            "**************************************************\n",
            "epoch: 1, train_acc: 0.8799999952316284, avg_cost: 0.4883580300336082\n",
            "**************************************************\n",
            "test_acc: 0.8768999972939491\n",
            "batch: 0, train_acc: 0.8399999737739563, avg_cost: 0.0003431189556916555\n",
            "batch: 100, train_acc: 0.8199999928474426, avg_cost: 0.031325578168034565\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.06468463463708761\n",
            "batch: 300, train_acc: 0.8799999952316284, avg_cost: 0.09628066445390386\n",
            "batch: 400, train_acc: 0.8799999952316284, avg_cost: 0.12988047463198515\n",
            "batch: 500, train_acc: 0.8399999737739563, avg_cost: 0.15907125595957058\n",
            "batch: 600, train_acc: 0.800000011920929, avg_cost: 0.19102394703775658\n",
            "batch: 700, train_acc: 0.8399999737739563, avg_cost: 0.22645730018615737\n",
            "batch: 800, train_acc: 0.8799999952316284, avg_cost: 0.2618941244110468\n",
            "batch: 900, train_acc: 0.8999999761581421, avg_cost: 0.29278321844836097\n",
            "batch: 1000, train_acc: 0.8600000143051147, avg_cost: 0.32186508232106786\n",
            "batch: 1100, train_acc: 0.8399999737739563, avg_cost: 0.35179589890564517\n",
            "**************************************************\n",
            "epoch: 2, train_acc: 0.9200000166893005, avg_cost: 0.37930023430536214\n",
            "**************************************************\n",
            "test_acc: 0.8613999953866005\n",
            "batch: 0, train_acc: 0.8600000143051147, avg_cost: 0.0003160362193981806\n",
            "batch: 100, train_acc: 0.8799999952316284, avg_cost: 0.027991671053071803\n",
            "batch: 200, train_acc: 0.9200000166893005, avg_cost: 0.05603676157693067\n",
            "batch: 300, train_acc: 0.8799999952316284, avg_cost: 0.08163414849589266\n",
            "batch: 400, train_acc: 0.8600000143051147, avg_cost: 0.11917070884257554\n",
            "batch: 500, train_acc: 0.8999999761581421, avg_cost: 0.1501826719070475\n",
            "batch: 600, train_acc: 0.8999999761581421, avg_cost: 0.17672413272783172\n",
            "batch: 700, train_acc: 0.9399999976158142, avg_cost: 0.20367068517953169\n",
            "batch: 800, train_acc: 0.8799999952316284, avg_cost: 0.22829852864767128\n",
            "batch: 900, train_acc: 0.8999999761581421, avg_cost: 0.25455528105919584\n",
            "batch: 1000, train_acc: 0.9599999785423279, avg_cost: 0.276966332619389\n",
            "batch: 1100, train_acc: 0.8799999952316284, avg_cost: 0.30020202824225045\n",
            "**************************************************\n",
            "epoch: 3, train_acc: 0.8799999952316284, avg_cost: 0.3223392209038139\n",
            "**************************************************\n",
            "test_acc: 0.8927999964356422\n",
            "batch: 0, train_acc: 0.9599999785423279, avg_cost: 9.756543363134067e-05\n",
            "batch: 100, train_acc: 0.9599999785423279, avg_cost: 0.024623567185675106\n",
            "batch: 200, train_acc: 0.8199999928474426, avg_cost: 0.0494512140223136\n",
            "batch: 300, train_acc: 0.8999999761581421, avg_cost: 0.07212218585424128\n",
            "batch: 400, train_acc: 0.8999999761581421, avg_cost: 0.09775691389106225\n",
            "batch: 500, train_acc: 0.9200000166893005, avg_cost: 0.12434764347660054\n",
            "batch: 600, train_acc: 0.9399999976158142, avg_cost: 0.15066563780419542\n",
            "batch: 700, train_acc: 0.8999999761581421, avg_cost: 0.17671841857023557\n",
            "batch: 800, train_acc: 0.9200000166893005, avg_cost: 0.20019265440913553\n",
            "batch: 900, train_acc: 0.8999999761581421, avg_cost: 0.2252262211435786\n",
            "batch: 1000, train_acc: 0.8799999952316284, avg_cost: 0.24774207192783573\n",
            "batch: 1100, train_acc: 0.8600000143051147, avg_cost: 0.2729373291693632\n",
            "**************************************************\n",
            "epoch: 4, train_acc: 0.9399999976158142, avg_cost: 0.29455751923223367\n",
            "**************************************************\n",
            "test_acc: 0.8839999961853028\n",
            "batch: 0, train_acc: 0.9200000166893005, avg_cost: 0.0002453622221946716\n",
            "batch: 100, train_acc: 0.9200000166893005, avg_cost: 0.022059472550948454\n",
            "batch: 200, train_acc: 0.9599999785423279, avg_cost: 0.0452092283219099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGhTEy1JksJv",
        "colab_type": "text"
      },
      "source": [
        "# Restore : rl = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-C7NqJVkzv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "수정사항 \n",
        "0. 28*2 / 28*2 / 28*2 wide resnet layer\n",
        "1. 맨 처음 conv layer를 160 channel 단층으로 전환\n",
        "2. #filter를 160-320-640로 전환\n",
        "3. 마지막 maxpool 후 바로 average pool 적용\n",
        "4. 마지막 fc에서 dropout 적용\n",
        "5. 1epoch당 60000/50 = 1200iteration\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 50\n",
        "batch_size = 50\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/MLpy'\n",
        "load_file = 'model_mnist_fashion_cnn_WRN_revised_0816_BN_cost.ckpt'\n",
        "load_file = root + load_file\n",
        "save_file = root + 'model_mnist_fashion_cnn_WRN_revised_0816_BN_acc.ckpt'\n",
        "save_file2 = root + 'model_mnist_fashion_cnn_WRN_revised_0816_BN_cost.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "batch_prob = tf.placeholder(tf.bool)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once_wo_pooling(x_img, 160, keep_prob, '1a', batch_prob)\n",
        "h1 = tf.layers.batch_normalization(h1, center=True, scale=True, training=batch_prob)\n",
        "\n",
        "iList1 = [160,160,160]\n",
        "iList1_1 = [160,160,320]\n",
        "iList2 = [320,320,320]\n",
        "iList2_1 = [320,320,640]\n",
        "iList3 = [640,640,640]\n",
        "hidden_layer_list = []\n",
        "h2 = my_resNet_model(h1, keep_prob, batch_prob, iList1, 2)\n",
        "hidden_layer_list.append(h2)\n",
        "for i in range(3,5):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h5 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList1_1, 5)\n",
        "h5 = tf.nn.max_pool(h5, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h5)\n",
        "\n",
        "for i in range(6,9):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h9 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList2_1, 9)\n",
        "h9 = tf.nn.max_pool(h9, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h9)\n",
        "\n",
        "for i in range(10,13):\n",
        "  hl = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, i)\n",
        "  hidden_layer_list.append(hl)\n",
        "\n",
        "h13 = my_resNet_model(hidden_layer_list[-1], keep_prob, batch_prob, iList3, 13)\n",
        "h13 = tf.nn.max_pool(h13, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "hidden_layer_list.append(h13)\n",
        "  \n",
        "h14 = tf.nn.avg_pool(h13, ksize=[1,7,7,1], strides=[1,7,7,1], padding=\"SAME\")\n",
        "h14_flat = tf.reshape(h14, [-1,640])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[640,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([10])\n",
        "\n",
        "logits = tf.matmul(h14_flat,fcw1) + fcb1\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6aMZGX1kz6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"training start\")\n",
        "\n",
        "accuracy_list = [0]\n",
        "loss_list = [100]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, load_file)\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    sf_images, sf_labels = shuffleData(train_images, train_labels)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = sf_images[batch_size*i:batch_size*(i+1),:,:], sf_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:1, batch_prob:True}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "      if i%100 == 0:\n",
        "        print(\"batch: {}, train_acc: {}, avg_cost: {}\".format(i,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "    print(\"*\"*50)\n",
        "    final_acu = 0\n",
        "    for num in range(200):\n",
        "      acu = sess.run(acc, feed_dict={x:test_images[50*num:50*(num+1),:,:],y:test_labels[50*num:50*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:True})\n",
        "      final_acu += acu\n",
        "    final_acu = final_acu / 200\n",
        "    print(\"test_acc: {}\".format(final_acu))\n",
        "    if final_acu > accuracy_list[-1]:\n",
        "      accuracy_list.append(final_acu)\n",
        "      saver.save(sess, save_file)\n",
        "    if avg_cost < loss_list[-1]:\n",
        "      loss_list.append(avg_cost)\n",
        "      saver.save(sess, save_file2)\n",
        "      \n",
        "print(accuracy_list)\n",
        "print(loss_list)\n",
        "  \n",
        "#   final_acu = 0\n",
        "#   for num in range(100):\n",
        "#     acu = sess.run(acc, feed_dict={x:test_images[100*num:100*(num+1),:,:],y:test_labels[100*num:100*(num+1),:],keep_prob:1,keep_prob_flatten:1,batch_prob:False})\n",
        "#     final_acu += acu\n",
        "#   final_acu = final_acu / 100\n",
        "#   print(\"final_acc: {}\".format(final_acu))\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}