# 3. Metrics Optimization & Advanced Feature Engineering

- evaluation metrics

  - 측정 단위!
  - 어떤 metric => 어떤 competition!
  - 회귀 / 분류
  - 기준선! : 어떤 기준(feature)이 가장 중요한지 설정하고 그것을 최적화한다!
  - 어떻게 metrics가 작동하고, 어떻게 최적화 할지 결정하라!
  - 최적의 hyperplane 찾기 어려운 경우도 많음
    - 다양한 추론 방법 사용해야함
    - train & test value : metric이 다르게 작용할 수도 있다
  - 시계열은 더 어려울 수도 있음
    - train data가 pattern을 다 포함하지 못할 경우
    - 특별한 metric을 사용하여 학습해야할 수 있음
    - metric 최적화 위해 좋은 방법 찾기 => 경쟁력!
      - 이 사람은 loss를 그냥 차이의 절댓값으로 했더니 잘 안됐다고 함
  - why so many metrics & metrics의 설정은 왜 중요한가?
    - 다양한 문제에 따라 다양한 metrics를 써야하니까!
    - 그걸로 rank!
- Metrics

  - 어떤 놈이 어떤 상황에 더 좋은지 아는 것은 중요!
- Regression
  
  - MSE : 일반적, 걍 기본적 사용 : 평균 제곱 거리
      - best constant : target average
      - 데이터 세트의 평균 오류를 가장 적게하는 metric
    - RMSE : root MSE : scaling : 선형적으로 분석 가능
      - MSE와 동일한 방식으로 작동 : 모델 점수 부여의 측면
      - gradient 모델은 조금 다름 : RMSE를 보고 바로 모델개선한다? lr고친다?
    - R-squared
      - 0이면 잘 못함 / 1이면 잘 함 <= MSE의 개략적인 평가 기준
    - MAE : Mean Absolute Error
      -  모델의 예측값과 실제값의 차이를 모두 더한다는 개념
      - best constant : target median
      - 강건한 모델 생성 가능 : robust => 이상치의 영향을 덜 받음
      - gradient  : 미분불가능하기 때문에 못 씀 / 얼마나 자주 정확히 예측했는지 빈도로 파악 가능
      - 2번 미분하면 싹다 0
    - MSE vs MAE
      - 아웃라이어 있니? => MAE : 아웃라이어라고 확신할 때...?
      - 물체가 rare하지만, care about 하여 사용할 만하다! => MSE
    - 위의 metrics는 표본의 크기에 따라 값이 좌우될 수 있다! => ratio 고려
    - sensitive to relative errors한 친구들
    - MSPE
      - MSE의 weighted version
      - best constant : weighted target mean
  
  - MAPE
      - MAE의 weighted version
      - best constant : weighted target median
    - RMSLE
      - log RMSE
      - 그래프의 비대칭성 : 
      - best constant in log space : mean target value 
      - 그다음 exp로 다시 원래 사이즈로 돌려줘야 함!
      - 작은 target들에게 덜 편향되어있지만, 상대적으로 오류가 있어도 작동
  - Classification
- 돌아온 Acc / Precision / Recall
  - TP / TN / FP / FN : T / F는 실제로 맞췄느냐, 못맞췄느냐 : P / N 은 예측값이 어떻게 나왔느냐
  - Accuracy : 그냥 정확도 : TP + TN / TP + TN + FP + FN
  - Precision : 모델이 True라고 분류한 것 중 실제 True의 비율 : TP / TP + FP
  - Recall : 실제 True 중 모델이 True라고 예측한 것의 비율 : TP / TP + FN
  - Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우에 관심이 있으나, 바라보고자 하는 관점만 다름 => Precision (모델 입장) / Recall (실제 정답(data)입장)
    - 확실한 날만 맞다고 하면 precision은 올라가겠지만 그게 의미가 있나?
      - ex: 20일 비옴, 그 중 이틀은 특정한 날 확신. 그 두날만 하면 100% => meaningless
    - 그래서 Recall도 같이 고려해야 함
    - Acc가 좋긴 하나, data의 domain이 불균형하면 성능지표로 부적합!
      - 비 오는거 예측하는데 그 지역이 비가 엄청 안오는 곳이다. 의미가 있는가? => 노의미
  - **그래서 Precision과 Recall 조화평균 => F1 score**
- ROC curve
  - TPR : True Positive Rate : 민감도 : 1인 케이스에 대해 1로 잘 예측한 반응
  - FPR : False Positivie Rate : 1-특이도 : 0인 케이스에 대해 1로 잘못 예측한 반응
  - ROC curve가 위로 볼록할수록 잘못 예측하는 확률이 낮으면서 잘 예측하는 확률은 높은 것이기 때문에 좋은 모델! => 그래서 AUC 구해서 모델의 Metric으로 사용 가능

- mean encodings
  - 말 그대로, 각 feature에 따라서 target이 얼마만큼 True인지 그걸 mean으로 encoding!
  - 라벨인코딩은 Random 순서여서 target과 상관관계 없음
  - mean 인코딩은 1에게서 0을 분리하는데 도움! (어쨌든 분리가 되더라)
  - 더 복잡하고 feature-target간에 비선형일 때 mean 인코딩이 더 효과적
  - ![image-20191029233600094](.\feature engineering using target.jpg)
  - 이런식으로 만든다
  - 근데 그냥 닥치고 mean으로 때리면 잘 안되더라
    - K-fold CV로 만들면 더 잘된다? <= Regularization?