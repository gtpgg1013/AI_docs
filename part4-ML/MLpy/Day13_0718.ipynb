{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085197/websockets-trying-to-convert-phython-code-to-node\n",
      "Websockets - trying to convert Phython Code to Node\n",
      "\n",
      "\n",
      "I'm trying to copy the features of this library https://github.com/rnbguy/pysphere/blob/master/misphere.py which connects via websockets to the Mi Sphere 360 camera. \n",
      "The important code is here\n",
      "ms_ip = '192.168.42.1'\n",
      "ms_tcp_port = 7878\n",
      "ms_uk_port = 8787\n",
      "ms_fs_port = 50422\n",
      "\n",
      "def init(self):\n",
      "    self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "    self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
      "    self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
      "    self.socket.connect((self.ms_ip, self.ms_tcp_port))\n",
      "    self.socket_1 = socket.socket()\n",
      "    self.recv_handle_live = True\n",
      "    self.recv_thread = threading.Thread(target=self.recv_handler)\n",
      "    self.recv_thread.daemon = True\n",
      "    self.session = Session()\n",
      "    self.session.conf = {}\n",
      "    self.session.locks = {}\n",
      "    self.last_send = 0\n",
      "\n",
      "I'm trying to do this with the ws library in Node\n",
      "const ip = '192.168.42.1'\n",
      "const port = '7878'\n",
      "const url = `ws://${ip}:${port}`\n",
      "const websocket = new WebSocket(url)\n",
      "\n",
      "However I'm not getting a connection to the camera.  (It's connected via Wifi, but the websocket never sends a connection message)\n",
      "I'm wondering if it's to do with the protocols, which are listed in the Phython code. \n",
      "I see in the websocket documentation you can define protocols in the connection, but I can't find any documentation about what those protocols are and how you add them. \n",
      "i.e. \n",
      "self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
      "self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
      "\n",
      "Anyone know how I add something like this in the Node websocket connection?\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085178/how-to-assign-value-to-a-pandas-dataframe-when-subset-by-complex-index-and-bool\n",
      "How to assign value to a pandas dataframe, when subset by complex index and boolean based conditions?\n",
      "\n",
      "\n",
      "I would like to replace values in a pandas dataframe, with a complex subsetting pattern. \n",
      "With the loc. accessor, I was only able to subset by chaining multiple conditions, because some of the conditions are index based. But it seems I can not assign values after such a chain of subsetting.\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame({'a': range(10), 'b': range(10)}, index=pd.date_range('2019-01-01','2019-01-10'))\n",
      "\n",
      "df.loc[df['a'] % 2 == 0, 'b'].loc[pd.to_datetime(['2019-01-05','2019-01-09'])] = np.nan\n",
      "\n",
      "df\n",
      "\n",
      "Result:\n",
      "            a   b\n",
      "2019-01-01  0   0\n",
      "2019-01-02  1   1\n",
      "2019-01-03  2   2\n",
      "2019-01-04  3   3\n",
      "2019-01-05  4   4\n",
      "2019-01-06  5   5\n",
      "2019-01-07  6   6\n",
      "2019-01-08  7   7\n",
      "2019-01-09  8   8\n",
      "2019-01-10  9   9\n",
      "\n",
      "Expected:\n",
      "            a   b\n",
      "2019-01-01  0   0\n",
      "2019-01-02  1   1\n",
      "2019-01-03  2   2\n",
      "2019-01-04  3   3\n",
      "2019-01-05  4   NaN\n",
      "2019-01-06  5   5\n",
      "2019-01-07  6   6\n",
      "2019-01-08  7   7\n",
      "2019-01-09  8   NaN\n",
      "2019-01-10  9   9\n",
      "\n",
      "I have tried alternative approaches like:\n",
      "df.loc[df['a'] % 2 == 0 and df.index.isin(['2019-01-05','2019-01-09']), 'b']\n",
      "\n",
      "which drops:\n",
      "ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "\n",
      "Not even this works, as the isin returns an array without the date based indexing:\n",
      "df['a'] % 2 == 0 and pd.Series(df.index.isin(['2019-01-05','2019-01-09']))\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085169/dwa-dynamic-windows-approach-research-paper-how-to-implement-the-algorithm-u\n",
      "DWA (Dynamic Windows Approach ) research paper, how to implement the algorithm using python\n",
      "\n",
      "\n",
      "I am new in the robotics field and been requested to do a project for a differential drive robot.in the project, I need to implement obstacle avoidance algorithm with a stereo camera.\n",
      "one of the suggested algorithms is DWA which need to be implemented using Python.\n",
      "I have read the published research paper about DWA(1997). I think before starting with the paper I need to build a kinematic model for the differential drive robot is this correct if it's correct how to do it .differnetial drive robot will have angular velocity, linear velocity and heading angle (Theta).\n",
      "Please, I will appreciate any help or suggestions on how to start implementing a scientific paper related to a robot with python.\n",
      "Please let me know if any extra information is needed.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085147/error-bad-transparency-mask-when-converting-pngs-with-alpha-transparency-to\n",
      "“Error: bad transparency mask” when converting PNG's with alpha transparency to JPEG's\n",
      "\n",
      "\n",
      "I'm writing a short function that converts PNG's with alpha transparent background to JPEG's with white backgrounds. Despite the error, the code works by successfully finding all PNG's within a directory and saving JPEG's. Why am I getting this error?\n",
      "The commented-out line of code shows a previous method I tried, which without running error converted PNG's to JPEG's using PIL's \"convert\" function. However it also converted alpha values to black, giving the JPEG's black backgrounds instead of white ones as desired.\n",
      "import os, sys, fnmatch\n",
      "\n",
      "path_in = 'path_in/'\n",
      "dir = os.listdir(path_in)\n",
      "\n",
      "for image in dir:\n",
      "    #checks if image is png\n",
      "    if fnmatch.fnmatch(image, '*.png'):\n",
      "        png_image = Image.open(path_in + image)\n",
      "        f, e = os.path.splitext(path_out + image)\n",
      "\n",
      "        #creates new all-white jpg based on size of jpg\n",
      "        jpg_image = Image.new('RGB', png_image.size, (255,255,255))\n",
      "        #pastes png over jpg\n",
      "        jpg_image.paste(png_image, (0,0), png_image)\n",
      "\n",
      "        #code that made transparent backgrounds black, so not in use\n",
      "        #jpg_image = jpg_image.convert('RGB')\n",
      "\n",
      "        #saves as jpg to same path\n",
      "        jpg_image.save(f + '.jpg', 'JPEG')\n",
      "\n",
      "I want to know why this code is producing an error message despite functioning properly. Otherwise, I would like to use the simpler method of \"convert\" built into PIL, but converting alpha values of PNG's to white instead of black.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085127/problem-to-convert-object-to-str-in-phyton\n",
      "Problem to convert object to str in phyton\n",
      "\n",
      "\n",
      "I need convert object to string \n",
      "import pandas as pd\n",
      "import locale\n",
      "import numpy as np\n",
      "\n",
      "df_csv = pd.read_csv('df_empenho.csv', sep=';', encoding='utf-8')\n",
      "\n",
      "df_csv['valor2'] = df_csv['Empenhado'].astype(str)\n",
      "\n",
      "df_csv['valor2'].astype(str).apply(lambda x : x.replace('.',''))\n",
      "\n",
      "df_csv['valor2'].head()\n",
      "\n",
      "Output\n",
      "0    13.188,15\n",
      "1     8.492,40\n",
      "2     3.570,00\n",
      "3     1.486,20\n",
      "4     2.660,67\n",
      "Name: valor2, dtype: object\n",
      "I expect dtype = string\n",
      "Thanks to help\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085125/importing-a-text-file-with-multiple-keys\n",
      "Importing a text file with multiple keys\n",
      "\n",
      "\n",
      "I am trying to import a text file with multiple 'sets' of values, some lists, some integers. \n",
      "I am struggling to figure out what the best course of action to do this would be. \n",
      "I have provided an example of the text file I am attempting to import and manipulate below. Basically, each line of the text file holds 6 values, and all these values relate to one another. My end goal is to be able to have almost a \"list of list\" set up so that I can define each set of six values almost as one.\n",
      "Does anyone have any thoughts on this? \n",
      "house, brick, 876, no, yes, 3\n",
      "apartment, wood, 345, yes, yes, 1\n",
      "condominium, brick, 453, no, yes, 8\n",
      "etc.. \n",
      "my intended end point is to be able to categorize each variable (for example, building type = house, material = brick, etc. and be able to search through these\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085118/equivalent-matlab-function-mod-in-numpy-or-python\n",
      "Equivalent matlab function mod in numpy or python\n",
      "\n",
      "\n",
      "I was converting some codes from Matlab to Python.\n",
      "In matlab there is the function mod which gives the modulo operation.\n",
      "For example the following example shows different results between the matlab mod and the equivalent numpy remainder operation:\n",
      "Matlab:\n",
      ">> mod(6, 0.05)\n",
      "\n",
      "ans =\n",
      "\n",
      "     0\n",
      "\n",
      "Numpy:\n",
      "np.remainder(6, 0.05) \n",
      "0.04999999999999967\n",
      "\n",
      "np.mod(6, 0.05)\n",
      "0.04999999999999967\n",
      "\n",
      "Python modulus operator gives the same results as numpy.\n",
      "6%0.05\n",
      "0.04999999999999967\n",
      "\n",
      "Is there anything in python which gives the same mod operation as the one in Matlab? preferably can be operated on numpy 2d/3d arrays.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085074/please-how-do-i-sum-up-all-the-integer-elements-from-a-sqlite3-table-row\n",
      "Please how do i sum up all the integer elements from a sqlite3 table row\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to calculate the sum total of the elements of a row using user data from a sqlite3 database table\n",
      "i used cursor.fetchall() to get the amount row in my database table and tried using the for loop to sum the elements of the table but i am getting an error\n",
      "db = sqlite3.connect('my.db')\n",
      "cursor = db.cursor()\n",
      "cursor.execute('SELECT amount FROM expenses')\n",
      "result = cursor.fetchall()\n",
      "c = 0\n",
      "for x in result:\n",
      "    c += x\n",
      "print(c)\n",
      "\n",
      "i got\n",
      "TypeError: unsupported operand type(s) for +=: 'int' and 'tuple'\n",
      "i expected to get the sum total of the amount\n",
      "I am aware of the sqlite3 SUM command. But I would like to learn how to solve this problem using Tkinter\n",
      "\n",
      "Answer\n",
      "\n",
      "Use \n",
      "c = 0\n",
      "for x in result:\n",
      "    c += x[0]\n",
      "\n",
      "cursor.fetchall() returns a list of tuples, with the tuple containing the column values per row. In your case, there happens to be only one column, but it's still a (one element) tuple.\n",
      "As far as i know cursor.fetchall() returns a tuple (like your error tells you) that means all your selections of the sql query will be pushed into that tuple. \n",
      "For example if you want to select multiple columns:\n",
      "'SELECT amount, tax, date FROM expenses' \n",
      "\n",
      "you will retrieve a tuple like this:\n",
      "(amount, tax, date)\n",
      "\n",
      "This tuple is given for every row. In your case result is an array containing a all rows, where each row itself is a tuple with just the amount in it.\n",
      "To keep a long story short you either need to rebuild the result as a list with list comprehension for example:\n",
      "db = sqlite3.connect('test.db')\n",
      "cursor = db.cursor()\n",
      "cursor.execute('SELECT amount FROM expenses')\n",
      "result = cursor.fetchall()\n",
      "\n",
      "result = [amount[0] for amount in result] # Rebuild your result array to contain \n",
      "                                          # single values instead of tuples\n",
      "c = 0\n",
      "for x in result:\n",
      "    c += x\n",
      "print(c)\n",
      "\n",
      "Another solution would be:\n",
      "db = sqlite3.connect('test.db')\n",
      "cursor = db.cursor()\n",
      "cursor.execute('SELECT amount FROM expenses')\n",
      "result = cursor.fetchall()\n",
      "c = 0\n",
      "for x in result:\n",
      "    c += x[0] # This will retrieve the 0th element from the tuple, which is the amount\n",
      "print(c)\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085064/struggling-to-see-why-reverse-sort-doesnt-take-into-account-100\n",
      "Struggling to see why reverse sort doesnt take into account 100\n",
      "\n",
      "\n",
      "I am importing a file into a multidimensional array. I am reverse sorting based on highest score - however when I run the code it currently ignores scores over 100 - can anyone help?\n",
      "scores = []\n",
      "file = open(\"hScores.txt\",\"r\")\n",
      "for line in file:\n",
      "    scores.append(line.strip(\"\\n\").split(\",\"))\n",
      "scores.sort(key=lambda x: x[1])\n",
      "scores.reverse()\n",
      "print(scores)\n",
      "\n",
      "Answer\n",
      "\n",
      "Strings sort the way words sort: \"taco\" comes before \"tesseract\" even though \"tesseract\" is \"bigger\". You are sorting numbers as if they were strings, so \"100\" comes before \"2\" because \"1\" comes before \"2\":\n",
      "s = \"1,2,3,110,89,108,160,36,19\"\n",
      "sorted(s.split(','))\n",
      "# ['1', '108', '110', '160', '19', '2', '3', '36', '89']\n",
      "\n",
      "Turn them into numbers and your sort will look more like you expect:\n",
      "s = \"1,2,3,110,89,108,160,36,19\"\n",
      "n = map(int, s.split(','))    # map to ints\n",
      "# now n is an iterator of numbers not strings\n",
      "sorted(n)\n",
      "# [1, 2, 3, 19, 36, 89, 108, 110, 160]\n",
      "# or\n",
      "sorted(n, reverse=True)\n",
      "# [160, 110, 108, 89, 36, 19, 3, 2, 1]\n",
      "Everything you read from a file is read as a string. You need to convert the numbers to ints before using them to compare as you expect. You can accomplish this by just changing the following line:\n",
      "scores.sort(key=lambda x: int(x[1]))\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085063/python-pandas-dropping-grouped-rows-based-on-missing-territory-code\n",
      "Python pandas - Dropping grouped rows based on missing territory code\n",
      "\n",
      "\n",
      "In the df below, we have two \"Mixes\" as indicated by the Mix_Name and Mix_ID columns. And within each of these mixes are multiple tracks with unique Track_ID's that contain different territories (see Territories column).\n",
      "\n",
      "What I'm hoping to do here find out which tracks do not have the US territory, and if any of the tracks do not have the US territory, I will want to drop the entire mix from my dataframe. With the resulting dataframe looking like this because \"Coachella Mix Vol 2\" is missing the US territory in one of its tracks:\n",
      "\n",
      "I know that I need to Groupby: 'Mix_ID', 'Track', and 'Artist' but I'm unsure of how to search the territories column to see if it contains the \"US\" territory. Any help would be much appreciated!\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085050/selenium-python-iterate-through-web-table-columns-stop-after-condition-is-true\n",
      "Selenium Python Iterate Through Web Table Columns, Stop after condition is True, Throw Error if Condition is False\n",
      "\n",
      "\n",
      "I am trying to validate whether or not data values (which include dollars and cents amounts) are being populated in certain columns of an HTML table. I have written a Selenium Python Script that uses a For Loop to iterate through the HTML table. I have added an IF / ELSE in the For Loop to check if a decimal point is found in any of the text in the columns on the table. If a values with a decimal point is found, then variable \"values_filled\" is set to True (if not, then \"values_filled\" is set to False). My thinking is that if a value with a decimal point is found, then the column is being populated successfully with dollars and cents values. If no decimal point is found, then the column is NOT being populated and this should trigger a \"FAIL\".\n",
      "The code that I have written successfully iterates through the rows and columns of the HTML table. Further, the IF / ELSE section is correctly flagging \"values_filled\" = True if there is a value with a decimal point and values_filled = False if no values found. Last, I have a TRY / EXCEPT section where an exception is thrown if the values_filled is \"False\".\n",
      "What I would like to do next is when the first instance of the \"values_filled\" = True is found, I want to end the loop. However, the loop continues even when the values_filled value is true.\n",
      "account_balances_table = driver.find_element(By.XPATH, \"/html/body/div[1]/div[3]/div/div/ui-view/div/div/div/div[4]/div[1]/table\")\n",
      "rows = account_balances_table.find_elements(By.TAG_NAME, \"tr\")\n",
      "for row in rows:\n",
      "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
      "    for col in cols:\n",
      "        text_found = cols[1].text\n",
      "        if (\".\" in text_found):\n",
      "            values_filled = True\n",
      "            if values_filled == True:\n",
      "                break\n",
      "            else:\n",
      "                continue\n",
      "        else:\n",
      "            values_filled = False\n",
      "\n",
      "        try:\n",
      "            assert values_filled is True\n",
      "        except AssertionError:\n",
      "            screenshot_name = \"FAIL\" + \"_\" + test_case_ID + \"_\" + browser + \"_\" + env + \"_\" + time_stamp + \".png\"\n",
      "            saved_screenshot_location = str(screenshot_directory / screenshot_name)\n",
      "            driver.get_screenshot_as_file(saved_screenshot_location)\n",
      "            raise\n",
      "\n",
      "The problem I am encountering is that the For Loop continues even when values_filled is \"true\". I inserted a break but the loop does not stop. This is a problem as the script continues and falsely flags an exception for blank values found in other cells. I would like the loop to end after finding the first instance of values_filled = true.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085013/linear-regression-unexpected-results-python\n",
      "Linear Regression - Unexpected Results (Python)\n",
      "\n",
      "\n",
      "I am getting unexpected results for the implementation of linear regression I coded.\n",
      "Sometimes I get out of memory error, squaring errors, multiplication errors, basically that I've run out of size.\n",
      "The code seems pretty okay to me, and I'm unable to identify why it fails to work.\n",
      "X = np.array([ 6.1101,  5.5277,  8.5186,  7.0032,  5.8598,  8.3829, 7.4764,\n",
      "        8.5781,  6.4862,  5.0546,  5.7107, 14.164 ,  5.734 ,  8.4084,\n",
      "        5.6407,  5.3794,  6.3654,  5.1301,  6.4296,  7.0708,  6.1891,\n",
      "       20.27  ,  5.4901,  6.3261,  5.5649, 18.945 , 12.828 , 10.957 ,\n",
      "       13.176 , 22.203 ,  5.2524,  6.5894,  9.2482,  5.8918,  8.2111,\n",
      "        7.9334,  8.0959,  5.6063, 12.836 ,  6.3534,  5.4069,  6.8825,\n",
      "       11.708 ,  5.7737,  7.8247,  7.0931,  5.0702,  5.8014, 11.7   ,\n",
      "        5.5416,  7.5402,  5.3077,  7.4239,  7.6031,  6.3328,  6.3589,\n",
      "        6.2742,  5.6397,  9.3102,  9.4536,  8.8254,  5.1793, 21.279 ,\n",
      "       14.908 , 18.959 ,  7.2182,  8.2951, 10.236 ,  5.4994, 20.341 ,\n",
      "       10.136 ,  7.3345,  6.0062,  7.2259,  5.0269,  6.5479,  7.5386,\n",
      "        5.0365, 10.274 ,  5.1077,  5.7292,  5.1884,  6.3557,  9.7687,\n",
      "        6.5159,  8.5172,  9.1802,  6.002 ,  5.5204,  5.0594,  5.7077,\n",
      "        7.6366,  5.8707,  5.3054,  8.2934, 13.394 ,  5.4369])\n",
      "y = np.array([17.592  ,  9.1302 , 13.662  , 11.854  ,  6.8233 , 11.886  ,\n",
      "        4.3483 , 12.     ,  6.5987 ,  3.8166 ,  3.2522 , 15.505  ,\n",
      "        3.1551 ,  7.2258 ,  0.71618,  3.5129 ,  5.3048 ,  0.56077,\n",
      "        3.6518 ,  5.3893 ,  3.1386 , 21.767  ,  4.263  ,  5.1875 ,\n",
      "        3.0825 , 22.638  , 13.501  ,  7.0467 , 14.692  , 24.147  ,\n",
      "       -1.22   ,  5.9966 , 12.134  ,  1.8495 ,  6.5426 ,  4.5623 ,\n",
      "        4.1164 ,  3.3928 , 10.117  ,  5.4974 ,  0.55657,  3.9115 ,\n",
      "        5.3854 ,  2.4406 ,  6.7318 ,  1.0463 ,  5.1337 ,  1.844  ,\n",
      "        8.0043 ,  1.0179 ,  6.7504 ,  1.8396 ,  4.2885 ,  4.9981 ,\n",
      "        1.4233 , -1.4211 ,  2.4756 ,  4.6042 ,  3.9624 ,  5.4141 ,\n",
      "        5.1694 , -0.74279, 17.929  , 12.054  , 17.054  ,  4.8852 ,\n",
      "        5.7442 ,  7.7754 ,  1.0173 , 20.992  ,  6.6799 ,  4.0259 ,\n",
      "        1.2784 ,  3.3411 , -2.6807 ,  0.29678,  3.8845 ,  5.7014 ,\n",
      "        6.7526 ,  2.0576 ,  0.47953,  0.20421,  0.67861,  7.5435 ,\n",
      "        5.3436 ,  4.2415 ,  6.7981 ,  0.92695,  0.152  ,  2.8214 ,\n",
      "        1.8451 ,  4.2959 ,  7.2029 ,  1.9869 ,  0.14454,  9.0551 ,\n",
      "        0.61705])\n",
      "theta = np.array([0,0]) #Initial values of theta_0 and theta_1\n",
      "\n",
      "#Calculates cost function for given theta\n",
      "def costFunction(X,y,theta):\n",
      "    m = y.size\n",
      "    hypothesis = (X * theta[1]) + theta[0]\n",
      "    return (1/m) * sum((hypothesis - y) ** 2)\n",
      "\n",
      "#Calculates the partial derivatives of theta_0 and theta_1\n",
      "def slope(X,y,theta):\n",
      "    m=y.size\n",
      "    hypothesis = (X * theta[1]) + theta[0]\n",
      "    theta_0 = 2/(m) * sum(hypothesis - y) \n",
      "    theta_1 = 2/(m) * sum((hypothesis - y) * X)\n",
      "    return np.array([theta_0,theta_1])\n",
      "\n",
      "#running the gradient descent with 200 iters with learning rate 0.1\n",
      "for i in range(200):\n",
      "    theta = theta - 0.1*slope(X,y,theta)\n",
      "\n",
      "costFunction(X,y,theta) # Prints inf\n",
      "\n",
      "Answer\n",
      "\n",
      "Your learning rate is too large and GD does not converge. Try changing it to 0.01 and run it for more epochs, it worked for me.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57085000/remove-list-type-in-columns-while-preserving-list-structure\n",
      "Remove list type in columns while preserving list structure\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have two columns that from the way my data was pulled are in lists. This may be a really easy question, I just haven't found the exactly correct way to create the result I'm looking for.\n",
      "I need the \"c_u\" column to be a string without the [] and the \"tawgs.db_id\" column to be integers separated by a column if that's possible. \n",
      "I've tried this code:\n",
      "df['c_u'] = df['c_u'].astype(str)\n",
      "\n",
      "to convert c_u to a string: but it failed and outputs:\n",
      "\n",
      "What I need the output to look like is:\n",
      "c_u                              tawgs.db_id\n",
      "hbhprecision.com         10813,449,6426,6427\n",
      "thomsonreuters.com            12519,510,6426\n",
      "\n",
      "etc.\n",
      "Please help and thank you very much in advance!\n",
      "\n",
      "Answer\n",
      "\n",
      "for the first part, removing the brackets [ ] \n",
      "df['c_u'].apply(lambda x : x.strip(\"['\").strip(\"']\"))\n",
      "\n",
      "for the second part (assuming you removed your brackets as well), splitting the values across columns:\n",
      "df['tawgs.db_id'].str.split(',',  expand=True)\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084986/how-can-i-say-if-error-occurs-do-not-implement-this-code\n",
      "How can i say “if error occurs do not implement this code”\n",
      "\n",
      "\n",
      "I want to write a code in python that basically says, \" if error occurs while implementing int(a) print('valid character')      elif no error occurs while implementing int(a)  print('invalid character').\" a is an input\n",
      "I wanna make a simple hangman game and if the input is not a letter i want a certain message to be displayed. I tried using \"if a==int()\" but inputs are always a string\n",
      "\n",
      "Answer\n",
      "\n",
      "Normally you would use a Try, Except clause to handle errors, but because you're not actually getting an error -- you just want to know if the input is an alphabetical character or not, you would use the string.isalpha function.\n",
      "guess = input()\n",
      "if not guess.isalpha():\n",
      "    print('You must supply an alphabetical character')\n",
      "else:\n",
      "    #the rest of your code would go here\n",
      "explore try-except designs\n",
      "try:\n",
      "   # do something here\n",
      "except (ErrorName):\n",
      "   # error catching\n",
      "   print(\"invalid character\")\n",
      "\n",
      "Please try and include reproducible code, or a code block of pseudo-code so others can follow along easier!\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084983/what-is-the-proper-way-to-code-the-main-py-in-python\n",
      "What is the proper way to code the main.py in python?\n",
      "\n",
      "\n",
      "Using PyQt designer, what is the proper way to lay out your main.py? I have multiple windows I've created using QtDesigner. I've already coded all the window funtions, as in buttons and menu bar funtions. But I've coded all the funtions in the main.py. Should all the window funtions be coded in their own .py file? Or is it ok with coding them all in the main? It doesn't look good to me, but everything is working properly.\n",
      "\n",
      "Answer\n",
      "\n",
      "In general, you should try to keep your main.py as empty as possible:\n",
      "import stuff\n",
      "\n",
      "def main():\n",
      "    class_object = Class_object()\n",
      "    class_object.run()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "Something like this is clear. Obviously you can initiate logging, import configs and such, but only do the most general stuff in the main.py .\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084929/i-try-to-update-odoo-code-but-i-get-strptime-argument-1-must-be-str-not-bool\n",
      "i try to update odoo code but i get strptime() argument 1 must be str, not bool\n",
      "\n",
      "\n",
      "The objective is to take the sale date and create a plan for automatic visits, 2 visits per year for three years, and i can do it in older version of odoo but now i get this error.\n",
      "that worked in openerp 7, but now i want to do it in Odoo 11.0 Python 3, really i don't get it what i missed\n",
      "\n",
      "    class garantias(models.Model):\n",
      "        _name = 'itriplee.garantias'\n",
      "\n",
      "        equipo = fields.Many2one('itriplee.equipos', 'Equipo')\n",
      "        fecha_de_venta = fields.Date('Fecha de Venta', related='equipo.venta', readonly=True)\n",
      "\n",
      "     @api.model\n",
      "        def create(self, vals):\n",
      "            obj_visita = self.pool.get('itriplee.servicio')\n",
      "            obj = self.env['itriplee.garantias']\n",
      "            cliente = obj.cliente.id\n",
      "            fecha_compra = obj.fecha_de_venta\n",
      "            fm = ('%Y-%m-%d')\n",
      "            cantidad_meses = 6\n",
      "            ind = 0\n",
      "            now = datetime.now()\n",
      "            now_str = now.strftime(fm)\n",
      "            now_int = datetime.strptime(now_str, fm)\n",
      "            # fecha_compra_original = datetime.strptime(fecha_compra, fm)\n",
      "            fecha_compra_inicial = datetime.strptime(fecha_compra, fm)\n",
      "            while ind < cantidad_meses:\n",
      "                fecha_6_meses = fecha_compra_inicial + relativedelta(months=6)\n",
      "                if fecha_6_meses >= now_int:\n",
      "                    obj_visita.create({'cliente':cliente,'visita':fecha_6_meses,'estado':'confirmar','visitas':obj.id},context=None)\n",
      "                ind = ind + 1\n",
      "                fecha_compra_inicial = fecha_6_meses\n",
      "            return True\n",
      "\n",
      "\n",
      "and get this error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 651, in _handle_exception\n",
      "    return super(JsonRequest, self)._handle_exception(exception)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 310, in _handle_exception\n",
      "    raise pycompat.reraise(type(exception), exception, sys.exc_info()[2])\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/tools/pycompat.py\", line 87, in reraise\n",
      "    raise value\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 693, in dispatch\n",
      "    result = self._call_function(**self.params)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 342, in _call_function\n",
      "    return checked_call(self.db, *args, **kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/service/model.py\", line 97, in wrapper\n",
      "    return f(dbname, *args, **kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 335, in checked_call\n",
      "    result = self.endpoint(*a, **kw)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 937, in __call__\n",
      "    return self.method(*args, **kw)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 515, in response_wrap\n",
      "    response = f(*args, **kw)\n",
      "  File \"/home/openerp/odoo-dev/odoo/addons/web/controllers/main.py\", line 934, in call_kw\n",
      "    return self._call_kw(model, method, args, kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/addons/web/controllers/main.py\", line 926, in _call_kw\n",
      "    return call_kw(request.env[model], method, args, kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/api.py\", line 687, in call_kw\n",
      "    return call_kw_model(method, model, args, kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/api.py\", line 672, in call_kw_model\n",
      "    result = method(recs, *args, **kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/addons/itriplee/models/garantias.py\", line 53, in create\n",
      "    fecha_compra_inicial = datetime.strptime(fecha_compra, fm).date()\n",
      "TypeError: strptime() argument 1 must be str, not bool\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084912/why-is-the-output-2-when-i-run-this-code\n",
      "Why is the output 2 when i run this code?\n",
      "\n",
      "\n",
      "Doing a bioinformatic course on coursera, for the function below, the input is GCGCG, the output should be 2 but nothing puts up when i try to run the code.\n",
      "def PatternCount(Text, Pattern):\n",
      " count = 0\n",
      " for i in range(len(Text)-len(Pattern)+1):\n",
      "    if Text[i:i+len(Pattern)] == Pattern:\n",
      "        count = count+1\n",
      " return count \n",
      "Text= \"GCGCG\"\n",
      "Pattern= \"GCG\"\n",
      "print PatternCount(Text, Pattern)\n",
      "\n",
      "Answer\n",
      "\n",
      "Check out SyntaxVoid comment. => Convert to python 3\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084901/telethon-issue-need-to-read-messages-from-channels\n",
      "Telethon issue, need to read messages from channels\n",
      "\n",
      "\n",
      "I've read a lot of answers here and tried their code, but I cannot get the examples to work for me and I don't understand why.\n",
      "I want to capture messages from telegram channels but noone of telehon methods works for me...\n",
      "I'm using telethon but it is not important how. I just need to capture these messages.\n",
      "from telethon import TelegramClient, events,utils\n",
      "from telethon.tl.functions.messages import GetFullChatRequest\n",
      "from telethon.tl.functions.messages import GetHistoryRequest\n",
      "from telethon.tl.functions.channels import GetChannelsRequest\n",
      "from telethon.tl.functions.contacts import ResolveUsernameRequest\n",
      "from telethon.tl.types import PeerUser, PeerChat, PeerChannel , sync\n",
      "import re\n",
      "\n",
      "\n",
      "client = TelegramClient('session_name', api id, api hash )\n",
      "client.start()\n",
      "my_self=client.get_me()\n",
      "print(myself)\n",
      "\n",
      "This is the output I get:\n",
      " C:/Users/Riccardo/Desktop/yobitpumpR.py:15: RuntimeWarning: coroutine \n",
      " 'AuthMethods._start' was never awaited\n",
      "\n",
      " C:/Users/Riccardo/Desktop/yobitpumpR.py:19: RuntimeWarning: coroutine \n",
      " 'get_me' was never awaited\n",
      " <coroutine object get_me at 0x00000083F93E9410>\n",
      "\n",
      "How should I proceed?\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084899/how-to-modify-np-array-by-slicing-through-the-broadcast-of-an-index-vector\n",
      "How to modify np array by slicing through the broadcast of an index vector?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have this m x n numpy array which I want to apply certain operation over the row elements. Although, it must be cast only on those elements whose index is prior to those specified by the entries on a vector of indexes.\n",
      "I've already gone through the classic for-loop way, but I was expecting something more NumPythonic.\n",
      "The following code would complete the job:\n",
      "for i,j in enumerate(x):\n",
      "    M[i, 0:j] = 2*M[i, 0:j]\n",
      "\n",
      "But I was look for a broadcast, no-for approach. Any Ideas?\n",
      "For example, lets say that\n",
      "M = [[ 1, 2, 3, 4, 5],\n",
      "     [ 6, 7, 8, 9,10]]\n",
      "\n",
      "x = [2, 3]\n",
      "\n",
      "and our application is to double certain element. According to the indexes specified in x we should have the resulting array:\n",
      "M = [[ 2, 4, 3, 4, 5],\n",
      "     [12,14,16, 9,10]]\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084898/how-to-use-python-create-the-chart-below-or-what-library-i-should-use\n",
      "How to use Python create the chart below? or what library I should use?\n",
      "\n",
      "\n",
      "I want to create a chart for companies interlocks as below (but I don't know how this chart was created...) Does anyone have clues about what library or data I should use? \n",
      "Connected companies chart\n",
      "I used Python Networkx package to build a graph but didn't get the same output.\n",
      "\n",
      "g = nx.from_pandas_edgelist(df, source='CompanyList1', target='CompanyList2')\n",
      "Any thoughts or suggestions about how to create the chart above using Python?\n",
      "Thanks!\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084884/how-can-i-improve-my-function-to-be-more-memory-efficient\n",
      "How can I improve my function to be more memory efficient?\n",
      "\n",
      "\n",
      "I'm writing an AWS Lambda in Python 3.6\n",
      "I have a large amount of large space separated text files and I need to loop through these files and pull out the first N (in this case 1000) lines of text. Once I have those lines I need to put them in to a new file and upload that to S3.\n",
      "I'm also not a python developer, so the language and environment is new to me.\n",
      "Right now I'm collecting the S3 object summaries, and for each of those, I'm running a check on them and then getting the object's data, opening that as a file-like object and also opening the output variable as a file-like object, and then doing my processing.\n",
      "I've given my Lambda 3GB RAM but the lambda is running out of memory before it can process any files (Each file is about 800MB and there are about 210 of them).\n",
      "    for item in object_summary:\n",
      "        # Check if the object exists, and skip it if so\n",
      "        try:\n",
      "            head_object_response = s3Client.head_object(Bucket=target_bucket_name, Key=item)\n",
      "            logger.info('%s: Key alredy exists.' % item)\n",
      "        except:\n",
      "            # if the key does not exist, we need to swallow the 404 that comes from boto3\n",
      "            pass\n",
      "\n",
      "        # and then do our logic to headify the files\n",
      "        logger.info('Key does not exist in target, headifying: %s' % item)\n",
      "\n",
      "        # If the file doesn't exist, get the full object\n",
      "        s3_object = s3Client.get_object(Bucket=inputBucketName, Key=item)\n",
      "        long_file = s3_object['Body']._raw_stream.data\n",
      "        file_name = item\n",
      "        logger.info('%s: Processing 1000 lines of input.' % file_name)\n",
      "\n",
      "        '''\n",
      "        Looks like the Lambda hits a memory limit on the line below.\n",
      "        It crashes with 2500MB of memory used, the file it's trying \n",
      "        to open at that stage is 800MB large which puts it over the \n",
      "        max allocation of 3GB\n",
      "        '''\n",
      "        try:\n",
      "            with open(long_file, 'r') as input_file, open(file_name, 'w') as output_file:\n",
      "                for i in range(1000):\n",
      "                    output_file.write(input_file.readline())\n",
      "        except OSError as exception:\n",
      "            if exception.errno ==36:\n",
      "                logger.error('File name: %s' %exception.filename)\n",
      "                logger.error(exception.__traceback__)\n",
      "\n",
      "I put the whole function for completeness above, but I think that the specific area I can improve it is the try: while: block that handles the file processing. \n",
      "Have I got that right? Is there anywhere else I can improve it?\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084861/4d-plot-in-python-matplotlib\n",
      "4D plot in python matplotlib\n",
      "\n",
      "\n",
      "I am quite new with python programming so i am a bit lost with it. \n",
      "I am trying to create a scatter3D plot which plots X, Y, Z coordinates for time position and use the color as 4 dimension to plot the value. \n",
      "The problem i am facing is that... it seems impossible to me to add the variable as colour and add a colorbar for help. \n",
      "I explain the background here: I have created a meshgrid for X, Y, Z and an array for computing the value in each position for X, Y, Z coordinates (which is an trimensional array). \n",
      "It follows an loop for filling the array with the values for each X,Y,Z combination and then i try to create the graph. \n",
      "x = np.linspace(0, 15, 15)\n",
      "y = np.linspace(0, 30, 15)\n",
      "z = np. linspace(5, 45, 15)\n",
      "\n",
      "X, Y, Z = np.meshgrid(x, y, z)\n",
      "\n",
      "sumatorio = np.zeros(shape=(len(X[0, :, 0]), len(Y[:, 0, 0])))\n",
      "\n",
      "sumatorio_1 = np.zeros(shape=(len(Y[:, 0, 0]), len(Z[0, 0, :])))\n",
      "\n",
      "sum_total = np.zeros(shape=(len(X[:, 0, 0]), len(Y[0, :, 0]), len(Z[0, 0, \n",
      ":])))\n",
      "\n",
      "c = np.array([])\n",
      "\n",
      "for i in range(len(X[0, :, 0])):\n",
      "    for j in range(len(Y[:, 0, 0])):\n",
      "        sum_1 = X[0, i, 0] + Y[j, 0, 0]\n",
      "        sumatorio[i, j] = sum_1\n",
      "            for k in range(len(Z[0, 0, :])):\n",
      "            sum_2 = sum_1 ** 2.0 + Z[0, 0, k] + X[0, i, 0]\n",
      "            sumatorio_1[j, k] = sum_2\n",
      "            sum_total[i, j, k] = sumatorio[i, j] + sumatorio_1[j, k]\n",
      "fig = plt.figure(figsize=(50.0, 50.0))\n",
      "ax = fig.add_subplot(111, projection='3d')\n",
      "\n",
      "ax.scatter3D(X, Y, Z, c=sum_total, cmap='coolwarm', depthshade=0)\n",
      "fig.colorbar(sum_total)\n",
      "\n",
      "plt.title(\"DV at departure from Earth\")\n",
      "ax.set_xlabel(\"Beginning\")\n",
      "ax.set_ylabel(\"Time of flight\")\n",
      "ax.set_zlabel(\"Time of flight 2\")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "When i execute the code, the following error comes into view: \n",
      "'c' argument has 15 elements, which is not acceptable for use with 'x' with size 3375, 'y' with size 3375.\n",
      "\n",
      "It seems when i put the array sum_total here, it did not recognize the shape of it.\n",
      "I try to add the color as c=np.ravel(sum_total) which returns me the graphic with color, but i think it is not given the appropiate color to each point.\n",
      "Also, when i create the graphic with np.ravel(sum_total), the colorbar gives the following error: \" 'numpy.ndarray' object has no attribute 'get_array'\"\n",
      "Thank you in advance! I will appreciate the help as i wrote, i am quite new with python.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084842/how-to-install-mysqlclient\n",
      "How to install mysqlclient?\n",
      "\n",
      "\n",
      "I try to install mysqlclient on Windows 10 with \n",
      "pip install mysqlclient\n",
      "I have visual studio 2019 and python 3.7.4 also the build tools the last one and I update the pip\n",
      "But I get this error:\n",
      "ERROR: Complete output from command 'c:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import setuptools, tokenize;__file__='\"'\"'C:\\\\Users\\\\ponce\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yg2gzqfi\\\\mysqlclient\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\ponce\\AppData\\Local\\Temp\\pip-record-34g102m_\\install-record.txt' --single-version-externally-managed --compile:\n",
      "    ERROR: running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win32-3.7\n",
      "    creating build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\__init__.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\_exceptions.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\compat.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\connections.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\converters.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\cursors.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\release.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\times.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    creating build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\__init__.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\CLIENT.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\CR.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\ER.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\FIELD_TYPE.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\FLAG.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    running build_ext\n",
      "    building 'MySQLdb._mysql' extension\n",
      "    creating build\\temp.win32-3.7\n",
      "    creating build\\temp.win32-3.7\\Release\n",
      "    creating build\\temp.win32-3.7\\Release\\MySQLdb\n",
      "    C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.21.27702\\bin\\HostX86\\x86\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,4,2,'post',1) -D__version__=1.4.2.post1 \"-IC:\\Program Files (x86)\\MySQL\\MySQL Connector C 6.1\\include\\mariadb\" -Ic:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\include -Ic:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.21.27702\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\" /TcMySQLdb/_mysql.c /Fobuild\\temp.win32-3.7\\Release\\MySQLdb/_mysql.obj /Zl /D_CRT_SECURE_NO_WARNINGS\n",
      "    _mysql.c\n",
      "    MySQLdb/_mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory\n",
      "    error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.21.27702\\\\bin\\\\HostX86\\\\x86\\\\cl.exe' failed with exit status 2\n",
      "    ----------------------------------------\n",
      "ERROR: Command \"'c:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import setuptools, tokenize;__file__='\"'\"'C:\\\\Users\\\\ponce\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yg2gzqfi\\\\mysqlclient\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\ponce\\AppData\\Local\\Temp\\pip-record-34g102m_\\install-record.txt' --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\ponce\\AppData\\Local\\Temp\\pip-install-yg2gzqfi\\mysqlclient\\\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084817/how-to-fix-a-i-in-rangen-when-it-refuses-to-repeat\n",
      "How to fix a “i in range(n)” when it refuses to repeat\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am trying to copy the code from a Python tutorial https://youtu.be/BfS2H1y6tzQ?t=156 and copied it word for word, but it isn't working like in the video.\n",
      "import random\n",
      "\n",
      "def random_walk(n):\n",
      "    \"\"\"Return coodrinates after 'n' block random walk.\"\"\"\n",
      "    x = 0\n",
      "    y = 0\n",
      "    for i in range(n):\n",
      "        step = random.choice(['N','S','E','W'])\n",
      "        if step == 'N':\n",
      "            y = y + 1\n",
      "        elif step == 'S':\n",
      "            y = y - 1\n",
      "        elif step == 'E':\n",
      "            x = x + 1\n",
      "        elif step == 'W':\n",
      "            x = x - 1\n",
      "        return (x, y)\n",
      "\n",
      "I expected the \"for i in range(n):\" to repeat the next line for the n amount of times, but it seems to only try it once, thus diving the new coordinates a one number difference.  I was expecting a result more like the one at https://youtu.be/BfS2H1y6tzQ?t=176. For reference, I'm using Python 3.7.3.\n",
      "\n",
      "Answer\n",
      "\n",
      "Your return statement has a wrong intent, and you have to delete some blanks before it. Below is the correct code, and be careful of the last line.\n",
      "import random\n",
      "\n",
      "def random_walk(n):\n",
      "    \"\"\"Return coodrinates after 'n' block random walk.\"\"\"\n",
      "    x = 0\n",
      "    y = 0\n",
      "    for i in range(n):\n",
      "        step = random.choice(['N','S','E','W'])\n",
      "        if step == 'N':\n",
      "            y = y + 1\n",
      "        elif step == 'S':\n",
      "            y = y - 1\n",
      "        elif step == 'E':\n",
      "            x = x + 1\n",
      "        elif step == 'W':\n",
      "            x = x - 1\n",
      "\n",
      "    return (x, y)\n",
      "You are returning (x,y) in the for loop. Just indent it properly.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084787/changing-marker-size-in-pyplot-plot\n",
      "Changing Marker Size in pyplot.plot\n",
      "\n",
      "\n",
      "I wish to change the marker size in a matplotlib.pyplot.plot NOT a scatter.  The documentation has the kwarg \"markersize\", but adding that to the command has not changed the markersize.  I know this can be done in a scatter, but I don't want a scatter plot, I want to still be able to connect the points.  As far as I can tell, points can only be connected in a pyplot.plot and not a scatter.\n",
      "import matplotlib as plt\n",
      "size = 30.\n",
      "plt.plot(xArray,yArray, alpha = 0.75, marker = 'o', color = 'b', ls = '--', markersize = size)\n",
      "\n",
      "Changing size does not change the size of the points in my plot.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084773/change-weights-on-base-of-mean-prediction-keras\n",
      "Change weights on base of mean prediction keras\n",
      "\n",
      "\n",
      "I'm setting up a few shot image segmentation with keras. My first network (VGG-like) outputs a feature vector with the shape [2,1024] that separates information of foreground and background.\n",
      "I want to compare one prototypical output (fix image) with the mean feature vector of all the other images. My loss function is a nearest neighbor implementation to update the weights.\n",
      "My GPU can only take 4 images max. with an accurate resolution.\n",
      "My question is, how to calculate the losses based on a mean output of all images while training.\n",
      "Would be greateful for every hint.\n",
      "The input is an image with separated marked foreground and marked background with shape [2,512,512,1]\n",
      "    def conv_model(self, input):\n",
      "        c1 = Conv2D(self.feat_lev_1, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (input) # added input shape\n",
      "        c1 = BatchNormalization()(c1)\n",
      "        c1 = Conv2D(self.feat_lev_1, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
      "        c1 = BatchNormalization()(c1)\n",
      "        p1 = MaxPooling2D((2, 2)) (c1)\n",
      "\n",
      "        c2 = Conv2D(self.feat_lev_2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
      "        c2 = BatchNormalization()(c2)\n",
      "        c2 = Conv2D(self.feat_lev_2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
      "        c2 = BatchNormalization()(c2)\n",
      "        p2 = MaxPooling2D((2, 2)) (c2)\n",
      "\n",
      "        c3 = Conv2D(self.feat_lev_3, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
      "        c3 = BatchNormalization()(c3)\n",
      "        c3 = Conv2D(self.feat_lev_3, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
      "        c3 = BatchNormalization()(c3)\n",
      "        p3 = MaxPooling2D((2, 2)) (c3)\n",
      "\n",
      "        c4 = Conv2D(self.feat_lev_4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
      "        c4 = BatchNormalization()(c4)\n",
      "        c4 = Conv2D(self.feat_lev_4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
      "        c4 = BatchNormalization()(c4)\n",
      "        # added\n",
      "        #d4 = Dropout(0.5)(c4)\n",
      "        p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
      "\n",
      "        c5 = Conv2D(self.feat_lev_5, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
      "        c5 = BatchNormalization()(c5)\n",
      "        c5 = Conv2D(self.feat_lev_5, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
      "        c5 = BatchNormalization()(c5)\n",
      "        #c5 = Dropout(0.3) (c5) # 0.5\n",
      "        result = GlobalAveragePooling2D()(c5)# activation='softmax'\n",
      "\n",
      "        return result\n",
      "\n",
      "I already used the predict function of keras to extract the mean vector. But running the network always uses the input images separately.\n",
      "    # the prototypical image\n",
      "    prototype = model.predict(xv)\n",
      "\n",
      "    learner_train = DataGenerator_Learner(X, Y, prototype)\n",
      "\n",
      "    # create mean prototype\n",
      "    prototypes = []\n",
      "    for i in range(len(X)):\n",
      "        element = learner_train.get_data(i)\n",
      "        prototypes.append(model.predict(element))\n",
      "    mean_prototype = tf.add_n(prototypes) / len(X)\n",
      "\n",
      "    result = model.fit_generator(generator=learner_train, epochs=num_epochs, callbacks=[checkpointer, earlystopper], steps_per_epoch=1)\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084762/have-a-string-and-trying-to-input-that-input-a-tableview\n",
      "Have a string and trying to input that input a tableview\n",
      "\n",
      "\n",
      "So here's the problem that I am having, I have to read from multiple different files and each file has a different format of how it represents the data, so for example one file could be:\n",
      "1 2 3 4 5\n",
      "and another could be:\n",
      "0\n",
      "and another from the file can be is:\n",
      "col1 col2 col3\n",
      "\n",
      "0    2     1\n",
      "\n",
      "how can I use a tableview to output this type of data.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084760/mechanize-requests-post-image-encoded-picture\n",
      "Mechanize / requests post image encoded picture\n",
      "\n",
      "\n",
      "I am trying to upload an image to a webpage, where the image file gets encoded while uploading.\n",
      "The encoded picture looks like that:\n",
      "\n",
      "I tryed it with some different encoding types, but i think i am doing it wrong.\n",
      "# Base64\n",
      "with open(r'C:/Users/Oli/Google Drive/IMG_20190616_153823.jpg', 'rb') as image_file:\n",
      "    encoded_string = base64.b64encode(image_file.read())\n",
      "\n",
      "# bz2\n",
      "with open(r'C:/Users/Oli/Google Drive/IMG_20190616_153823.jpg', 'rb') as image_file:\n",
      "    encoded_string = bz2.open(image_file)\n",
      "    print(bz2.compress(encoded_string))\n",
      "\n",
      "# hex_codec\n",
      "import binascii\n",
      "with open(r'C:/Users/Oli/Google Drive/IMG_20190616_153823.jpg', 'rb') as image_file:\n",
      "    encoded_string = binascii.b2a_hex(image_file)\n",
      "\n",
      "# quopri_codec\n",
      "import quopri\n",
      "with open(r'C:/Users/Oli/Google Drive/IMG_20190616_153823.jpg', 'rb') as image_file:\n",
      "    txt_file = open(r'C:\\Users\\Oli\\Downloads\\IF\\out.txt', 'w')\n",
      "    encoded_string = quopri.encode(image_file, txt_file, quotetabs=True)\n",
      "    print(encoded_string)\n",
      "\n",
      "\n",
      "# uu_codec\n",
      "\n",
      "import uu\n",
      "with open(r'C:/Users/Oli/Google Drive/IMG_20190616_153823.jpg', 'rb') as image_file:\n",
      "    txt_file = open(r'C:\\Users\\Oli\\Downloads\\IF\\out.txt', 'w')\n",
      "    encoded_string = uu.encode(image_file, txt_file)\n",
      "    print(encoded_string)\n",
      "\n",
      "# zlib_codec\n",
      "import zlib\n",
      "with open(r'C:/Users/Oli/Google Drive/IMG_20190616_153823.jpg', 'rb') as image_file:\n",
      "    txt_file = open(r'C:\\Users\\Oli\\Downloads\\IF\\out.txt', 'w')\n",
      "    encoded_string = zlib.compress(image_file)\n",
      "    print(encoded_string)\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084757/python-how-does-the-while-not-loop-work\n",
      "Python: How does the “while not” loop work?\n",
      "\n",
      "\n",
      "I am new to programming. Sorry if I break any rules of asking. I am currently trying to learn python. I was going through Al Sweigart's Automate the Boring stuff with Python. In his example of while loops, he uses a \"not\" condition with his while loop (as shown in the code below)\n",
      "name = ''\n",
      "while not name != '':\n",
      "    print('Enter your name:')\n",
      "    name = input()\n",
      "print('How many guests will you have?')\n",
      "numOfGuests = int(input())\n",
      "if numOfGuests !=0:\n",
      "    print('Be sure to have enough room for all your guests.')\n",
      "print('Done')\n",
      "\n",
      "This code works fine. I am confused about how this works though. We set name to '' (blank value) and then in the while loop we have \"while not name !='' \". Why does this not work with \"while name != '' \". I am sorry again if this is a silly question but I am a little bit confused. Thank you for the help\n",
      "\n",
      "Answer\n",
      "\n",
      "The while loop will only loop as long as the condition after it remains true. Putting a not before the condition inverts it. not True == False, not False == True\n",
      "while not name != '' will loop as long as (not (name != '')) is True.\n",
      "The not operator will invert your condition so the while loop condition is logically equivalent to saying, while name is equal to the empty string ''. This is because you have the statement, name != '', and then you use the not operator on it which inverts it. This is so the while loop will continue to request the user for an input name that does not equal ''.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084730/how-to-send-webkitformboundary-post-via-python\n",
      "How to send WebKitFormBoundary post via python\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there I am facing with this issue and I am having a real hard time \n",
      "I am trying to make a nice webclient of a 4chan board and ways \n",
      "the problem that I am facing is that the payload is WebKitFormBoundary\n",
      "Now the post-payload is \n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"resto\"\n",
      "\n",
      "804890085\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"com\"\n",
      "\n",
      "Hello world this is a test\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"mode\"\n",
      "\n",
      "regist\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"pwd\"\n",
      "\n",
      "_HwWOVhsZy08kMr1Gmq75BLc293fANR64\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"g-recaptcha-response\"\n",
      "\n",
      "03AOLTBLTaPVx7Y96wcfYaraLNGRPA_-sXCt6Bu5O3n8SgsnBRgc0zi38L9L6BMf3azYoV2GsjWy9Epm7YBCY0ciHLVVPE3o8rmNef24DQajXHqp21yRcw1k4z0qTR2PvUNSXr7BCQkWzk5h5bay8nCXX7dWf6BKKa3aR4nid2umKXciIrqhp5zfAtdM2BOASdfNg1NiPRX_pJKhkL6AQNbjsRPyL0VSNZLC8Qcn4wu8ITcbc7Pdnzz2kxl_KzOjqh7vbYdawPQgVF\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW--\n",
      "\n",
      "the headers is \n",
      ":authority: sys.4chan.org\n",
      ":method: POST\n",
      ":path: /b/post\n",
      ":scheme: https\n",
      "accept: */*\n",
      "accept-encoding: gzip, deflate, br\n",
      "accept-language: en-US,en;q=0.9\n",
      "content-length: 848\n",
      "content-type: multipart/form-data; boundary=----WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "cookie: __cfduid=d822b2332925f67ed3a21d3fcd15ce42d1563307415; 4chan_pass=_HwWOVhsZy08kMr1Gmq75BLc293fANR64\n",
      "origin: https://boards.4chan.org\n",
      "referer: https://boards.4chan.org/\n",
      "user-agent: Mozilla/5.0 (X11; U; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3690.133 Safari/537.36\n",
      "\n",
      "Now I want to focus on \n",
      "content-length: 848\n",
      "content-type: multipart/form-data; boundary=----WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "\n",
      "And also how to I get 'boundary' / 'WebKitFormBoundary'?\n",
      "and I how to I get the right amount of 'content-length'?\n",
      "Can someone can hit me with a solution please ? I am have a real hard time solving it no meter what I do it seem like I miss something and the post doesn't work, lol trying no meter what but I cant figure out how to deal with it thanks! I never had to deal with this kind of system before it seem kind of weird I think I might miss some values ... what should I do to get it right.\n",
      "would like to get a semi demo python file that deals with it and explication about how to deal with it in python ! \n",
      "Thanks for everyone who is willing to help or even try ! much appreciated!\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084723/enumerating-columns-of-large-data-using-numpy\n",
      "Enumerating columns of large data using NumPy\n",
      "\n",
      "\n",
      "I have a large data, and I want to name the columns, for instance '1', '2', ... . For a small data, I can do\n",
      "np.random.randint(5, size=(50, 2))   # synthesis data\n",
      "A = A.ravel().view([('1','i8'),('2','i8'),]).astype([('1','i4'),('2','i8'),])\n",
      "\n",
      "and then call an individual column using\n",
      "print(A['2'])\n",
      "\n",
      "I was wondering if there is a way to automate this for any random size and column numbers. My preference is to use NumPy, not Pandas. Thanks!\n",
      "\n",
      "Answer\n",
      "\n",
      "Extending from your work, you can use a list comprehension to accomplish this. It will automatically create the required number of columns with the proper labels:\n",
      "A = np.random.randint(5, size=(10, 10))\n",
      "B = A.ravel().view([ (str(x),'i4') for x in range(1, len(A[0])+1) ])\n",
      "\n",
      "Then you can do print(A['2']) from 1 to 10 in this case.\n",
      "Pandas would make a good job of this, so if your only reason to avoid it is so you don't need to get to grips with another library, it may be worth your time learning Pandas so you can use it in future.\n",
      "An alternative is xarray, which handles n-dimensional data.\n",
      "http://xarray.pydata.org/en/stable/\n",
      "You can name both the axes (e.g. calling one \"time\") and the values in them (e.g. calling a particular point on the time axis \"start\").  You could then slice out all the data that has value \"start\" on the \"time\" axis.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084710/programing-how-do-i-ask-questions-am-new-operator\n",
      "Programing how do i ask questions am new-operator\n",
      "\n",
      "\n",
      "Pls can I know how to ask questions on this platform concerning python program I am new in this site and I need help on this and in programing. Tnks\n",
      "Adei\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084704/how-to-combine-desired-values-from-multiple-csv-table\n",
      "How to combine desired values from multiple CSV table\n",
      "\n",
      "\n",
      "I have numerous amounts of csv files that contains a table with longitude, latitude, date, and wave height. I want to create a table that contains the date and wave heights of values that lie within a certain longitude/longitude from all csv files. \n",
      "For example, if I have two csv files that each contains 2 values that falls within the desired longitude/latitude, then I want to create a table that has the 4 values with their corresponding dates and wave height. This table would be very large, so I prefer to convert this into excel.\n",
      "How would I approach this with python?\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084664/capturing-picture-of-webview-display-of-raspberry-pi-camera-using-html\n",
      "Capturing Picture of webview display of raspberry pi camera using HTML\n",
      "\n",
      "\n",
      "I am live-streaming my raspberry pi camera with some python code, and I created an app to access the webview of the stream with android studio. I want to be able to take a picture using the raspberry pi camera with the click of a button.\n",
      "import io\n",
      "import picamera\n",
      "import logging\n",
      "import socketserver\n",
      "from threading import Condition\n",
      "from http import server\n",
      "\n",
      "PAGE=\"\"\"\\\n",
      "<html>\n",
      "<head>\n",
      "<title>Raspberry Pi - Surveillance Camera</title>\n",
      "</head>\n",
      "<body>\n",
      "<center><h1>Raspberry Pi - Surveillance Camera</h1></center>\n",
      "<center><img src=\"stream.mjpg\" width=\"640\" height=\"480\"></center>\n",
      "</body>\n",
      "</html>\n",
      "\"\"\"\n",
      "\n",
      "class StreamingOutput(object):\n",
      "    def __init__(self):\n",
      "        self.frame = None\n",
      "        self.buffer = io.BytesIO()\n",
      "        self.condition = Condition()\n",
      "\n",
      "    def write(self, buf):\n",
      "        if buf.startswith(b'\\xff\\xd8'):\n",
      "            # New frame, copy the existing buffer's content and notify all\n",
      "            # clients it's available\n",
      "            self.buffer.truncate()\n",
      "            with self.condition:\n",
      "                self.frame = self.buffer.getvalue()\n",
      "                self.condition.notify_all()\n",
      "            self.buffer.seek(0)\n",
      "        return self.buffer.write(buf)\n",
      "\n",
      "class StreamingHandler(server.BaseHTTPRequestHandler):\n",
      "    def do_GET(self):\n",
      "        if self.path == '/':\n",
      "            self.send_response(301)\n",
      "            self.send_header('Location', '/index.html')\n",
      "            self.end_headers()\n",
      "        elif self.path == '/index.html':\n",
      "            content = PAGE.encode('utf-8')\n",
      "            self.send_response(200)\n",
      "            self.send_header('Content-Type', 'text/html')\n",
      "            self.send_header('Content-Length', len(content))\n",
      "            self.end_headers()\n",
      "            self.wfile.write(content)\n",
      "        elif self.path == '/stream.mjpg':\n",
      "            self.send_response(200)\n",
      "            self.send_header('Age', 0)\n",
      "            self.send_header('Cache-Control', 'no-cache, private')\n",
      "            self.send_header('Pragma', 'no-cache')\n",
      "            self.send_header('Content-Type', 'multipart/x-mixed-replace; boundary=FRAME')\n",
      "            self.end_headers()\n",
      "            try:\n",
      "                while True:\n",
      "                    with output.condition:\n",
      "                        output.condition.wait()\n",
      "                        frame = output.frame\n",
      "                    self.wfile.write(b'--FRAME\\r\\n')\n",
      "                    self.send_header('Content-Type', 'image/jpeg')\n",
      "                    self.send_header('Content-Length', len(frame))\n",
      "                    self.end_headers()\n",
      "                    self.wfile.write(frame)\n",
      "                    self.wfile.write(b'\\r\\n')\n",
      "            except Exception as e:\n",
      "                logging.warning(\n",
      "                    'Removed streaming client %s: %s',\n",
      "                    self.client_address, str(e))\n",
      "        else:\n",
      "            self.send_error(404)\n",
      "            self.end_headers()\n",
      "\n",
      "class StreamingServer(socketserver.ThreadingMixIn, server.HTTPServer):\n",
      "    allow_reuse_address = True\n",
      "    daemon_threads = True\n",
      "\n",
      "with picamera.PiCamera(resolution='640x480', framerate=24) as camera:\n",
      "    output = StreamingOutput()\n",
      "    #Uncomment the next line to change your Pi's Camera rotation (in degrees)\n",
      "    #camera.rotation = 90\n",
      "    camera.start_recording(output, format='mjpeg')\n",
      "    try:\n",
      "        address = ('', 8000)\n",
      "        server = StreamingServer(address, StreamingHandler)\n",
      "        server.serve_forever()\n",
      "    finally:\n",
      "        camera.stop_recording()\n",
      "\n",
      "I want to be able to take a picture with the raspberry pi and be able to access it on either my phone gallery or in the web view itself.\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084649/what-does-python-regard-as-true\n",
      "What does Python regard as “true”? [duplicate]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just came to know there are Truthy and Falsy values in python which are different from the normal True and False?\n",
      "Can someone please explain in depth what truthy and falsy values are? \n",
      "Where should I use them?\n",
      "What is the difference between truthy and True values and falsy and False values ?\n",
      "\n",
      "Answer\n",
      "\n",
      "As the comments described, it just refers to values which are evaluated to True or False.\n",
      "For instance, to see if a list is not empty, instead of checking like this:\n",
      "if len(my_list) != 0:\n",
      "   print \"Not empty!\"\n",
      "\n",
      "You can simply do this:\n",
      "if my_list:\n",
      "   print \"Not empty!\"\n",
      "\n",
      "This is because some values, such as empty lists, are considered False when evaluated for a boolean value. Non-empty lists are True.\n",
      "Similarly for the integer 0, the empty string \"\", and so on, for False, and non-zero integers, non-empty strings, and so on, for True.\n",
      "The idea of terms like \"truthy\" and \"falsy\" simply refer to those values which are considered True in cases like those described above, and those which are considered False.\n",
      "For example, an empty list ([]) is considered \"falsy\", and a non-empty list (for example, [1]) is considered \"truthy\".\n",
      "See also this section of the documentation.\n",
      "All values are considered \"truthy\" except for the following, which are \"falsy\":\n",
      "\n",
      "None\n",
      "False\n",
      "0\n",
      "0.0\n",
      "0j\n",
      "Decimal(0)\n",
      "Fraction(0, 1)\n",
      "[] - an empty list\n",
      "{} - an empty dict\n",
      "() - an empty tuple\n",
      "'' - an empty str\n",
      "b'' - an empty bytes\n",
      "set() - an empty set\n",
      "an empty range, like range(0)\n",
      "objects for which \n",
      "\n",
      "\n",
      "obj.__bool__() returns False\n",
      "obj.__len__() returns 0 \n",
      "\n",
      "\n",
      "A \"truthy\" value will satisfy the check performed by if or while statements.  We use \"truthy\" and \"falsy\" to differentiate from the bool values True and False.\n",
      "Truth Value Testing\n",
      "Python determines the truthiness by applying bool() to the type, which returns True or False which is used in an expression like if or while. \n",
      "Here is an example for a custom class Vector2dand it's instance returning False when the magnitude (lenght of a vector) is 0, otherwise True.\n",
      "import math\n",
      "class Vector2d(object):\n",
      "    def __init__(self, x, y):\n",
      "        self.x = float(x)\n",
      "        self.y = float(y)\n",
      "\n",
      "    def __abs__(self):\n",
      "        return math.hypot(self.x, self.y)\n",
      "\n",
      "    def __bool__(self):\n",
      "        return bool(abs(self))\n",
      "\n",
      "a = Vector2d(0,0)\n",
      "print(bool(a))        #False\n",
      "b = Vector2d(10,0)    \n",
      "print(bool(b))        #True\n",
      "\n",
      "Note: If we wouldn't have defined __bool__ it would always return True, as instances of a user-defined class are considered truthy by default.\n",
      "Example from the book: \"Fluent in Python, clear, concise and effective programming\"\n",
      "Truthy values refer to the objects used in a boolean context and not so much the boolean value that returns true or false.Take these as an example:\n",
      ">>> bool([])\n",
      "False\n",
      ">>> bool([1])\n",
      "True\n",
      ">>> bool('')\n",
      "False\n",
      ">>> bool('hello')\n",
      "True\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084632/how-to-remove-unnecessary-variables-of-a-dataframe-to-predict-a-binary-output\n",
      "How to remove unnecessary variables of a dataframe to predict a binary output?\n",
      "\n",
      "\n",
      "I have a data set with a huge amount of variables for an output that can either be A or not A. How can I remove the variables that are useless to predict the output.\n",
      "Example\n",
      "I have a data set of loans which status loan_status are currently Fully Paid or Charged OffSince the data set is very large, I would like to keep only the essential variables that would help me predicting the output.\n",
      ">>> subset.head()\n",
      "\n",
      "    id  member_id   loan_amnt   funded_amnt funded_amnt_inv term    int_rate    installment grade   sub_grade   emp_title   emp_length  home_ownership  annual_inc  verification_status issue_d loan_status pymnt_plan  url desc    purpose title   zip_code    addr_state  dti delinq_2yrs earliest_cr_line    inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc    pub_rec revol_bal   revol_util  total_acc   initial_list_status out_prncp   out_prncp_inv   total_pymnt total_pymnt_inv total_rec_prncp total_rec_int   total_rec_late_fee  recoveries  collection_recovery_fee last_pymnt_d    last_pymnt_amnt next_pymnt_d    last_credit_pull_d  collections_12_mths_ex_med  mths_since_last_major_derog policy_code application_type    annual_inc_joint    dti_joint   verification_status_joint   acc_now_delinq  tot_coll_amt    tot_cur_bal open_acc_6m open_act_il open_il_12m open_il_24m mths_since_rcnt_il  total_bal_il    il_util open_rv_12m open_rv_24m max_bal_bc  all_util    total_rev_hi_lim    inq_fi  total_cu_tl inq_last_12m    acc_open_past_24mths    avg_cur_bal bc_open_to_buy  bc_util chargeoff_within_12_mths    delinq_amnt mo_sin_old_il_acct  mo_sin_old_rev_tl_op    mo_sin_rcnt_rev_tl_op   mo_sin_rcnt_tl  mort_acc    mths_since_recent_bc    mths_since_recent_bc_dlq    mths_since_recent_inq   mths_since_recent_revol_delinq  num_accts_ever_120_pd   num_actv_bc_tl  num_actv_rev_tl num_bc_sats num_bc_tl   num_il_tl   num_op_rev_tl   num_rev_accts   num_rev_tl_bal_gt_0 num_sats    num_tl_120dpd_2m    num_tl_30dpd    num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75    pub_rec_bankruptcies    tax_liens   tot_hi_cred_lim total_bal_ex_mort   total_bc_limit  total_il_high_credit_limit  revol_bal_joint sec_app_earliest_cr_line    sec_app_inq_last_6mths  sec_app_mort_acc    sec_app_open_acc    sec_app_revol_util  sec_app_open_act_il sec_app_num_rev_accts   sec_app_chargeoff_within_12_mths    sec_app_collections_12_mths_ex_med  sec_app_mths_since_last_major_derog hardship_flag   hardship_type   hardship_reason hardship_status deferral_term   hardship_amount hardship_start_date hardship_end_date   payment_plan_start_date hardship_length hardship_dpd    hardship_loan_status    orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  hardship_last_payment_amount    debt_settlement_flag    debt_settlement_flag_date   settlement_status   settlement_date settlement_amount   settlement_percentage   settlement_term\n",
      "11  NaN NaN 10000   10000   10000.0 60 months   14.07%  233.05  C   C3  Teacher 4 years RENT    42000.0 Source Verified Mar-2018    Fully Paid  n   NaN NaN major_purchase  Major purchase  341xx   FL  24.69   0   Oct-2004    0   32.0    NaN 17  0   707 15.7%   34  w   0.0 0.0 11153.669505    11153.67    10000.00    1153.67 0.0 0.0 0.0 Mar-2019    10.38   NaN Jun-2019    0   40.0    1   Individual  NaN NaN NaN 0   0   93913   0   15  0   0   54.0    93206   116.0   0   1   707 111.0   4500    0   0   0   1   5524.0  3793.0  15.7    0   0   161.0   88  18  18  0   18.0    32.0    18.0    32.0    14  1   1   2   4   30  2   4   1   17  0.0 0   0   0   43.8    0.0 0   0   84930   93913   4500    80430   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "16  NaN NaN 7000    7000    7000.0  36 months   11.98%  232.44  B   B5  Parole  < 1 year    MORTGAGE    40000.0 Verified    Mar-2018    Fully Paid  n   NaN NaN home_improvement    Home improvement    797xx   TX  20.25   0   Mar-2007    0   60.0    NaN 13  0   5004    36% 29  w   0.0 0.0 7693.314943 7693.31 7000.00 693.31  0.0 0.0 0.0 Mar-2019    5364.25 NaN Mar-2019    0   60.0    1   Individual  NaN NaN NaN 0   0   131726  1   6   0   2   16.0    126722  102.0   2   2   3944    90.0    13900   2   1   4   4   10977.0 4996.0  50.0    0   0   122.0   132 1   1   0   10.0    64.0    5.0 60.0    3   2   2   3   4   19  7   10  2   13  0.0 0   0   2   89.7    33.3    0   0   132817  131726  10000   118917  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "17  NaN NaN 20000   20000   20000.0 60 months   26.77%  607.97  E   E5  Mental Health Provider  3 years RENT    33500.0 Not Verified    Mar-2018    Charged Off n   NaN NaN house   Home buying 604xx   IL  24.40   0   Aug-2008    1   NaN NaN 27  0   7364    46% 34  w   0.0 0.0 7236.150000 7236.15 2195.37 5040.78 0.0 0.0 0.0 Apr-2019    607.97  NaN Jun-2019    0   NaN 1   Individual  NaN NaN NaN 0   308 160804  0   21  0   0   29.0    153440  118.0   0   2   2607    110.0   16000   0   0   2   2   5956.0  2767.0  68.6    0   0   115.0   115 20  20  0   26.0    NaN 5.0 NaN 0   3   6   3   3   27  6   7   6   27  0.0 0   0   0   100.0   33.3    0   0   146514  160804  8800    130514  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "20  NaN NaN 21000   21000   21000.0 60 months   20.39%  560.94  D   D4  Machine operator    10+ years   OWN 85000.0 Source Verified Mar-2018    Fully Paid  n   NaN NaN house   Home buying 135xx   NY  15.76   1   Nov-2008    0   2.0 NaN 15  0   14591   34.2%   27  w   0.0 0.0 24217.170915    24217.17    21000.00    3217.17 0.0 0.0 0.0 Feb-2019    183.26  NaN May-2019    0   NaN 1   Individual  NaN NaN NaN 0   0   128270  1   1   2   2   7.0 37076   NaN 2   5   5354    34.0    42700   6   4   13  8   8551.0  16684.0 38.4    0   0   67.0    112 4   4   3   4.0 NaN 0.0 2.0 0   5   7   6   10  3   12  21  7   15  0.0 0   0   4   92.6    16.7    0   0   172433  51667   27100   39733   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "...\n",
      "\n",
      "My attempt\n",
      "I wasn't able to find a way to reduce the number of useless variables. I only used dataframe.describe() to know  a bit more about them. But even if you try to summarize all the loans, first, I'm not sure it helps me summarize, since I still have many variables, and, second, I'm not sure I do the calculation correctly. In fact, as you can read there :\n",
      "    id  member_id   loan_amnt   funded_amnt funded_amnt_inv term    int_rate    installment grade   sub_grade   emp_title   emp_length  home_ownership  annual_inc  verification_status issue_d loan_status pymnt_plan  url desc    purpose title   zip_code    addr_state  dti delinq_2yrs earliest_cr_line    inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc    pub_rec revol_bal   revol_util  total_acc   initial_list_status out_prncp   out_prncp_inv   total_pymnt total_pymnt_inv total_rec_prncp total_rec_int   total_rec_late_fee  recoveries  collection_recovery_fee last_pymnt_d    last_pymnt_amnt next_pymnt_d    last_credit_pull_d  collections_12_mths_ex_med  mths_since_last_major_derog policy_code application_type    annual_inc_joint    dti_joint   verification_status_joint   acc_now_delinq  tot_coll_amt    tot_cur_bal open_acc_6m open_act_il open_il_12m open_il_24m mths_since_rcnt_il  total_bal_il    il_util open_rv_12m open_rv_24m max_bal_bc  all_util    total_rev_hi_lim    inq_fi  total_cu_tl inq_last_12m    acc_open_past_24mths    avg_cur_bal bc_open_to_buy  bc_util chargeoff_within_12_mths    delinq_amnt mo_sin_old_il_acct  mo_sin_old_rev_tl_op    mo_sin_rcnt_rev_tl_op   mo_sin_rcnt_tl  mort_acc    mths_since_recent_bc    mths_since_recent_bc_dlq    mths_since_recent_inq   mths_since_recent_revol_delinq  num_accts_ever_120_pd   num_actv_bc_tl  num_actv_rev_tl num_bc_sats num_bc_tl   num_il_tl   num_op_rev_tl   num_rev_accts   num_rev_tl_bal_gt_0 num_sats    num_tl_120dpd_2m    num_tl_30dpd    num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75    pub_rec_bankruptcies    tax_liens   tot_hi_cred_lim total_bal_ex_mort   total_bc_limit  total_il_high_credit_limit  revol_bal_joint sec_app_earliest_cr_line    sec_app_inq_last_6mths  sec_app_mort_acc    sec_app_open_acc    sec_app_revol_util  sec_app_open_act_il sec_app_num_rev_accts   sec_app_chargeoff_within_12_mths    sec_app_collections_12_mths_ex_med  sec_app_mths_since_last_major_derog hardship_flag   hardship_type   hardship_reason hardship_status deferral_term   hardship_amount hardship_start_date hardship_end_date   payment_plan_start_date hardship_length hardship_dpd    hardship_loan_status    orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  hardship_last_payment_amount    debt_settlement_flag    debt_settlement_flag_date   settlement_status   settlement_date settlement_amount   settlement_percentage   settlement_term\n",
      "count   0.0 0.0 110909.000000   110909.000000   110909.000000   110909  110909  110909.000000   110909  110909  101148  101310  110909  1.109090e+05    110909  110909  110909  110909  0.0 0.0 110909  110909  110909  110909  110616.000000   110909.000000   110909  110909.000000   50074.000000    16811.000000    110909.000000   110909.000000   1.109090e+05    110739  110909.000000   110909  110909.0    110909.0    110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110201  110909.000000   0.0 110909  110909.000000   27474.000000    110909.0    110909  14193.000000    14193.000000    13876   110909.000000   1.109090e+05    1.109090e+05    110909.000000   110909.000000   110909.000000   110909.000000   106966.000000   1.109090e+05    92546.000000    110909.000000   110909.000000   110909.000000   110877.000000   1.109090e+05    110909.000000   110909.000000   110909.000000   110909.000000   110903.000000   109104.000000   109017.000000   110909.000000   110909.000000   106966.000000   110909.000000   110909.000000   110909.000000   110909.000000   109225.000000   23011.000000    101505.000000   32915.000000    110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   107934.0    110909.000000   110909.000000   110909.000000   110908.000000   109104.000000   110909.000000   110909.000000   1.109090e+05    1.109090e+05    110909.000000   1.109090e+05    14193.000000    14193   14193.000000    14193.000000    14193.000000    13921.000000    14193.000000    14193.000000    14193.000000    14193.000000    5062.000000 110909  131 131 131 131.0   131.000000  131 131 131 131.0   131.000000  131 26.000000   131.000000  131.000000  110909  1470    1470    1470    1470.000000 1470.000000 1470.000000\n",
      "unique  NaN NaN NaN NaN NaN 2   74  NaN 7   34  22652   11  4   NaN 3   6   2   1   NaN NaN 13  12  867 50  NaN NaN 614 NaN NaN NaN NaN NaN NaN 1042    NaN 2   NaN NaN NaN NaN NaN NaN NaN NaN NaN 18  NaN 0.0 20  NaN NaN NaN 2   NaN NaN 3   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 510 NaN NaN NaN NaN NaN NaN NaN NaN NaN 1   1   8   2   NaN NaN 11  9   9   NaN NaN 4   NaN NaN NaN 2   12  3   13  NaN NaN NaN\n",
      "top NaN NaN NaN NaN NaN 36 months   11.98%  NaN B   B5  Manager 10+ years   MORTGAGE    NaN Source Verified Apr-2018    Fully Paid  n   NaN NaN debt_consolidation  Debt consolidation  945xx   CA  NaN NaN Aug-2005    NaN NaN NaN NaN NaN NaN 0%  NaN w   NaN NaN NaN NaN NaN NaN NaN NaN NaN Jan-2019    NaN NaN Jun-2019    NaN NaN NaN Individual  NaN NaN Not Verified    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aug-2005    NaN NaN NaN NaN NaN NaN NaN NaN NaN N   INTEREST ONLY-3 MONTHS DEFERRAL UNEMPLOYMENT    BROKEN  NaN NaN Mar-2019    Apr-2019    Mar-2019    NaN NaN Late (16-30 days)   NaN NaN NaN N   Jun-2019    ACTIVE  May-2019    NaN NaN NaN\n",
      "freq    NaN NaN NaN NaN NaN 81370   3485    NaN 30833   6964    2030    36774   54812   NaN 44735   29655   87515   110909  NaN NaN 56217   56217   1223    15847   NaN NaN 987 NaN NaN NaN NaN NaN NaN 1843    NaN 92893   NaN NaN NaN NaN NaN NaN NaN NaN NaN 9952    NaN NaN 47569   NaN NaN NaN 96716   NaN NaN 6353    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 164 NaN NaN NaN NaN NaN NaN NaN NaN NaN 110909  131 55  105 NaN NaN 27  27  34  NaN NaN 68  NaN NaN NaN 109439  374 1182    320 NaN NaN NaN\n",
      "mean    NaN NaN 15133.124228    15133.124228    15129.913485    NaN NaN 451.703280  NaN NaN NaN NaN NaN 7.866525e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 18.658485   0.240945    NaN 0.543842    36.703838   82.156683   11.373595   0.162151    1.414609e+04    NaN 23.704154   NaN 0.0 0.0 13093.720734    13090.819511    11834.579108    1065.864157 1.193620    192.083862  34.107295   NaN 8988.970489 NaN NaN 0.018060    45.958725   1.0 NaN 123220.229856   19.046298   NaN 0.000045    3.007498e+02    1.487752e+05    1.048427    2.624494    0.845035    1.801720    18.464428   3.570383e+04    71.071878   1.399805    2.941727    5191.785689 53.011716   3.665310e+04    1.330749    1.696274    2.404187    5.083699    14573.002065    15451.859776    45.630734   0.008728    3.745034    121.109044  171.096953  13.539704   7.627740    1.476652    23.266752   40.444831   6.617723    37.548321   0.499932    3.305349    4.951492    4.712936    7.286199    8.664689    8.084673    13.359015   4.904345    11.346311   0.0 0.000045    0.066108    2.404593    94.636698   29.690390   0.145534    0.016464    1.886942e+05    5.006460e+04    25196.060734    4.486298e+04    32620.767914    NaN 0.753047    1.609244    11.366589   55.994512   2.893328    12.913619   0.052913    0.083492    36.830304   NaN NaN NaN NaN 3.0 194.040611  NaN NaN NaN 3.0 15.007634   NaN 677.778462  14977.221908    197.404962  NaN NaN NaN NaN 7235.872456 51.259034   18.638776\n",
      "std NaN NaN 10063.626492    10063.626492    10062.605730    NaN NaN 291.529754  NaN NaN NaN NaN NaN 8.273838e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 20.105508   0.759381    NaN 0.791792    21.864945   23.731842   5.938104    0.445418    2.109029e+04    NaN 12.636174   NaN 0.0 0.0 10677.374875    10675.875613    10274.588973    1127.992027 8.430334    844.626003  150.986459  NaN 9417.206405 NaN NaN 0.149077    21.220192   0.0 NaN 61899.006621    8.171933    NaN 0.006714    2.447485e+04    1.670928e+05    1.212451    2.877455    1.041107    1.708049    23.830165   4.476172e+04    22.776782   1.595896    2.734340    5728.066547 22.057862   3.675638e+04    1.715969    2.911280    2.682558    3.482714    17992.870300    20176.943227    29.912941   0.107913    340.178661  54.487416   97.531613   17.183313   8.446866    1.798038    32.073651   22.185204   5.688223    22.098123   1.393569    2.312572    3.307021    3.167090    4.704991    7.612502    4.949893    8.197814    3.218891    5.928676    0.0 0.006714    0.438064    2.050780    8.906956    34.368549   0.359703    0.254046    1.863843e+05    5.154504e+04    25322.554746    4.691188e+04    28067.364794    NaN 1.108586    1.836520    6.574821    26.571334   3.085046    8.499611    0.510133    0.386138    24.069226   NaN NaN NaN NaN 0.0 144.144733  NaN NaN NaN 0.0 8.734103    NaN 445.752949  9097.533521 207.337176  NaN NaN NaN NaN 5009.981486 8.231359    6.806635\n",
      "min NaN NaN 1000.000000 1000.000000 1000.000000 NaN NaN 30.120000   NaN NaN NaN NaN NaN 0.000000e+00    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.000000    0.000000    NaN 0.000000    0.000000    3.000000    0.000000    0.000000    0.000000e+00    NaN 2.000000    NaN 0.0 0.0 0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    NaN 0.000000    NaN NaN 0.000000    0.000000    1.0 NaN 15400.000000    0.190000    NaN 0.000000    0.000000e+00    0.000000e+00    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000e+00    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000e+00    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    1.000000    2.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    2.000000    0.000000    0.000000    0.0 0.000000    0.000000    0.000000    12.500000   0.000000    0.000000    0.000000    0.000000e+00    0.000000e+00    0.000000    0.000000e+00    0.000000    NaN 0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    NaN NaN NaN NaN 3.0 5.950000    NaN NaN NaN 3.0 0.000000    NaN 151.860000  424.110000  0.070000    NaN NaN NaN NaN 413.930000  29.920000   1.000000\n",
      "25% NaN NaN 7200.000000 7200.000000 7200.000000 NaN NaN 232.750000  NaN NaN NaN NaN NaN 4.595300e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 10.400000   0.000000    NaN 0.000000    18.000000   67.000000   7.000000    0.000000    4.085000e+03    NaN 15.000000   NaN 0.0 0.0 5007.935312 5007.250000 3500.000000 292.490000  0.000000    0.000000    0.000000    NaN 823.590000  NaN NaN 0.000000    29.000000   1.0 NaN 83862.000000    13.000000   NaN 0.000000    0.000000e+00    2.739200e+04    0.000000    1.000000    0.000000    1.000000    6.000000    8.377000e+03    58.000000   0.000000    1.000000    1747.000000 38.000000   1.530000e+04    0.000000    0.000000    1.000000    3.000000    2958.000000 2983.000000 19.900000   0.000000    0.000000    83.000000   104.000000  4.000000    3.000000    0.000000    6.000000    23.000000   2.000000    20.000000   0.000000    2.000000    3.000000    3.000000    4.000000    4.000000    5.000000    8.000000    3.000000    7.000000    0.0 0.000000    0.000000    1.000000    92.300000   0.000000    0.000000    0.000000    5.299800e+04    1.849600e+04    8900.000000 1.466100e+04    14026.000000    NaN 0.000000    0.000000    7.000000    36.500000   1.000000    7.000000    0.000000    0.000000    15.000000   NaN NaN NaN NaN 3.0 90.360000   NaN NaN NaN 3.0 7.000000    NaN 338.100000  7428.245000 40.040000   NaN NaN NaN NaN 3466.597500 45.000000   18.000000\n",
      "50% NaN NaN 12000.000000    12000.000000    12000.000000    NaN NaN 368.690000  NaN NaN NaN NaN NaN 6.500000e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 16.780000   0.000000    NaN 0.000000    34.000000   86.000000   10.000000   0.000000    9.061000e+03    NaN 22.000000   NaN 0.0 0.0 10297.070860    10294.240000    10000.000000    696.570000  0.000000    0.000000    0.000000    NaN 6119.670000 NaN NaN 0.000000    46.000000   1.0 NaN 110000.000000   18.560000   NaN 0.000000    0.000000e+00    8.415200e+04    1.000000    2.000000    1.000000    1.000000    11.000000   2.329500e+04    74.000000   1.000000    2.000000    3859.000000 54.000000   2.710000e+04    1.000000    0.000000    2.000000    4.000000    7869.000000 8439.000000 43.100000   0.000000    0.000000    128.000000  154.000000  8.000000    5.000000    1.000000    13.000000   39.000000   5.000000    35.000000   0.000000    3.000000    4.000000    4.000000    6.000000    7.000000    7.000000    12.000000   4.000000    10.000000   0.0 0.000000    0.000000    2.000000    100.000000  18.200000   0.000000    0.000000    1.248450e+05    3.653800e+04    17800.000000    3.328500e+04    25326.000000    NaN 0.000000    1.000000    10.000000   57.800000   2.000000    11.000000   0.000000    0.000000    35.000000   NaN NaN NaN NaN 3.0 146.390000  NaN NaN NaN 3.0 17.000000   NaN 367.950000  15652.360000    146.070000  NaN NaN NaN NaN 5891.670000 45.230000   20.000000\n",
      "75% NaN NaN 20000.000000    20000.000000    20000.000000    NaN NaN 620.660000  NaN NaN NaN NaN NaN 9.500000e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 24.122500   0.000000    NaN 1.000000    53.000000   101.000000  14.000000   0.000000    1.725500e+04    NaN 31.000000   NaN 0.0 0.0 18766.242514    18755.060000    17500.000000    1443.380000 0.000000    0.000000    0.000000    NaN 13912.100000    NaN NaN 0.000000    63.000000   1.0 NaN 148000.000000   24.650000   NaN 0.000000    0.000000e+00    2.285040e+05    2.000000    3.000000    1.000000    3.000000    21.000000   4.672800e+04    87.000000   2.000000    4.000000    6943.000000 69.000000   4.640000e+04    2.000000    2.000000    3.000000    7.000000    20477.000000    19997.000000    70.500000   0.000000    0.000000    152.000000  220.000000  17.000000   10.000000   2.000000    27.000000   57.000000   10.000000   53.000000   0.000000    4.000000    6.000000    6.000000    9.000000    11.000000   10.000000   17.000000   6.000000    14.000000   0.0 0.000000    0.000000    3.000000    100.000000  50.000000   0.000000    0.000000    2.771000e+05    6.409300e+04    33000.000000    6.074100e+04    41986.000000    NaN 1.000000    3.000000    15.000000   77.300000   4.000000    17.000000   0.000000    0.000000    57.000000   NaN NaN NaN NaN 3.0 279.065000  NaN NaN NaN 3.0 23.000000   NaN 1193.032500 22114.265000    291.190000  NaN NaN NaN NaN 10090.202500    60.000000   24.000000\n",
      "max NaN NaN 40000.000000    40000.000000    40000.000000    NaN NaN 1628.080000 NaN NaN NaN NaN NaN 9.300000e+06    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 999.000000  19.000000   NaN 5.000000    226.000000  126.000000  86.000000   52.000000   1.113293e+06    NaN 129.000000  NaN 0.0 0.0 51653.389338    51653.390000    40000.000000    13842.210000    320.700000  33122.070000    5961.972600 NaN 41353.670000    NaN NaN 8.000000    226.000000  1.0 NaN 1000000.000000  39.980000   NaN 1.000000    6.214661e+06    4.151547e+06    13.000000   49.000000   6.000000    16.000000   383.000000  1.378570e+06    309.000000  22.000000   28.000000   389468.000000   175.000000  1.680300e+06    32.000000   40.000000   67.000000   33.000000   393082.000000   331957.000000   146.600000  9.000000    65000.000000    518.000000  806.000000  368.000000  260.000000  24.000000   551.000000  152.000000  24.000000   152.000000  36.000000   50.000000   72.000000   59.000000   66.000000   105.000000  72.000000   94.000000   65.000000   82.000000   0.0 1.000000    17.000000   23.000000   100.000000  100.000000  5.000000    52.000000   4.358152e+06    1.394335e+06    460900.000000   1.380346e+06    280272.000000   NaN 6.000000    17.000000   58.000000   212.600000  35.000000   75.000000   20.000000   11.000000   117.000000  NaN NaN NaN NaN 3.0 649.970000  NaN NaN NaN 3.0 29.000000   NaN 1369.860000 32300.260000    1072.990000 NaN NaN NaN NaN 28503.000000    80.000000   24.000000\n",
      "\n",
      "It seems unlikely that the loan_amnt count is 110909.000000 when you see the first 4 lines (and there are many).\n",
      "Annex : how to recreate the dataset\n",
      "I used a csv file that I downloaded here (bank loans for 2018. They are divided into four quarters). Using Python 3 can be obtained by doing:\n",
      "import pandas as pd \n",
      "# Control delimiters, rows, column names with read_csv (see later) \n",
      "data_Q1 = pd.read_csv(\"LoanStats_2018Q1.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "data_Q2 = pd.read_csv(\"LoanStats_2018Q2.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "data_Q3 = pd.read_csv(\"LoanStats_2018Q2.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "data_Q4 = pd.read_csv(\"LoanStats_2018Q2.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "frames = [data_Q1,data_Q2,data_Q3,data_Q4]\n",
      "\n",
      "result = pd.concat(frames)\n",
      "subset = result.loc[result[\"loan_status\"].isin(['Charged Off','Fully Paid'])]\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084629/printing-into-a-txt-file\n",
      "Printing into a txt file\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to print in a txt file only then a specific event, that is my code:\n",
      "txt = tutto()\n",
      "file = open(\"psw.txt\", \"w\")\n",
      "file.write(str(txt))\n",
      "file.close()\n",
      "\n",
      "tutto() is a def ( def tutto(): ). In tutto there are a lot of print commands. How can I redirect all these print commands only after tutto() is started?\n",
      "\n",
      "Answer\n",
      "\n",
      "print accepts a file parameter that designates where the output will be printed to. By default, the location is sys.stdout (which is normally your terminal). You can modify the definition of tutto so that it uses a special print function. functools.partial is used to help you create your own print function so that you don't have to type print(\"...\", file = fout) everywhere. Once tutto ends, print will return to its normal behavior since you are only changing it's definition in the scope of tutto. \n",
      "import sys\n",
      "from functools import partial\n",
      "\n",
      "def tutto(fout = sys.stdout):\n",
      "    print = partial(__builtins__.print, file = fout)\n",
      "    print(\"Here are all\")\n",
      "    print(\"my print statements\")\n",
      "    print(\"They will automatically show up in\")\n",
      "    print(\"The file designated by fout...\")\n",
      "    return\n",
      "\n",
      "with open(\"Sample.txt\", \"w\") as f:\n",
      "    print(\"Calling tutto\")\n",
      "    tutto(f)\n",
      "    print(\"Tutto is finished, and I'll appear in your terminal.\")\n",
      "\n",
      "Outputs:\n",
      "Terminal:\n",
      "Calling tutto\n",
      "Tutto is finished, and I'll appear in your terminal.\n",
      "\n",
      "Sample.txt:\n",
      "Here are all\n",
      "my print statements\n",
      "They will automatically show up in\n",
      "The file designated by fout...\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084592/what-is-the-behavior-of-sortedarr-when-arr-is-a-numpy-array-with-nans\n",
      "What is the behavior of sorted(arr) when arr is a numpy array with nans? [duplicate]\n",
      "\n",
      "\n",
      "sorted([2, float('nan'), 1]) returns [2, nan, 1]\n",
      "(At least on Activestate Python 3.1 implementation.)\n",
      "I understand nan is a weird object, so I wouldn't be surprised if it shows up in random places in the sort result. But it also messes up the sort for the non-nan numbers in the container, which is really unexpected.\n",
      "I asked a related question about max, and based on that I understand why sort works like this. But should this be considered a bug?\n",
      "Documentation just says \"Return a new sorted list [...]\" without specifying any details.\n",
      "EDIT: \n",
      "I now agree that this isn't in violation of the IEEE standard. However, it's a bug from any common sense viewpoint, I think. Even  Microsoft, which isn't known to admit their mistakes often, has recognized this one as a bug, and fixed it in the latest version: http://connect.microsoft.com/VisualStudio/feedback/details/363379/bug-in-list-double-sort-in-list-which-contains-double-nan.\n",
      "Anyway, I ended up following @khachik's answer:\n",
      "sorted(list_, key = lambda x : float('-inf') if math.isnan(x) else x)\n",
      "\n",
      "I suspect it results in a performance hit compared to the language doing that by default, but at least it works (barring any bugs that I introduced).\n",
      "\n",
      "Answer\n",
      "\n",
      "The previous answers are useful, but perhaps not clear regarding the root of the problem.\n",
      "In any language, sort applies a given ordering, defined by a comparison function or in some other way, over the domain of the input values.  For example, less-than, a.k.a. operator <, could be used throughout if and only if less than defines a suitable ordering over the input values.\n",
      "But this is specifically NOT true for floating point values and less-than:\n",
      "\"NaN is unordered: it is not equal to, greater than, or less than anything, including itself.\" (Clear prose from GNU C manual, but applies to all modern IEEE754 based floating point)\n",
      "So the possible solutions are:\n",
      "\n",
      "\n",
      "remove the NaNs first, making the input domain well defined via <\n",
      "  (or the other sorting function being used)\n",
      "define a custom comparison function (a.k.a. predicate) that does\n",
      "  define an ordering for NaN, such as less than any number, or greater\n",
      "  than any number.\n",
      "\n",
      "\n",
      "Either approach can be used, in any language.\n",
      "Practically, considering python, I would prefer to remove the NaNs if you either don't care much about fastest performance or if removing NaNs is a desired behavior in context. \n",
      "Otherwise you could use a suitable predicate function via \"cmp\" in older python versions, or via this and  functools.cmp_to_key().  The latter is a bit more awkward, naturally, than removing the NaNs first. And care will be required to avoid worse performance, when defining this predicate function.\n",
      "The problem is that there's no correct order if the list contains a NAN, since a sequence a1, a2, a3, ..., an is sorted if a1 <= a2 <= a3 <= ... <= an. If any of these a values is a NAN then the sorted property breaks, since for all a, a <= NAN and NAN <= a are both false.\n",
      "I'm not sure about the bug, but the workaround may be the following:\n",
      "sorted(\n",
      "    (2, 1, float('nan')),\n",
      "    lambda x,y: x is float('nan') and -1 \n",
      "                or (y is float('nan') and 1\n",
      "                or cmp(x,y)))\n",
      "\n",
      "which results in:\n",
      "('nan', 1, 2)\n",
      "\n",
      "Or remove nans before sorting or anything else.\n",
      "IEEE754 is the standard that defines floating point operations in this instance. This standard defines the compare operation of operands, at least one of which is a NaN, to be an error.  Hence, this is not a bug.  You need to deal with the NaNs before operating on your array.\n",
      "Assuming you want to keep the NaNs and order them as the lowest \"values\", here is a workaround working both with non-unique nan, unique numpy nan, numerical and non numerical objects:\n",
      "def is_nan(x):\n",
      "    return (x is np.nan or x != x)\n",
      "\n",
      "list_ = [2, float('nan'), 'z', 1, 'a', np.nan, 4, float('nan')]\n",
      "sorted(list_, key = lambda x : float('-inf') if is_nan(x) else x)\n",
      "# [nan, nan, nan, 1, 2, 4, 'a', 'z']\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084554/apscheduler-not-remembering-jobs-after-heroku-sleeps\n",
      "APScheduler not remembering jobs after Heroku sleeps\n",
      "\n",
      "\n",
      "I am currently trying to implement APScheduler through Heroku. I have the scheduler working through Heroku with Python just as expected when the Heroku is running. The issue I am running into is when the Heroku dyno goes to sleep after 30 minutes of inactivity. I want to be able to make Heroku active again and have the scheduler remember all of the jobs that were not triggered before it went to sleep. I am using the SQLAlchemyJobStore which is supposed to remember jobs after the scheduler is shut down.\n",
      "Here is the code that I am using currently:\n",
      "from apscheduler.schedulers.blocking import BlockingScheduler\n",
      "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
      "\n",
      "jobstores = {\n",
      "    'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
      "}\n",
      "job_defaults = {\n",
      "    'coalesce': False,\n",
      "    'max_instances': 1\n",
      "}\n",
      "\n",
      "sched = BlockingScheduler()\n",
      "sched.configure(jobstores=jobstores, job_defaults=job_defaults, timezone='America/New_York')\n",
      "\n",
      "def my_job(text):\n",
      "    print(text)\n",
      "\n",
      "def add_jobs():\n",
      "    print(\"scheduling\")\n",
      "    sched.add_job(my_job, 'date', run_date='2019-7-17 15:25:00', args=['date job firing'], id = \"Job1\", misfire_grace_time = 18000)\n",
      "    sched.add_job(my_job, 'date', run_date='2019-7-17 15:30:00', args=['date to run'], id = \"Job2\", misfire_grace_time=18000)\n",
      "    sched.add_job(my_job, 'date', run_date='2019-7-17 16:30:00', args=['job after shutdown'], id = 'Job3', misfire_grace_time = 18000)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    add_jobs()\n",
      "    #sched.print_jobs()\n",
      "    sched.start()\n",
      "\n",
      "In this case, I commit and push the code to heroku, which runs this file in heroku. The results I get are as followed:\n",
      "2019-07-17T19:24:38.000000+00:00 app[api]: Build started by user\n",
      "2019-07-17T19:25:03.445241+00:00 heroku[clock.1]: State changed from down to starting\n",
      "2019-07-17T19:25:03.227462+00:00 app[api]: Release v268 created by user \n",
      "2019-07-17T19:25:03.227462+00:00 app[api]: Deploy 6d599479 by user\n",
      "2019-07-17T19:25:09.364735+00:00 heroku[clock.1]: Starting process with command `python clock.py`\n",
      "2019-07-17T19:25:10.118949+00:00 heroku[clock.1]: State changed from starting to up\n",
      "2019-07-17T19:25:13.928223+00:00 app[clock.1]: scheduling\n",
      "2019-07-17T19:25:14.142912+00:00 app[clock.1]: date job firing\n",
      "2019-07-17T19:25:14.000000+00:00 app[api]: Build succeeded\n",
      "2019-07-17T19:30:00.051064+00:00 app[clock.1]: date to run\n",
      "\n",
      "30 minutes after the the previous job triggered, Heroku sleeps. This is when I ping Heroku to wake up and then the last job at 16:30:00 does not trigger at the time it should. \n",
      "I believe the issue might have to do with the database. I am not sure what is in the database at any certain time or if the scheduler is pulling the non executed jobs every time Heroku goes to sleep. \n",
      "If anyone knows why this is happening that would be very helpful.\n",
      "\n",
      "Answer\n",
      "\n",
      "You are using an SQLite database to store your jobs. This doesn't play nicely with Heroku's ephemeral filesystem, which loses all changes every time your dyno restarts. This happens frequently (at least once per day) and likely occurs when your app sleeps and then wakes back up.\n",
      "You should use a client-server database instead. Heroku's Postgres service might be a better choice. It's supported out of the box and the base plan is free.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084553/how-do-i-override-qlistwidget-dragged-items\n",
      "How do I override QListWidget dragged items?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a QListWidget populated with file paths (i.e \"C:/Folder/file.ext\"). I have the list set to drag and drop via qlistwidget.setDragDropMode(QAbstractItemView.DragDrop)\n",
      "And it works, but it drags the text it looks like?\n",
      "I want to convert the path to a URL, so I can drag the file in the listwidget into other programs, an explorer window, etc.\n",
      "Is there an event I can override to override the content of the drag?\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084552/migrating-problems-when-porting-django-project-to-python-3-and-django-2\n",
      "migrating problems when porting django project to python 3 and django 2\n",
      "\n",
      "\n",
      "I've been porting a Django project to python 3 and Django 2. I have had to add on_delete to all my models with foreign keys as required in Django 2. Now I have tried to make migrations for those changes have been getting TypeError: __init__() missing 1 required positional argument: 'on_delete' the file it references is the 0002 migration file not the models file which has been updated. I am not sure how to go about fixing this. I have tried faking the migrations and I still get the same error. I am not sure why it thinks the database doesn't exist, I have checked and everything is intact and working in postgres. Any ideas?\n",
      "\n",
      "Answer\n",
      "\n",
      "Since django-2.0 ForeignKey fields [Django-doc] and OneToOneField fields fields now have a required on_delete parameter.\n",
      "This is specified in the release notes of Django-2.0 under Features removed in 2.0:\n",
      "\n",
      "The on_delete argument for ForeignKey and OneToOneField is now required in models and migrations. Consider squashing migrations so that you have fewer of them to update.\n",
      "\n",
      "You thus should inspect your migration files for ForeignKeys and OneToOneFields, and add an on_delete parameter, like:\n",
      "class Migration(migrations.Migration):\n",
      "\n",
      "    initial = False\n",
      "\n",
      "    dependencies = [\n",
      "        ('app', '0001_initial'),\n",
      "    ]\n",
      "\n",
      "    operations = [\n",
      "        migrations.CreateModel(\n",
      "            name='Model',\n",
      "            fields=[\n",
      "                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n",
      "                ('some_foreignkey', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='app.OtherModel')),\n",
      "            ],\n",
      "        ),\n",
      "    ]\n",
      "You should inspect the documentation on the on_delete parameter to see what deletion strategy is the best for each situation. The options are, at the time of writing CASCADE, PROTECT, SET_NULL, SET_DEFAULT, SET(..), DO_NOTHING.\n",
      "If you did not specify the on_delete in the pre-django-2.0 versions, it made a default to CASCADE. So if you want the same behavior, you should add on_delete=django.db.models.deletion.CASCADE. This is noted in the 1.11 version of the documentation on on_delete:\n",
      "\n",
      "Deprecated since version 1.9: on_delete will become a required argument in Django 2.0. In older versions it defaults to CASCADE.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084532/importing-modules-is-consuming-too-much-memory\n",
      "Importing modules is consuming too much memory\n",
      "\n",
      "\n",
      "I have a pretty big application, that uses a lot of different libraries.\n",
      "The result is that memory consumption is a bit high. Just an import of python pandas consumes over 50mb on my system. In total, the process needs more than 200mb to run. That's too much, especially if multiple processes are started. \n",
      "Is there some way to optimize memory consumption? Or is it normal that just an import of python pandas already needs more than 50 MB?\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084530/creating-multiple-dataframes-2-based-off-value-in-one-column-and-stop-when-ano\n",
      "Creating multiple dataframes (2) based off value in one column and stop when another value is observed\n",
      "\n",
      "\n",
      "A dataframe structure looks like so:\n",
      "Date   Amount1     Amount2     Amount3\n",
      "\n",
      "NaN     Port        NaN        thing1\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3      \n",
      "NaN     Port        NaN        thing2\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3  \n",
      "Total     Nan        NaN        NaN\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3  \n",
      "NaN        Nan       Nan         Nan\n",
      "\n",
      "I am interested in creating 2 dataframes.\n",
      "1.One dataframe collects the rows after thing2 is observed in the amount3 column and stops the row before total is observed in the date column.\n",
      "2.The 2nd dataframe will start on the row after total is observed and stop when a NaN (null) value is observed.\n",
      "\n",
      "Answer\n",
      "\n",
      "You can do with idxmax and .iloc \n",
      "df1=df.loc[df.Amount3.eq('thing2').idxmax():df.Date.eq('Total').idxmax()-1].copy()\n",
      "df2=df.loc[df.Date.eq('Total').idxmax():]\n",
      "df2=df2.loc[:df2.Date.isnull().idxmax()-1]\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084508/scrapy-robotstxt-obey-true-gives-an-error\n",
      "Scrapy: ROBOTSTXT_OBEY=True gives an error\n",
      "\n",
      "\n",
      "When setting ROBOTSTXT_OBEY=True in scrapy settings I get the following error: \n",
      "    TypeError: to_bytes must receive a unicode, str or bytes object, got list.\n",
      "I've tried multiple websites to see if it was an issue with the robots.txt file, but i get the same error for all websites.  Even for google \n",
      "    scrapy shell https://www.google.com/ --set=\"ROBOTSTXT_OBEY=True\"\n",
      "TypeError: to_bytes must receive a unicode, str or bytes object, got list\n",
      "\n",
      "How can I respect robots.txt and crawl using scrapy?\n",
      "\n",
      "Answer\n",
      "\n",
      "Found out the issue.  When USER_AGENT is set I get the error.  If no USER_AGENT is set then ROBOTSTXT_OBEY=True works without error.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084473/return-rows-with-max-min-values-at-bottom-of-dataframe-python-pandas\n",
      "Return rows with max/min values at bottom of dataframe (python/pandas)\n",
      "\n",
      "\n",
      "I want to write a function that can look at a dataframe, find the max or min value in a specified column, then return the entire datafrane with the row(s) containing the max or min value at the bottom.\n",
      "I have made it so that the rows with the max or min value alone get returned.\n",
      "def findAggregate(df, transType, columnName=None):\n",
      "\n",
      "    if transType == 'max1Column':\n",
      "        return df[df[columnName] == df[columnName].max()]\n",
      "\n",
      "    elif transType == 'min1Column':\n",
      "        return df[df[columnName] == df[columnName].min()]\n",
      "\n",
      "Given the dataframe below, I want to check col2 for the MIN value\n",
      "Original Dataframe:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "orange   18       cat\n",
      "black    6        fish\n",
      "\n",
      "Expected output:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "orange   18       cat\n",
      "black    6        fish\n",
      "blue     2        dog\n",
      "\n",
      "Actual output:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "\n",
      "Answer\n",
      "\n",
      "Use idxmin or idxmax:\n",
      "df.append(df.iloc[df['col2'].idxmin()], ignore_index=True)\n",
      "\n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "3    blue     2   dog\n",
      "Focus on the index values\n",
      "And use one loc\n",
      "i = df.col2.idxmin()\n",
      "df.loc[[*df.index] + [i]]\n",
      "\n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "0    blue     2   dog\n",
      "\n",
      "\n",
      "Same idea but with Numpy and iloc\n",
      "i = np.arange(len(df))\n",
      "a = df.col2.to_numpy().argmin()\n",
      "df.iloc[np.append(i, a)]\n",
      "\n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "0    blue     2   dog\n",
      "You can do this in oneliner:\n",
      "df.append(df.loc[df['col2'].idxmin()])\n",
      "\n",
      "Output:\n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "0    blue     2   dog\n",
      "So sort_values\n",
      "df.append(df.sort_values('col2').iloc[[0]])\n",
      "Out[764]: \n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "0    blue     2   dog\n",
      "\n",
      "If need max \n",
      "df.append(df.sort_values('col2').iloc[[-1]])\n",
      "\n",
      "adding both min and max \n",
      "df.append(df.sort_values('col2').iloc[[0,-1]])\n",
      "Out[765]: \n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "You can do it various ways. Here is one:\n",
      "def findAggregate(df, transType, columnName=None):\n",
      "\n",
      "    if transType == 'max1Column':\n",
      "        return df.append(df.loc[df['col2'].idxmax()])\n",
      "\n",
      "    elif transType == 'min1Column':\n",
      "        return df.append(df.loc[df['col2'].idxmin()])\n",
      "Here is another way, but not the most elegant:\n",
      "In [30]: df.append(df.loc[lambda x:x.col2 == x.col2.min()]).reset_index(drop=True)\n",
      "Out[30]:\n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "3    blue     2   dog\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084443/how-to-fix-modulenotfounderror-for-python-service\n",
      "How to fix 'ModuleNotFoundError: ' for Python Service\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module name is 'PService.py'.\n",
      "'PService' (_svc_name_) and 'Reader' (_svc_display_name_)\n",
      "Using python PService.py install, I've made a service and shows up in the services.msc.\n",
      "Going through lots of different suggestions, spent almost all day trying to fix, however, I was not able to solve ModuleNotFoundError: No module named 'PService'..\n",
      "Thank you in advance\n",
      "I've read through all resources that I can possibly find, some had same error, but there was no solutions..\n",
      "This is running on Windows Server 2012 R2, I am using Python 3.6.\n",
      "\n",
      "Y:>python PService.py debug\n",
      "Debugging service PService - press Ctrl+C to stop.\n",
      "Error 0xC000000A - Python could not construct the class instance\n",
      "\n",
      "(null): (null)\n",
      "\n",
      "Services:\n",
      "Windows could not start the SiteReader on Local Computer. For more information, review the System Event log... refer to service-specific error code 1.\n",
      "System Event Properties:\n",
      "The Reader service terminated with the following service-specific error: \n",
      "Incorrect function.\n",
      "Application Event Properties: (Source: Python Service)\n",
      "Python could not import the service's module \n",
      "ModuleNotFoundError: No module named 'PService' \n",
      "%2: %3\n",
      "Event Id: 4\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084413/how-to-reduce-the-install-size-of-a-spacy-based-python-application\n",
      "How to Reduce the Install Size of a Spacy based Python Application\n",
      "\n",
      "\n",
      "I'm building a docker container that contains a the Python library spacy. I'm now trying to reduce the size of this container, and spacy appears to be the main contributor to the disk size.\n",
      "Without any models installed, and without any other code/dependencies etc, spacy consumes around 500MB of disk when installed! Does anyone have any useful hints/tips on installing spacy in a disk-space-friendly manner. \n",
      "My repro steps are:\n",
      "mkdir foo1                  # create a folder \n",
      "cd foo1                     # change directory\n",
      "python3 -m venv .venv       # create virtual environment\n",
      "source .venv/bin/activate   # activate virtual environment\n",
      "pip install --upgrade pip   # upgrade pip\n",
      "pip install spacy           # install spacy\n",
      "\n",
      "After doing this, I then navigate into the following folder...\n",
      "foo1/.venv/lib/python3.7/site-packages\n",
      "... and can see that the spacy folder is very large:\n",
      "$ du -sh spacy\n",
      "425M    spacy\n",
      "\n",
      "Specifically, it's the language folder that's large:\n",
      "$ du -sh spacy/lang\n",
      "401M spacy/lang\n",
      "\n",
      "There are 52 languages in that folder, and for many situations I only care about one or two languages. Specifically, for my current situation, that's English.\n",
      "When I look at the sizes, English is the 14th largest (only showing the top 14 in this list)...\n",
      "$ du -sH spacy/lang/* | sort -n -r \n",
      "\n",
      "142024 spacy/lang/tr\n",
      "86608 spacy/lang/pt\n",
      "78368 spacy/lang/nb\n",
      "76592 spacy/lang/da\n",
      "74840 spacy/lang/sv\n",
      "60672 spacy/lang/ca\n",
      "50880 spacy/lang/es\n",
      "48296 spacy/lang/fr\n",
      "41688 spacy/lang/de\n",
      "36960 spacy/lang/nl\n",
      "34008 spacy/lang/it\n",
      "32632 spacy/lang/ro\n",
      "24160 spacy/lang/lt\n",
      "8712 spacy/lang/en  <--- THE ONLY ONE I WANT\n",
      "\n",
      "Is there a spacy-specifc way of installing spacy without all of these languages?\n",
      "I can hack around post-install, but is there a safer way to install fewer languages?\n",
      "Versions installed, on MacOS, by the above steps are as follows:\n",
      "$ pip freeze\n",
      "blis==0.2.4\n",
      "certifi==2019.6.16\n",
      "chardet==3.0.4\n",
      "cymem==2.0.2\n",
      "idna==2.8\n",
      "murmurhash==1.0.2\n",
      "numpy==1.16.4\n",
      "plac==0.9.6\n",
      "preshed==2.0.1\n",
      "requests==2.22.0\n",
      "spacy==2.1.6\n",
      "srsly==0.0.7\n",
      "thinc==7.0.8\n",
      "tqdm==4.32.2\n",
      "urllib3==1.25.3\n",
      "wasabi==0.2.2\n",
      "\n",
      "$ python --version\n",
      "Python 3.7.4\n",
      "\n",
      "Answer\n",
      "\n",
      "I raised this as an issue against the spacy project on GitHub, and it looks like this is a known issue, and that there are plans to address the size of spacy installs.\n",
      "https://github.com/explosion/spaCy/issues/3983\n",
      "So, at this time, there isn't a supported/recommended way to reduce the size of the package install.\n",
      "If you tack an && rm -Rf foo1/.venv/lib/python3.7/site-packages/spacy/lang/tr onto the end of the RUN pip install spacy command that I presume you have in your Dockerfile, you can delete all the files for that language without letting them get saved into a layer in the Docker container.\n",
      "I'm not sure if you would still have a working spacy after just ripping out the languages you didn't want, and you'd have to basically repeat the command for each language you don't want to keep, but it might work as a workaround until spacy makes itself smaller or more modular.\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084389/unable-to-list-all-vertices-present-in-janusgraph-with-tolist-using-gremlin\n",
      "Unable to list all vertices present in Janusgraph with “.toList()” using Gremlinpython\n",
      "\n",
      "\n",
      "I have tried testing what is in a graph that I created to see whether nodes were indeed created.\n",
      "The code to create a small graph for testing:\n",
      "from gremlin_python import statics\n",
      "from gremlin_python.structure.graph import Graph\n",
      "from gremlin_python.process.graph_traversal import __\n",
      "from gremlin_python.process.strategies import *\n",
      "from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection\n",
      "\n",
      "graph = Graph()\n",
      "g = graph.traversal().withRemote(DriverRemoteConnection('ws://localhost:8182/gremlin','g'))\n",
      "\n",
      "# in a loop add nodes and properties to get a small graph for testing\n",
      "t = g.addV('testnode').property('val',1)\n",
      "    for i in range(2,11):\n",
      "    t = g.addV('testnode').property('val', i)\n",
      "    t.iterate()\n",
      "\n",
      "# proceed to create edge (as_ and from_ contain an underscore because as & from are python's reserved words)\n",
      "g.V().has(\"val\", 2).as_(\"a\").V().has(\"val\", 4).as_(\"b\").addE(\"link\").property(\"someproperty\", \"abc\").from_(\"a\").to(\"b\").iterate()\n",
      "\n",
      "list1 = []\n",
      "list1 = g.V().has(\"val\", 2).toList()\n",
      "print(len(list1))\n",
      "\n",
      "to which I would expect to have the value \"1\" returned in the terminal, which happened correctly while testing previously (and now fails).\n",
      "However, this returns an error:\n",
      "Traceback (most recent call last):\n",
      "  File \"test_addingVEs.py\", line 47, in <module>\n",
      "    list1 = g.V().has(\"val_i\", 2).toList()\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/process/traversal.py\", line 52, in toList\n",
      "    return list(iter(self))\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/process/traversal.py\", line 43, in __next__\n",
      "    self.traversal_strategies.apply_strategies(self)\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/process/traversal.py\", line 346, in apply_strategies\n",
      "    traversal_strategy.apply(traversal)\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/driver/remote_connection.py\", line 143, in apply\n",
      "    remote_traversal = self.remote_connection.submit(traversal.bytecode)\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/driver/driver_remote_connection.py\", line 54, in submit\n",
      "    results = result_set.all().result()\n",
      "  File \"/usr/lib/python3.5/concurrent/futures/_base.py\", line 405, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.5/concurrent/futures/_base.py\", line 357, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/driver/resultset.py\", line 81, in cb\n",
      "    f.result()\n",
      "  File \"/usr/lib/python3.5/concurrent/futures/_base.py\", line 398, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.5/concurrent/futures/_base.py\", line 357, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.5/concurrent/futures/thread.py\", line 55, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/driver/connection.py\", line 77, in _receive\n",
      "    self._protocol.data_received(data, self._results)\n",
      "  File \"/home/user/.local/lib/python3.5/site-packages/gremlin_python/driver/protocol.py\", line 98, in data_received\n",
      "    \"{0}: {1}\".format(status_code, message[\"status\"][\"message\"]))\n",
      "gremlin_python.driver.protocol.GremlinServerError: 598: \n",
      "    A timeout occurred during traversal evaluation of [RequestMessage\n",
      "    {, requestId=d56cce63-77f3-4c1f-9c14-3f5f33d4a67b, op='bytecode', processor='traversal', args={gremlin=[[], [V(), has(val, 2)]], aliases={g=g}}}]\n",
      "     - consider increasing the limit given to scriptEvaluationTimeout\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084381/python-how-to-annotate-pointer-to-c-character-array-in-method-signature\n",
      "Python how to annotate pointer to C character array in method signature\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am writing a wrapper for a C library in Python.  I am trying to properly annotate all of the methods, so my IDE can help me catch errors.  I am stuck annotating one method, can you help me figure out the proper annotation?\n",
      "One of the methods in the C library works as follows:\n",
      "\n",
      "Takes one arg: pointer to a character buffer\n",
      "\n",
      "\n",
      "Buffer is made via: char_buffer = ctypes.create_string_buffer(16)\n",
      "\n",
      "Populates the char buffer with the output value\n",
      "\n",
      "\n",
      "Done via CMethod(char_buffer)\n",
      "\n",
      "\n",
      "One then parses the buffer by doing something like char_buffer.value.\n",
      "How can I annotate the wrapper method to look for a pointer to a character buffer?  Currently, I have the below, but I feel like this is incorrect.\n",
      "from ctypes import POINTER\n",
      "\n",
      "\n",
      "def wrapped_method(char_buffer: POINTER):\n",
      "    CMethod(char_buffer)\n",
      "\n",
      "\n",
      "Thank you in advance for your help!\n",
      "\n",
      "Answer\n",
      "\n",
      "no answer\n",
      "======================================== \n",
      "\n",
      "Question\n",
      "\n",
      "http://stackoverflow.com/questions/57084371/how-to-generate-a-new-dataframe-in-python-by-considering-conditions-from-other-d\n",
      "How to generate a new dataframe in python by considering conditions from other dataframes?\n",
      "\n",
      "\n",
      "I am performing data manipulation in python using pandas on a very large dataset, say 100 million rows. I have two dataframes and wish to generate third dataframe as per the conditions mentioned, the scenario is explained below:\n",
      "dataframe 1:\n",
      "Col_B and Col_D are of int64 type\n",
      "Col_A   Col_B   Col_C   Col_D\n",
      " A       11      B       20\n",
      " A       11      C       24\n",
      " B       14      R       19\n",
      "...      ...    ...      ...\n",
      "\n",
      "dataframe 2:\n",
      "Col_Z is of type float64 and remaining columns are of int64\n",
      "Col_X   Col_Y   Col_P   Col_Q   Col_Z\n",
      " 10      15      16      21      0.99\n",
      " 10      15      17      22      0.89\n",
      " 11      15      16      20      0.67\n",
      "...     ...     ...     ...      ...\n",
      "\n",
      "Condition to be applied:\n",
      "Consider only first row of both the dataframe, for the sake of understanding conditions:\n",
      "\n",
      "if the value of (Col_B is between the value of Col_X and Col_Y) and value of (Col_D is between the value of Col_P and Col_Q) then return the corresponding value of Col_A, Col_C and Col_Z, otherwise return NaN\n",
      "\n",
      "Expected Output (Dataframe 3):\n",
      "Col_A   Col_C   Col_Z\n",
      " A       B       0.99\n",
      "NaN     NaN      NaN\n",
      " B       R       0.67\n",
      "\n",
      "Note: This output is generated merely considering if there are only these three rows in dataframes but in actual each value of Dataframe 1 has to scan all of the values in Dataframe 2 until desired conditions is achieved. \n",
      "My Code:\n",
      "df3 = {}\n",
      "Col_A = []\n",
      "Col_C = []\n",
      "Col_Z = []\n",
      "for i in df1.iterrows():    \n",
      "    value = float(df2[(i[1][1] > df2['Col_X'].values) &\n",
      "      (i[1][1] < df2['Col_Y'].values) &\n",
      "      (i[1][3] > df2['Col_P'].values) &\n",
      "      (i[1][3] < df2['Col_Q'].values)]['Col_Z'])\n",
      "\n",
      "    if bool(value):\n",
      "        Col_Z.append(value)\n",
      "        Col_A.append(i[1][0])\n",
      "        Col_C.append(i[1][2])\n",
      "    else:\n",
      "        Col_Z.append(float('NaN'))\n",
      "        Col_A.append(float('NaN'))\n",
      "        Col_C.append(float('NaN'))\n",
      "\n",
      "This code is working fine uptill the condition is met, as soon as condition does'nt met, it throws a TypeError. Please can any rectify this.\n",
      "Also, I wanted to know if there is any alternate and efficient way to perform it. Please let me know.\n",
      "\n",
      "Answer\n",
      "\n",
      "Try this:\n",
      "import pandas as pd\n",
      "\n",
      "df1 = pd.DataFrame({\n",
      "    'Col_A':pd.Series(['A','A','B'],dtype='str'),\n",
      "    'Col_B':pd.Series([11,11,14],dtype='int64'),\n",
      "    'Col_C':pd.Series(['B','C','R'],dtype='str'),\n",
      "    'Col_D':pd.Series([20,24,19],dtype='int64')\n",
      "})\n",
      "\n",
      "df2 = pd.DataFrame({\n",
      "    'Col_X':pd.Series([10,10,11],dtype='int64'),\n",
      "    'Col_Y':pd.Series([15,15,15],dtype='int64'),\n",
      "    'Col_P':pd.Series([16,17,16],dtype='int64'),\n",
      "    'Col_Q':pd.Series([21,22,20],dtype='int64'),\n",
      "    'Col_Z':pd.Series([0.99,0.89,0.67],dtype='float64')\n",
      "})\n",
      "\n",
      "print (df1)\n",
      "\n",
      "print (df2)\n",
      "\n",
      "result = []\n",
      "for i1 in df1.iterrows():   \n",
      "    for i2 in df2.iterrows():\n",
      "\n",
      "        # I write them like this just to be clear!\n",
      "        col_A = i1[1][0]\n",
      "        col_B = i1[1][1]\n",
      "        col_C = i1[1][2]\n",
      "        col_D = i1[1][3]\n",
      "\n",
      "        col_X = i2[1][0]\n",
      "        col_Y = i2[1][1]\n",
      "        col_P = i2[1][2]\n",
      "        col_Q = i2[1][3]\n",
      "        col_Z = i2[1][4]\n",
      "\n",
      "        if (col_X < col_B and col_B < col_Y) and (col_P < col_D and col_D < col_Q):\n",
      "            result.append([col_A, col_C, col_Z])\n",
      "        else:\n",
      "            result.append([float('NaN'),float('NaN'),float('NaN')])\n",
      "\n",
      "df3 = pd.DataFrame(result, columns=['colA', 'col_C', 'col_Z'])\n",
      "\n",
      "print(df3)\n",
      "You can do this by considering the data set as a whole.\n",
      "\n",
      "Firstly, for more convenience, I suggest you to join your two dataset as one dataset. You can do it with the merge function or just concat. Here, I use concat since another solution uses merge. To be clear with what there performing, you can have a look at this.\n",
      "Then, you can define you condition on the whole columns. Take care of the and operator that becomes &.\n",
      "Finally, you can call the where function that returns Nan when the condition isn't satisfied.\n",
      "To fit the desired output, you can filter the columns using iloc or just calling the columns name.\n",
      "\n",
      "Here the code:\n",
      "# Import module\n",
      "import pandas as pd\n",
      "\n",
      "df1 = pd.DataFrame([[\"A\", 11,  \"B\", 20],\n",
      "                    [\"A\", 11,  \"C\", 24],\n",
      "                    [\"B\", 14,  \"R\", 19]],\n",
      "                   columns=[\"Col_A\", \"Col_B\", \"Col_C\", \"Col_D\"])\n",
      "df2 = pd.DataFrame([[10, 15,  16, 21, 0.99],\n",
      "                    [10, 15,  17, 22, 0.89],\n",
      "                    [11, 15,  16, 20, 0.67]],\n",
      "                   columns=[\"Col_X\", \"Col_Y\", \"Col_P\", \"Col_Q\", \"Col_Z\"])\n",
      "\n",
      "# Concat the dataframe\n",
      "df = pd.concat([df1, df2], axis=1)\n",
      "print(df)\n",
      "\n",
      "# Define the conditions\n",
      "condition_col_b = ((df.Col_X <= df.Col_B) & (df.Col_B < df.Col_Y))\n",
      "condition_col_d = ((df.Col_P <= df.Col_D) & (df.Col_D < df.Col_Q))\n",
      "\n",
      "print(condition_col_b & condition_col_d)\n",
      "# 0     True\n",
      "# 1    False\n",
      "# 2     True\n",
      "\n",
      "# Apply the condition\n",
      "output = df.where(condition_col_b & condition_col_d)\n",
      "print(output)\n",
      "#   Col_A  Col_B Col_C  Col_D  Col_X  Col_Y  Col_P  Col_Q  Col_Z\n",
      "# 0     A   11.0     B   20.0   10.0   15.0   16.0   21.0   0.99\n",
      "# 1   NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "# 2     B   14.0     R   19.0   11.0   15.0   16.0   20.0   0.67\n",
      "\n",
      "# Filter output\n",
      "print(output[['Col_A', 'Col_C', 'Col_Z']])\n",
      "#   Col_A Col_C  Col_Z\n",
      "# 0     A     B   0.99\n",
      "# 1   NaN   NaN    NaN\n",
      "# 2     B     R   0.67\n",
      "Try pd.where().\n",
      "df3 = pd.merge(df,df2,left_index=True, right_index=True)\n",
      "df3\n",
      "\n",
      "df3 = df3.where( (df3['Col_X'] <= df3['Col_B']) & (df3['Col_B'] <= df3['Col_Y']) & (df3['Col_P'] <= df3['Col_D']) & (df3['Col_D'] <= df3['Col_Q'])  )\n",
      "\n",
      "df3.loc[:, ['Col_A', 'Col_C', 'Col_Z']]\n",
      "\n",
      "    Col_A   Col_C   Col_Z\n",
      "0   A       B       0.99\n",
      "1   NaN     NaN     nan\n",
      "2   B       R       0.67\n",
      "======================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import urllib.request as req\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "page= urlopen(\"https://stackoverflow.com/questions/tagged/python\")\n",
    "document=page.read()\n",
    "soup=BeautifulSoup(document, 'html.parser')\n",
    "questions=soup.find(id=\"questions\")\n",
    "questions_list=questions.find_all(\"a\", class_=\"question-hyperlink\")\n",
    "# questions=[]\n",
    "for questions in questions_list:\n",
    "    print(\"Question\\n\")\n",
    "    print('http://stackoverflow.com'+questions.get('href'))\n",
    "    print(questions.get_text())\n",
    "    print(\"\\n\")\n",
    "    url='https://stackoverflow.com'+questions.get('href')\n",
    "    response = req.urlopen(url)\n",
    "    soup= BeautifulSoup(response,'html.parser')\n",
    "    for question in soup.select(\"div.postcell div.post-text\"):\n",
    "        print(question.get_text().strip())\n",
    "    print(\"\\nAnswer\\n\")\n",
    "    if len(soup.select(\"div.answercell div.post-text\"))==0:\n",
    "        print('no answer')\n",
    "    else:\n",
    "        for answer in soup.select(\"div.answercell div.post-text\"):\n",
    "            print(answer.get_text().strip())\n",
    "\n",
    "    print('='*40,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "from urllib.request import urlopen\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력해주십시오-->파이썬 라이브러리 머신러닝\n",
      "검색할 페이지 번호를 입력하십시오-->1\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=4&dirId=40608&docId=327004200&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=1&search_sort=0&spq=0\n",
      "**********질문 제목\n",
      "파이썬 머신러닝 입문\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "머신러닝을 해보려고 하는데 파이썬으로 해보려고합니다. 그런데 뭐 부터 시작하고 어떻게 학습해야하는지 감이 안오네요우선, 파이썬부터 시작하는것이 맞지만 어느 문법에 비중을 둬야하는게 있나요?또 파이썬을 얼마나 깊게 공부해야하나요? 일단 점프 투 파이썬이라는 입문서를 다 봤는데이후에 어떻게 해야할지 모르겠어요 !도와주세요!\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스쿨 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요. 저는 IT전문교육기관진로·진학상담을 도와드리고 있는 교육팀장 입니다.질문해주신 내용에 도움을 주고싶어 답변남깁니다.​​파이썬을 기반으로 서버 네트워크까지 아시면 좋습니다우리나라에서 일을 계속하신다면 자바쪽도 필수입니다^^​=> 인공지능도 프로그래밍 개발 분야이기때문에 인공지능 분야로 성장하기에 있어서느Python, JAVA, JSP와 같은 언어가 기초작인 베이스가 되어있어야합니다.​웹개발이 기본적으로 프론트엔드 백앤드 뭐 크게는 이렇게 나눌수있는데요. 자바랑 스프링으로 서버만들고 게시판이나 회원가입같은 기능을 구현하는게 백엔드고. 보여지는 화면 ui를 얼마나 이쁘게 보기좋게 만드는건 html css javascript등등을 사용해서 디자인같은걸 하는걸 프론트앤드쪽 개발이라고 할수있겠네요. 웹퍼블리셔는 프론트앤드개발쪽에 가깝게 디자인이나 ui/ux등 웹을 개발할때 폼같은것을 만들거나 표준에 맞는 작업을 하는 것을 말합니다.​흔히 이 분야에서 이야기하는 개발자는 소프트웨어 개발자를 의미하는 것인데 소프트웨어 개발자는 범위가 큽니다. 기획, 설계, 구현, 테스트, 문서작성등 소프트웨어 개발에 참여하는 이해관계자 모두를 소프트웨어 개발자라 칭합니다. 프로그래머도 구현단계에 참여하는 소프트웨어 개발자 인거죠. 좁은 의미로는 소프트웨어 개발자 = 프로그래머 같은 의미가 됩니다.​황에 맞는 최적화된 공부방법과 세부적인 취업진로에 대한 컨설팅을 도와드리겠습니다. 궁금한 내용이나 자세한 상담을 바라시면 네임카드나 오픈카톡 참고하셔서 질문해 주시기를 바랍니다. (상담은 무료입니다.)​​<IT전문 교육상담>https://bit.ly/2vycHbo​​<멘토스쿨 플러스친구>https://bit.ly/2GYibD1​​<IT멘토스쿨>http://bit.ly/2ZTxJiT\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스쿨 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~​프로그래밍 언어를 공부하실 때 어떤 문법 위주로 하기보다는 문법들을 이용해서 예제나 만들어볼만한 것들을​만들면서 계속해서 프로그래밍 언어를 써보는게 중요합니다.​그리고 Python으로 머신러닝을 해볼 생각이라면 머신러닝과 관련된 라이브러리를 공부하시는게 좋은데​독학으로는 공부하기가 어렵습니다.​문법보다는 전체적으로 파이썬을 많이 사용해보고 어느정도 능숙하게 사용이 가능하신 것 같다면 그때​머신러닝 라이브러리를 찾아 공부하면 됩니다.​더 궁금하신 점이 있거나 상담을 받아보고 싶으시다면 오픈카톡으로 연락주세요~​오픈카톡 : https://open.kakao.com/o/sk5nMKab             IT상담 프로그래밍, 네트워크, 시스템, 정보보안 문의 open.kakao.com           ​\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스쿨 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "기업들은 빅데이터 플랫폼을 이용하여 데이터를 수집, 저장, 처리, 관리등을 할수있습니다.​빅데이터 플랫폼?빅데이터 기술의 집합체이며 기술을 잘 사용할수있도록 준비된 환경​빅데이터를 간략하게 설명드리면우선 개발과분석으로 나뉘며​개발은 분석파트에서 분석할수있는 환경을 만들어주며필요한 공부는 빅데이터의 대표적인 플랫폼 하둡수많은 데이터들을 서버로 분할 처리하기위한 리눅스하둡이 자바기반 오픈소스 프레임워크로 자바에 대해 공부하셔야합니다.​분석은 데이터 처리-> 데이터 시각화필요한 공부는데이터 자료형 함수를 포함하고 있어 가장 활발하게 쓰이는 파이썬처리된 데이터를 시각화 시켜줄 통계 R프로그래밍을 공부하셔야합니다.​프로그램을 제작하기 위해선 빅데이터 기술외에도 프로그램 제작을 위한프로그래밍을 하실 수 있으셔야 합니다.​​컴공 소프트웨어 등 컴퓨터관련 모든 학과 그리고 상경계열 파트의​경영 경제 통계 회계 등 다 도전이 가능합니다.​최근 경영 경제 통계 어문 등 인문계열이나 상경계열 취업난이 가중되고​있기 때문에 기술에 대한 부분의 융합이 가장 이슈로 떠오르고 있습니다.​그 중 가장 이슈가 되고 있는 파트가 빅데이터 전문분야입니다.​가능은 합니다 하지만 그만큼에 공부해야할 부분도 만만치 않습니다.​기본적인 웹사이트와 DB구축을 할 줄 아셔야합니다.​그러기 위해서는 컴퓨터의 구조와 프로그래밍에 대한 코딩력 그리고​운영체제에 대한 서비스능력 네트워크에 설정 능력부터준비하셔야합니다.​​산업이 점점 발전하면서 우리가 인지하고 있는 데이터들의 양이 방대해졌습니다.​그렇기 때문에 이러한 방대한 양의 데이터를 수집할수 있는 전문인력이​각광을 받는 것이곡 이러한 데이터의 수집과 분석을 통해 할 수 있는 것들이​많아지게 되었습니다.​그렇기 때문에 어느 어느 회사가 아니라 현재 진행되는 모든 산업의​필수가 되고 어느곳이나 취업이 가능합니다.​​실시간 쏟아지는 빅데이터를 어떻게, 어디에 활용할 것인지 기획하는 일부터 시작합니다.​예를 들어 모바일 쇼핑몰을 운영한다면 요즘 젊은이들 또는 중년층이 즐겨 찾는 키워드는​무엇이고, 어느 사이트에서 얼마나 머물며, 실제 구매하는 데는 가격과 상품 평가 중 어떤​요인이 영향을 미치는지​사전에 분석해 보는 분석전문가를 뜻합니다.​​일반적인 교육기관에서는 배우기 힘들고​전문적인 대규모교육기관을 이용해야합니다.​​구지 그 자격증이 없더라도 충분히 가능하고​하둡, R프로그래밍에 대한 실무력을 갖추어야합니다.​그러기 위해서는 그에 따른 사이트 구축에 대한 개발능력과 분석능력이​필요하기 때문에 프로그래밍과 운영체제에 대한 실력이 필수입니다.​​​둘다 병행하셔야 하고 전공자들보다 공부량도 더 많으셔야합니다.​독학은 현실적으로 불가능합니다.​독학으로 하면 왜 안되냐구요? 설명드리겠습니다.​관련 서적 무진장 많습니다. 요새는 책들이 아주 잘 나옵니다.​뭐 아무거나 본인입맛에 맞는 거 사면됩니다. ​책보다가 모르는 부분이 1개 2개 책 장 넘길수록 5개 10개 이런식으로​늘어날 것입니다.​이에 대한 피드백을 어떻게 받으실 예정이십니까?​아마 대충 넘어가다가 그냥 혹은 암기만 하다가 걷잡을 수 없는 상황이 되면​스스로 한달못가 포기하게 되는 경우가 허다합니다.​주의에 피드백을 받을 곳도 없고 왜 이걸해야하는지 어떻게 활용되는지 모르고​하다보면 대충 넘어가게 되겠죠? 그러다가​나 안해... 그러고 포기입니다.​​지금 가장 필요한건 상황에 맞는 최적화된 공부방법과​세부적인 진로에 대한 컨설팅이 반드시 필요합니다.​문의 주시면 자세한 1:1 전문상담 도와드리도록 하겠습니다.​https://open.kakao.com/o/swZ0WVjb\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "IT곰 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요. 저는 IT전문교육기관진로·진학상담을 도와드리고 있는 교육팀장 입니다.질문해주신 내용에 도움을 주고싶어 답변남깁니다.​​파이썬을 기반으로 서버 네트워크까지 아시면 좋습니다우리나라에서 일을 계속하신다면 자바쪽도 필수입니다^^​=> 인공지능도 프로그래밍 개발 분야이기때문에 인공지능 분야로 성장하기에 있어서느Python, JAVA, JSP와 같은 언어가 기초작인 베이스가 되어있어야합니다.​웹개발이 기본적으로 프론트엔드 백앤드 뭐 크게는 이렇게 나눌수있는데요. 자바랑 스프링으로 서버만들고 게시판이나 회원가입같은 기능을 구현하는게 백엔드고. 보여지는 화면 ui를 얼마나 이쁘게 보기좋게 만드는건 html css javascript등등을 사용해서 디자인같은걸 하는걸 프론트앤드쪽 개발이라고 할수있겠네요. 웹퍼블리셔는 프론트앤드개발쪽에 가깝게 디자인이나 ui/ux등 웹을 개발할때 폼같은것을 만들거나 표준에 맞는 작업을 하는 것을 말합니다.​흔히 이 분야에서 이야기하는 개발자는 소프트웨어 개발자를 의미하는 것인데 소프트웨어 개발자는 범위가 큽니다. 기획, 설계, 구현, 테스트, 문서작성등 소프트웨어 개발에 참여하는 이해관계자 모두를 소프트웨어 개발자라 칭합니다. 프로그래머도 구현단계에 참여하는 소프트웨어 개발자 인거죠. 좁은 의미로는 소프트웨어 개발자 = 프로그래머 같은 의미가 됩니다.​황에 맞는 최적화된 공부방법과 세부적인 취업진로에 대한 컨설팅을 도와드리겠습니다. 궁금한 내용이나 자세한 상담을 바라시면 네임카드나 오픈카톡 참고하셔서 질문해 주시기를 바랍니다. (상담은 무료입니다.)​​<IT전문 교육상담>https://bit.ly/2vycHbo​​<멘토스쿨 플러스친구>https://bit.ly/2GYibD1​​<IT멘토스쿨>http://bit.ly/2ZTxJiT\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "IT곰 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~​프로그래밍 언어를 공부하실 때 어떤 문법 위주로 하기보다는 문법들을 이용해서 예제나 만들어볼만한 것들을​만들면서 계속해서 프로그래밍 언어를 써보는게 중요합니다.​그리고 Python으로 머신러닝을 해볼 생각이라면 머신러닝과 관련된 라이브러리를 공부하시는게 좋은데​독학으로는 공부하기가 어렵습니다.​문법보다는 전체적으로 파이썬을 많이 사용해보고 어느정도 능숙하게 사용이 가능하신 것 같다면 그때​머신러닝 라이브러리를 찾아 공부하면 됩니다.​더 궁금하신 점이 있거나 상담을 받아보고 싶으시다면 오픈카톡으로 연락주세요~​오픈카톡 : https://open.kakao.com/o/sk5nMKab             IT상담 프로그래밍, 네트워크, 시스템, 정보보안 문의 open.kakao.com           ​\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "IT곰 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "기업들은 빅데이터 플랫폼을 이용하여 데이터를 수집, 저장, 처리, 관리등을 할수있습니다.​빅데이터 플랫폼?빅데이터 기술의 집합체이며 기술을 잘 사용할수있도록 준비된 환경​빅데이터를 간략하게 설명드리면우선 개발과분석으로 나뉘며​개발은 분석파트에서 분석할수있는 환경을 만들어주며필요한 공부는 빅데이터의 대표적인 플랫폼 하둡수많은 데이터들을 서버로 분할 처리하기위한 리눅스하둡이 자바기반 오픈소스 프레임워크로 자바에 대해 공부하셔야합니다.​분석은 데이터 처리-> 데이터 시각화필요한 공부는데이터 자료형 함수를 포함하고 있어 가장 활발하게 쓰이는 파이썬처리된 데이터를 시각화 시켜줄 통계 R프로그래밍을 공부하셔야합니다.​프로그램을 제작하기 위해선 빅데이터 기술외에도 프로그램 제작을 위한프로그래밍을 하실 수 있으셔야 합니다.​​컴공 소프트웨어 등 컴퓨터관련 모든 학과 그리고 상경계열 파트의​경영 경제 통계 회계 등 다 도전이 가능합니다.​최근 경영 경제 통계 어문 등 인문계열이나 상경계열 취업난이 가중되고​있기 때문에 기술에 대한 부분의 융합이 가장 이슈로 떠오르고 있습니다.​그 중 가장 이슈가 되고 있는 파트가 빅데이터 전문분야입니다.​가능은 합니다 하지만 그만큼에 공부해야할 부분도 만만치 않습니다.​기본적인 웹사이트와 DB구축을 할 줄 아셔야합니다.​그러기 위해서는 컴퓨터의 구조와 프로그래밍에 대한 코딩력 그리고​운영체제에 대한 서비스능력 네트워크에 설정 능력부터준비하셔야합니다.​​산업이 점점 발전하면서 우리가 인지하고 있는 데이터들의 양이 방대해졌습니다.​그렇기 때문에 이러한 방대한 양의 데이터를 수집할수 있는 전문인력이​각광을 받는 것이곡 이러한 데이터의 수집과 분석을 통해 할 수 있는 것들이​많아지게 되었습니다.​그렇기 때문에 어느 어느 회사가 아니라 현재 진행되는 모든 산업의​필수가 되고 어느곳이나 취업이 가능합니다.​​실시간 쏟아지는 빅데이터를 어떻게, 어디에 활용할 것인지 기획하는 일부터 시작합니다.​예를 들어 모바일 쇼핑몰을 운영한다면 요즘 젊은이들 또는 중년층이 즐겨 찾는 키워드는​무엇이고, 어느 사이트에서 얼마나 머물며, 실제 구매하는 데는 가격과 상품 평가 중 어떤​요인이 영향을 미치는지​사전에 분석해 보는 분석전문가를 뜻합니다.​​일반적인 교육기관에서는 배우기 힘들고​전문적인 대규모교육기관을 이용해야합니다.​​구지 그 자격증이 없더라도 충분히 가능하고​하둡, R프로그래밍에 대한 실무력을 갖추어야합니다.​그러기 위해서는 그에 따른 사이트 구축에 대한 개발능력과 분석능력이​필요하기 때문에 프로그래밍과 운영체제에 대한 실력이 필수입니다.​​​둘다 병행하셔야 하고 전공자들보다 공부량도 더 많으셔야합니다.​독학은 현실적으로 불가능합니다.​독학으로 하면 왜 안되냐구요? 설명드리겠습니다.​관련 서적 무진장 많습니다. 요새는 책들이 아주 잘 나옵니다.​뭐 아무거나 본인입맛에 맞는 거 사면됩니다. ​책보다가 모르는 부분이 1개 2개 책 장 넘길수록 5개 10개 이런식으로​늘어날 것입니다.​이에 대한 피드백을 어떻게 받으실 예정이십니까?​아마 대충 넘어가다가 그냥 혹은 암기만 하다가 걷잡을 수 없는 상황이 되면​스스로 한달못가 포기하게 되는 경우가 허다합니다.​주의에 피드백을 받을 곳도 없고 왜 이걸해야하는지 어떻게 활용되는지 모르고​하다보면 대충 넘어가게 되겠죠? 그러다가​나 안해... 그러고 포기입니다.​​지금 가장 필요한건 상황에 맞는 최적화된 공부방법과​세부적인 진로에 대한 컨설팅이 반드시 필요합니다.​문의 주시면 자세한 1:1 전문상담 도와드리도록 하겠습니다.​https://open.kakao.com/o/swZ0WVjb\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "오성쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요. 저는 IT전문교육기관진로·진학상담을 도와드리고 있는 교육팀장 입니다.질문해주신 내용에 도움을 주고싶어 답변남깁니다.​​파이썬을 기반으로 서버 네트워크까지 아시면 좋습니다우리나라에서 일을 계속하신다면 자바쪽도 필수입니다^^​=> 인공지능도 프로그래밍 개발 분야이기때문에 인공지능 분야로 성장하기에 있어서느Python, JAVA, JSP와 같은 언어가 기초작인 베이스가 되어있어야합니다.​웹개발이 기본적으로 프론트엔드 백앤드 뭐 크게는 이렇게 나눌수있는데요. 자바랑 스프링으로 서버만들고 게시판이나 회원가입같은 기능을 구현하는게 백엔드고. 보여지는 화면 ui를 얼마나 이쁘게 보기좋게 만드는건 html css javascript등등을 사용해서 디자인같은걸 하는걸 프론트앤드쪽 개발이라고 할수있겠네요. 웹퍼블리셔는 프론트앤드개발쪽에 가깝게 디자인이나 ui/ux등 웹을 개발할때 폼같은것을 만들거나 표준에 맞는 작업을 하는 것을 말합니다.​흔히 이 분야에서 이야기하는 개발자는 소프트웨어 개발자를 의미하는 것인데 소프트웨어 개발자는 범위가 큽니다. 기획, 설계, 구현, 테스트, 문서작성등 소프트웨어 개발에 참여하는 이해관계자 모두를 소프트웨어 개발자라 칭합니다. 프로그래머도 구현단계에 참여하는 소프트웨어 개발자 인거죠. 좁은 의미로는 소프트웨어 개발자 = 프로그래머 같은 의미가 됩니다.​황에 맞는 최적화된 공부방법과 세부적인 취업진로에 대한 컨설팅을 도와드리겠습니다. 궁금한 내용이나 자세한 상담을 바라시면 네임카드나 오픈카톡 참고하셔서 질문해 주시기를 바랍니다. (상담은 무료입니다.)​​<IT전문 교육상담>https://bit.ly/2vycHbo​​<멘토스쿨 플러스친구>https://bit.ly/2GYibD1​​<IT멘토스쿨>http://bit.ly/2ZTxJiT\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "오성쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~​프로그래밍 언어를 공부하실 때 어떤 문법 위주로 하기보다는 문법들을 이용해서 예제나 만들어볼만한 것들을​만들면서 계속해서 프로그래밍 언어를 써보는게 중요합니다.​그리고 Python으로 머신러닝을 해볼 생각이라면 머신러닝과 관련된 라이브러리를 공부하시는게 좋은데​독학으로는 공부하기가 어렵습니다.​문법보다는 전체적으로 파이썬을 많이 사용해보고 어느정도 능숙하게 사용이 가능하신 것 같다면 그때​머신러닝 라이브러리를 찾아 공부하면 됩니다.​더 궁금하신 점이 있거나 상담을 받아보고 싶으시다면 오픈카톡으로 연락주세요~​오픈카톡 : https://open.kakao.com/o/sk5nMKab             IT상담 프로그래밍, 네트워크, 시스템, 정보보안 문의 open.kakao.com           ​\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "오성쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "기업들은 빅데이터 플랫폼을 이용하여 데이터를 수집, 저장, 처리, 관리등을 할수있습니다.​빅데이터 플랫폼?빅데이터 기술의 집합체이며 기술을 잘 사용할수있도록 준비된 환경​빅데이터를 간략하게 설명드리면우선 개발과분석으로 나뉘며​개발은 분석파트에서 분석할수있는 환경을 만들어주며필요한 공부는 빅데이터의 대표적인 플랫폼 하둡수많은 데이터들을 서버로 분할 처리하기위한 리눅스하둡이 자바기반 오픈소스 프레임워크로 자바에 대해 공부하셔야합니다.​분석은 데이터 처리-> 데이터 시각화필요한 공부는데이터 자료형 함수를 포함하고 있어 가장 활발하게 쓰이는 파이썬처리된 데이터를 시각화 시켜줄 통계 R프로그래밍을 공부하셔야합니다.​프로그램을 제작하기 위해선 빅데이터 기술외에도 프로그램 제작을 위한프로그래밍을 하실 수 있으셔야 합니다.​​컴공 소프트웨어 등 컴퓨터관련 모든 학과 그리고 상경계열 파트의​경영 경제 통계 회계 등 다 도전이 가능합니다.​최근 경영 경제 통계 어문 등 인문계열이나 상경계열 취업난이 가중되고​있기 때문에 기술에 대한 부분의 융합이 가장 이슈로 떠오르고 있습니다.​그 중 가장 이슈가 되고 있는 파트가 빅데이터 전문분야입니다.​가능은 합니다 하지만 그만큼에 공부해야할 부분도 만만치 않습니다.​기본적인 웹사이트와 DB구축을 할 줄 아셔야합니다.​그러기 위해서는 컴퓨터의 구조와 프로그래밍에 대한 코딩력 그리고​운영체제에 대한 서비스능력 네트워크에 설정 능력부터준비하셔야합니다.​​산업이 점점 발전하면서 우리가 인지하고 있는 데이터들의 양이 방대해졌습니다.​그렇기 때문에 이러한 방대한 양의 데이터를 수집할수 있는 전문인력이​각광을 받는 것이곡 이러한 데이터의 수집과 분석을 통해 할 수 있는 것들이​많아지게 되었습니다.​그렇기 때문에 어느 어느 회사가 아니라 현재 진행되는 모든 산업의​필수가 되고 어느곳이나 취업이 가능합니다.​​실시간 쏟아지는 빅데이터를 어떻게, 어디에 활용할 것인지 기획하는 일부터 시작합니다.​예를 들어 모바일 쇼핑몰을 운영한다면 요즘 젊은이들 또는 중년층이 즐겨 찾는 키워드는​무엇이고, 어느 사이트에서 얼마나 머물며, 실제 구매하는 데는 가격과 상품 평가 중 어떤​요인이 영향을 미치는지​사전에 분석해 보는 분석전문가를 뜻합니다.​​일반적인 교육기관에서는 배우기 힘들고​전문적인 대규모교육기관을 이용해야합니다.​​구지 그 자격증이 없더라도 충분히 가능하고​하둡, R프로그래밍에 대한 실무력을 갖추어야합니다.​그러기 위해서는 그에 따른 사이트 구축에 대한 개발능력과 분석능력이​필요하기 때문에 프로그래밍과 운영체제에 대한 실력이 필수입니다.​​​둘다 병행하셔야 하고 전공자들보다 공부량도 더 많으셔야합니다.​독학은 현실적으로 불가능합니다.​독학으로 하면 왜 안되냐구요? 설명드리겠습니다.​관련 서적 무진장 많습니다. 요새는 책들이 아주 잘 나옵니다.​뭐 아무거나 본인입맛에 맞는 거 사면됩니다. ​책보다가 모르는 부분이 1개 2개 책 장 넘길수록 5개 10개 이런식으로​늘어날 것입니다.​이에 대한 피드백을 어떻게 받으실 예정이십니까?​아마 대충 넘어가다가 그냥 혹은 암기만 하다가 걷잡을 수 없는 상황이 되면​스스로 한달못가 포기하게 되는 경우가 허다합니다.​주의에 피드백을 받을 곳도 없고 왜 이걸해야하는지 어떻게 활용되는지 모르고​하다보면 대충 넘어가게 되겠죠? 그러다가​나 안해... 그러고 포기입니다.​​지금 가장 필요한건 상황에 맞는 최적화된 공부방법과​세부적인 진로에 대한 컨설팅이 반드시 필요합니다.​문의 주시면 자세한 1:1 전문상담 도와드리도록 하겠습니다.​https://open.kakao.com/o/swZ0WVjb\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=1&dirId=10402&docId=319408230&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=2&search_sort=0&spq=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********질문 제목\n",
      "머신러닝에 파이썬을사용하는 이유가 무엇인가요?그리고 c를이용해서 머신러닝이 가능할까요?\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "kate**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "속도가 중요하다면 C++ 을 이용해서 짤수도 있습니다. 실제로 속도가 중요한 분야에서는 대부분 Go 나 C++, Java 같은 속도가 빠른 언어를 이용하여 머신러닝 알고리즘을 구현합니다.파이썬을 많이 사용하는 이유는 파이썬은 위 언어들보다 쉽습니다. 위 언어들은 컴파일러 언어에 정적 타입을 지원하지만 파이썬은 스크립트 언어에 동적 타입을 지원합니다. 개발 진행시의 이와같은 특징은 장점이 될수도 단점이 될수도 있으나 개발을 처음 시작하는 초보자 입장에서는 파이썬이 훨씬 유리합니다.그리고 파이썬은 수많은 머신러닝 라이브러리, 최근에는 딥러닝 라이브러리(tensorflow, keras 등)를 제공합니다.파이썬은 범용 언어이기 때문에 응용어플리케이션 개발에 쓰이며 그때 파이썬에 있는 수많은 라이브러리를 그대로 활용할 수 있게 해주기에 큰 장점이 됩니다.파이썬에 많은 라이브러리가 개발된 이유에는 여러가지 배경이 있지만 미국 실리콘밸리를 중심으로 하는 프로그래밍 언어 시장이 파이썬이 많이 쓰이게 되면서 기업 뿐만 아니라 학계에서도 파이썬을 사용하게 되어 지금은 보편적으로 프로그래밍 입문시 파이썬을 많이 배우고 있습니다. (예를들어, 구글 검색 엔진은 처음에 파이썬으로 코딩되었습니다.)R 도 훌륭한 언어이나 통계용으로 용도가 제한되어있어 범용성이 떨어집니다.(성능도 보통 떨어져 거대한 데이터를 다룰때 불리합니다.) 대신, 통계분석용으로 설계된 언어이기 때문에 통계분석 자체만으로는 파이썬보다 훨씬 더 좋은 편리성을 제공합니다. C 만 가지고는 개발의 한계가 있기 때문에 C++ 을 이용한 머신러닝 알고리즘 구현을 할 수 있으며 유튜브에 홍정모 교수님의 C++ 머신러닝 수업을 찾아보시길 바랍니다.또한, 머신러닝은 컴퓨터 언어보다 알고리즘의 근간이 되는 수학이 중요한 학문이니 선형대수학 등에 대해서 궁금하시다면 스터디 참고 : http://bit.ly/2AC04z2저희 스터디에 구경와보시길 바랍니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "kate**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "가능하기야 하지만 파이썬이나 R언어가 가장 많이 쓰이는 언어다보니 주로 파이썬으로만 만들어져있고 C언어로된 머신러닝 라이브러리가 없다면 바닥부터 다시 직접 만들어줘야합니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "속도가 중요하다면 C++ 을 이용해서 짤수도 있습니다. 실제로 속도가 중요한 분야에서는 대부분 Go 나 C++, Java 같은 속도가 빠른 언어를 이용하여 머신러닝 알고리즘을 구현합니다.파이썬을 많이 사용하는 이유는 파이썬은 위 언어들보다 쉽습니다. 위 언어들은 컴파일러 언어에 정적 타입을 지원하지만 파이썬은 스크립트 언어에 동적 타입을 지원합니다. 개발 진행시의 이와같은 특징은 장점이 될수도 단점이 될수도 있으나 개발을 처음 시작하는 초보자 입장에서는 파이썬이 훨씬 유리합니다.그리고 파이썬은 수많은 머신러닝 라이브러리, 최근에는 딥러닝 라이브러리(tensorflow, keras 등)를 제공합니다.파이썬은 범용 언어이기 때문에 응용어플리케이션 개발에 쓰이며 그때 파이썬에 있는 수많은 라이브러리를 그대로 활용할 수 있게 해주기에 큰 장점이 됩니다.파이썬에 많은 라이브러리가 개발된 이유에는 여러가지 배경이 있지만 미국 실리콘밸리를 중심으로 하는 프로그래밍 언어 시장이 파이썬이 많이 쓰이게 되면서 기업 뿐만 아니라 학계에서도 파이썬을 사용하게 되어 지금은 보편적으로 프로그래밍 입문시 파이썬을 많이 배우고 있습니다. (예를들어, 구글 검색 엔진은 처음에 파이썬으로 코딩되었습니다.)R 도 훌륭한 언어이나 통계용으로 용도가 제한되어있어 범용성이 떨어집니다.(성능도 보통 떨어져 거대한 데이터를 다룰때 불리합니다.) 대신, 통계분석용으로 설계된 언어이기 때문에 통계분석 자체만으로는 파이썬보다 훨씬 더 좋은 편리성을 제공합니다. C 만 가지고는 개발의 한계가 있기 때문에 C++ 을 이용한 머신러닝 알고리즘 구현을 할 수 있으며 유튜브에 홍정모 교수님의 C++ 머신러닝 수업을 찾아보시길 바랍니다.또한, 머신러닝은 컴퓨터 언어보다 알고리즘의 근간이 되는 수학이 중요한 학문이니 선형대수학 등에 대해서 궁금하시다면 스터디 참고 : http://bit.ly/2AC04z2저희 스터디에 구경와보시길 바랍니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "가능하기야 하지만 파이썬이나 R언어가 가장 많이 쓰이는 언어다보니 주로 파이썬으로만 만들어져있고 C언어로된 머신러닝 라이브러리가 없다면 바닥부터 다시 직접 만들어줘야합니다.\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=1&dirId=104&docId=314878050&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=3&search_sort=0&spq=0\n",
      "**********질문 제목\n",
      "파이썬 외부 라이브러리 종류\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "파이썬의 외부 라이브러리에는 뭐가 있을까요? 라이브러리 이름과 기능을 간단하게 알려주세요.\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "spac**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "파이썬은 외부 라이브러리가 굉장히 많습니다. 사실상 그 확장성 때문에 사용하는거라 보면 되기 때문에 일일이 나열하기가 힘듭니다. 많이 쓰는 것부터 들면, 1. NumPy - 고성능의 다차원 배열을 제공하고 과학적 계산을 위한 라이브러리 입니다.2. Pandas - NumPy와 비슷하며 좀 더 데이터를 다루기 쉽습니다.3. SciPy - NumPy 기반의 라이브러리로 좀 더 확장적인 기능과 함수를 제공합니다.4. Matplotlib - 플롯을 만들거나 그래프를 만드는 등, 시각화에 많이 이용됩니다.5. PyGame - 게임과 같은 멅티미디어 개발 라이브러리입니다.6. pillow - 이미지 처리에 많이 이용됩니다.7. Django - 위에서 언급하신 Flask와 비슷한 라이브러리이나 가장 많이 사용하는 웹 프레임워크입니다. Flask, Pyramid, Bottle 중에 선택하시는게 좋습니다.머신 러닝 라이브러리 역시 파이썬이 가장 많이 쓰이는데, 기본적으로는 CUDA를 C++를 통해 지원합니다.1. TensorFlow - 구글에서 제공하는 라이브러리 입니다. 최대 규모의 커뮤니티를 자랑합니다.2. PyTorch - 텐서플로우 다음으로 많이 사용하는 것으로 생각되며 논문을 위한 실험 등에서 편하게 사용합니다.그외, Keras, Caffe, XGBoost, scikit-learn 등이 있습니다.대부분의 라이브러리, API는 Conda 혹은 pip로 쉽게 설치할 수 있습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "spac**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "mcpi - 마인크래프트 파이 에디션 연동 API입니다.numpy - 수학 계산을 도와주는 모듈입니다.discord - 정식 명칭은 discord.py이며, 디스코드 봇 API 래퍼입니다.flask - 파이썬 웹 프레임워크입니다.그 외에 pypi에서 다른 모듈을 검색 할 수도 있습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "프레타 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "파이썬은 외부 라이브러리가 굉장히 많습니다. 사실상 그 확장성 때문에 사용하는거라 보면 되기 때문에 일일이 나열하기가 힘듭니다. 많이 쓰는 것부터 들면, 1. NumPy - 고성능의 다차원 배열을 제공하고 과학적 계산을 위한 라이브러리 입니다.2. Pandas - NumPy와 비슷하며 좀 더 데이터를 다루기 쉽습니다.3. SciPy - NumPy 기반의 라이브러리로 좀 더 확장적인 기능과 함수를 제공합니다.4. Matplotlib - 플롯을 만들거나 그래프를 만드는 등, 시각화에 많이 이용됩니다.5. PyGame - 게임과 같은 멅티미디어 개발 라이브러리입니다.6. pillow - 이미지 처리에 많이 이용됩니다.7. Django - 위에서 언급하신 Flask와 비슷한 라이브러리이나 가장 많이 사용하는 웹 프레임워크입니다. Flask, Pyramid, Bottle 중에 선택하시는게 좋습니다.머신 러닝 라이브러리 역시 파이썬이 가장 많이 쓰이는데, 기본적으로는 CUDA를 C++를 통해 지원합니다.1. TensorFlow - 구글에서 제공하는 라이브러리 입니다. 최대 규모의 커뮤니티를 자랑합니다.2. PyTorch - 텐서플로우 다음으로 많이 사용하는 것으로 생각되며 논문을 위한 실험 등에서 편하게 사용합니다.그외, Keras, Caffe, XGBoost, scikit-learn 등이 있습니다.대부분의 라이브러리, API는 Conda 혹은 pip로 쉽게 설치할 수 있습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "프레타 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "mcpi - 마인크래프트 파이 에디션 연동 API입니다.numpy - 수학 계산을 도와주는 모듈입니다.discord - 정식 명칭은 discord.py이며, 디스코드 봇 API 래퍼입니다.flask - 파이썬 웹 프레임워크입니다.그 외에 pypi에서 다른 모듈을 검색 할 수도 있습니다.\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=1&dirId=10405&docId=297115037&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=4&search_sort=0&spq=0\n",
      "**********질문 제목\n",
      "딥러닝은 왜 파이썬으로 하나요?\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "딥러닝을 이제 입문하려고 합니다.기본적인 C 문법과 JAVA 문법은 숙지하고 있는 상태 입니다.제가 생각하기에 본디 프로그램은 많은 라이브러리와 함수를 알고 있고 또 알아보는 능력이 중요하다고 생각합니다.각설하고, 딥러닝을 파이썬으로 많이들 하시는데 그러한 이유가 무엇인지 궁금합니다.파이썬에 대하여 잘 모르는 제가 보아도, 파이썬은 C 나 JAVA 에 비하여 굉장히 편리하고 직관적으로 보이는데요. 왜 사람들은 딥러닝 할 때 파이썬을 사용하는 것 인가요?\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "IT정보고민해결사 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "말씀 하신것 처럼 편리하기때문이며요즘 모든 것들이 파이썬으로 많이 대체가 되고 있는 상태기 때문에파이썬으로 많이 쓰기 마련이죠딥러닝 머신러닝 인공지능 이런 부분 결국 알고리즘이라는 부분들을 파이썬으로 많이 바꿔가고 있기 때문에 파이썬을 좀 배우셔야할 필요 성이 있습니다.\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=4&dirId=40608&docId=330644805&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=5&search_sort=0&spq=0\n",
      "**********질문 제목\n",
      "전공대비 파이썬학원 추천\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "올해 컴공과 입학했는데 수업을 못따라가서 학원가서 전공 대비로 공부하려고 합니다.파이썬학원 추천 좀 부탁드립니다.\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요! IT 교육기관 컨설턴트 멘토스쌤이에요~​질문해주신 내용을 바탕으로 답변드릴게요!​IT 직업군은 4차 산업혁명으로 정보화시대에 미래 유망직종이 되었어요~​현재 정부의 많은 지원을 받고 있으며, 일자리의 수요가 높은 직업군으로서 앞으로의 전망이 밝은 편이에요.​IT 관련 취업에 있어서 현실적으로 가장 필요한 부분은 실무중심 역량이에요.​IT 비전공자 취업 준비생분들은 IT 전문교육기관을 통해 학습하면서 자격증, 포트폴리오뿐만 아니라 취업 이후 반드시 필요한 실무기술까지 완벽하게 갖춘 상태로 IT 취업시장에 도전하게 돼요.​실무중심으로 전문교육을 받은 비전공자는 취업시장에서 기술이 부족한 전공자보다 월등한 실력과 기술력으로 차별화되는 경쟁력을 보유한 상태로 취업시장에 도전하고 있는 상황이에요.​자격증은 국가, 국제 자격증을 위주로 취득하시는 게 좋고, IT 분야로 취직을 원하신다면 국제 자격증은 필수에요.​실무 중심의 교육으로 자격증 획득률, 취업률이 높은 IT 전문교육기관에서 체계적으로 배워보시는 것은 어떠세요?​----------------------------------------------------------------------------------------------------------​#지역별 전문교육기관 운영(서울 종로 본점, 강남점, 구로점, 대구점, 부산점)​교육기관 자체 강의 촬영한 #인터넷강의 사이트 운영​#체계화 된 IT기술 커리큘럼으로 효율적인 실무 위주의 교육 진행​직업능력의 달 직업능력개발 유공 #국무총리표창 수상​#정보통신분야 취업률 1위 선정기관​#고용노동부 훈련이수자 평가 최고등급 획득기관​#국비지원교육 취업반 훈련생 취업률 90%이상​#아마존 AWS클라우드 미취업자 국내유일 지정교육센터​#마이크로소프트 골드파트너쉽 체결 기관(모든 PC 정품 SW 사용)​#국제공인자격증 시험센터 운영(OCP, OCJP, CCNA, MCITP, LPIC 등)​#정보통신부 단체 검정장 운영(리눅스마스터, 네트워크관리사, PC정비사 등)​#(주)바쿠츄와 일본IT취업 교육센터 운영(취업을 위한 일본어 교육, 일본 면접 실무진과의 1:1 모의 면접, 일본 정규직 취업연계 가능)​----------------------------------------------------------------------------------------------------------​             '컴퓨터공학과' 진학부터 취업까지 철저하게 준비하자! 시대의 흐름이 변화할수록 현재가장 인기를 끌고 있는 학과들은컴퓨터공학과/IT 융합/정보 보안등의 IT... blog.naver.com           ​♥​ Python ♥​​프로그래밍 언어는 크게 '절차 지향', '객체 지향', '함수형 패러다임' 언어로 나누어지는데 가장 유명한 절차 지향 언어가 C언어, 객체 지향은 자바 등이 있어요. 파이썬은 이러한 언어 모두 구현이 가능하답니다.​따라서, C언어나 C++로 만든 프로그램에서도 파이썬은 사용이 가능하기에 '접착 언어'라고 부르는 경우도 많아요.​빠른 속도와 뛰어난 확장성으로 다양한 분야에 활용할 수 있으며 간결한 문법으로 입문자가 이해하기 쉽고 영어와 문법 및 사고가 비슷하여 마치 영어를 작문하는 과정처럼 느낄 수 있는 프로그래밍 언어에요.​파이썬은 초보자부터 전문가까지 누구든 쉽게 접근할 수 있는 프로그래밍 언어로 다양한 플랫폼에서 활용이 가능하며 다양한 라이브러리 지원으로 머신러닝, 그래픽, 웹 개발 등 IT 전분야에서 꾸준히 성장하고 있어요.​파이썬은 다양한 기능을 갖고 있기에 초기에는 파이썬의 문법이나 제어문 등에 대해 공부를 하게 돼요.심화로 넘어갈수록 웹 서버를 구축하거나 빅데이터에 관심이 있는 분들이라면 머신 러닝을 활용하여 빅데이터 프로젝트를 진행해보는 등에 대한 심화 교육이 필요하게 된답니다.​파이썬 기초부터 심화까지 교육을 할 수 있는 교육기관에서 단기간에 배우는 것이 중요해요.​개인별로 취업희망 분야와 처음 시작하는 상황이 다르기 때문에 똑같은 방법, 똑같은 기간 동안 준비하는 것은 맞지 않아요.​가장 효율적이고 시간 낭비를 하지 않는 취업 준비 방법은 본인에게 알맞은 취업 계획을 세우시고 현실적으로 준비를 해야 해요.​IT 분야로 체계적인 학습 패턴과 전문적인 커리큘럼을 통해 취업이나 진학을 원하거나 자기개발을 준비하고 싶다면 네임카드 보고 부담갖지말고 편하게 문의하세요~​지식IN을 통해 문의주시는 분들에 한해서 1:1 맞춤 컨설팅을 무료로 진행해드리고 있답니다! : 0\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=4&dirId=40608&docId=330980026&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=6&search_sort=0&spq=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********질문 제목\n",
      "부산 파이썬 학원 추천 부탁드립니다.올해 컴공과 입학한 1학년 학생인데요..고등학교때까지 배운 내용과 차원이 다른것같아 전공 수업에 따라가기 너무 힘드네요 ㅠ파이썬 배울수있는 학원 있으면 알려주세요..\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요! IT 교육기관 컨설턴트 멘토스쌤이에요~​질문해주신 내용을 바탕으로 답변드릴게요!​IT 직업군은 4차 산업혁명으로 정보화시대에 미래 유망직종이 되었어요~​현재 정부의 많은 지원을 받고 있으며, 일자리의 수요가 높은 직업군으로서 앞으로의 전망이 밝은 편이에요.​♥ Python ♥​프로그래밍 언어는 크게 '절차 지향', '객체 지향', '함수형 패러다임' 언어로 나누어지는데 가장 유명한 절차 지향 언어가 C언어, 객체 지향은 JAVA 등이 있어요. Python은 이러한 언어 모두 구현이 가능하답니다.​초보자부터 전문가까지 누구든 쉽게 접근할 수 있는 프로그래밍 언어로 다양한 플랫폼에서 활용이 가능하며 다양한 라이브러리 지원으로 머신러닝, 그래픽, 웹 개발 등 IT 전분야에서 꾸준히 성장하고 있어요.​영어와 문법과 사고가 비슷하여 마치 영어를 작문하는 과정처럼 느낄 수 있는 프로그래밍 언어에요.​다양한 기능을 갖고 있기에 초기에는 문법이나 제어문 등에 대해 공부를 하게 돼요.​심화로 넘어갈수록 웹 서버를 구축하거나 빅데이터에 관심이 있는 분들이라면 머신러닝을 활용하여 빅데이터 프로젝트를 진행해보는 등에 대한 심화교육이 필요하게 된답니다.​기초부터 심화까지 교육을 받을 수 있는 교육 기관에서 단기간에 배우는 게 중요해요.​KG 아이티뱅크 IT 전문교육기관에서는 Python 기초는 1달 과정으로 끝나고 웹서버 구축도 1개월 과정으로 배울 수 있어요.​또한, 머신러닝을 활용한 빅데이터 프로젝트도 운영하여 앞으로의 진로를 빅데이터나 인공지능쪽으로 생각하신 분들도 충분히 교육을 받을 수 있어요.​----------------------------------------------------------------------------------------------------------​#지역별 전문교육기관 운영(서울 종로 본점, 강남점, 구로점, 대구점, 부산점)​교육기관 자체 강의 촬영한 #인터넷강의 사이트 운영​#체계화 된 IT기술 커리큘럼으로 효율적인 실무 위주의 교육 진행​직업능력의 달 직업능력개발 유공 #국무총리표창 수상​#정보통신분야 취업률 1위 선정기관​#고용노동부 훈련이수자 평가 최고등급 획득기관​#국비지원교육 취업반 훈련생 취업률 90%이상​#아마존 AWS클라우드 미취업자 국내유일 지정교육센터​#마이크로소프트 골드파트너쉽 체결 기관(모든 PC 정품 SW 사용)​#국제공인자격증 시험센터 운영(OCP, OCJP, CCNA, MCITP, LPIC 등)​#정보통신부 단체 검정장 운영(리눅스마스터, 네트워크관리사, PC정비사 등)​#(주)바쿠츄와 일본IT취업 교육센터 운영(취업을 위한 일본어 교육, 일본 면접 실무진과의 1:1 모의 면접, 일본 정규직 취업연계 가능)​----------------------------------------------------------------------------------------------------------​개인별로 취업희망 분야와 처음 시작하는 상황이 다르기 때문에 똑같은 방법, 똑같은 기간 동안 준비하는 것은 맞지 않아요.​가장 효율적이고 시간 낭비를 하지 않는 취업 준비 방법은 본인에게 알맞은 취업 계획을 세우시고 현실적으로 준비를 해야 해요.​IT 분야로 체계적인 학습 패턴과 전문적인 커리큘럼을 가지고 취업이나 진학을 원하거나 자기개발을 준비하고 싶다면 네임카드 보고 부담갖지말고 편하게 문의하세요 ~​지식IN을 통해 문의주시는 분들에 한해 1:1 맞춤 컨설팅 무료이벤트 진행하고 있답니다! : 0\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 저는 IT전문교육기관의 예은쌤입니다.​저는 KG아이티뱅크 종로본점, 강남점, 구로점, 부산점, 대구점 추천드립니다.​제가 추천해준것 이외 다른 곳을 보신다면 주의사항 몇가지알려드리겠습니다.​IT 교육기관이 점점 생겨나가고 있어서, 본인에게 맞는 기관인지를 따져 고르시는것을 추천드립니다. IT 교육기관이 어떤 식으로 정해야하는지 간략하게 말씀드리겠습니다. ​​1. IT 교육기관 취업률 1위인지 확인 해봐야합니다.2. IT 교육기관의 재정적인 부분을 확인해야 합니다.3. 새로 생긴 학원이면 언제 사라질지 모릅니다. (재정난 등)4. 교육기관에서학생들을 가르치는 강사진이 타임강사인지 알아야 합니다.5. 학점은행제 시스템이 제대로 구비되있는지 확인합니다.6. IT쪽으로 취업을 원하시면 확인해야할 것은 \"취업지원실\" 입니다.7. IT교육기관의 시설과 장비를 알아야합니다. ​​혹은 개인적인 사정으로 인해 강의를 참여 못하는 경우, IT교육기관과 집이 너무 멀어 다니기 어려운 상황도 있습니다.​그래서, 이런 문제점을 해결하기 위해 IT기관 홈페이지에 인터넷강의 시스템이 있는지 확인하면 좋습니다.​유튜브, 인터넷사이트가 아닌 학원 전임강사의 실무강의가 인터넷강의 시스템이 있으면 좋은점이​1. 예습, 복습이 가능합니다2. 강의 참석을 못했을 때 보충자료가 가능합니다.3. 학원을 다니지 않더라도 집에서 공부하는 등, 편리하게 이용 하실 수 있으실 겁니다.​작성자님과 맞는 곳을 선택해야하니 세세한 조건을 따지는 것이 좋습니다.​1. 교육기관에서 실제로 활동내역이 있는 취업지원실이 있는지 확인합니다.2. ( 취업과 자격증을 따기 전 학력에 대해 피드백을 줄 수 있는) 학점은행제가 갖추어져 있는지 확인합니다.3. (여러 학원을 오가는) 타임강사가 아닌, 한 학원에 정착된 전문강사가 있는지 확인합니다.​그보다 필요한 것은 작성자님의 상황에 맞는 상세하고 정확한 커리큘럼, 취업 또는 진학을 위한 \"컨설팅\"입니다​더 상세한 상담이나 궁금한거 있으시면 언제든지 문의해주세요​IT 진학http://bit.ly/2Xt4iBz​​설문상담http://bit.ly/2RPgHPe\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요. 저는 IT전문교육기관진로·진학상담을 도와드리고 있는 교육팀장 입니다.질문해주신 내용에 도움을 주고싶어 답변남깁니다.​아래의 국내최대규모의 IT교육기관으로 추천드립니다​ http://bit.ly/2JIE8YE             누적 수강인원 9만명의 이유있는 선택 IT 교육의 중심! 아이티뱅크에서 성공하십시오! bit.ly            => 파이썬 관련된 답변드리겠습니다. 프로그래밍 공부라는건 단기간에 마스터할 수 있는게 아닙니다.단계별로 심화과정을 넘어갈 수 있는것처럼 실력을 향상시키는 것입니다.어떤 목적을 가지고, 어떤 프로그램을, 어떤 사이트를 구축할건지에 따라서배워야하는 과목과 공부과정이 틀린 것 입니다. 파이썬은 프로그래밍 언어의 한 종류이다보니무료 소프트웨어이며, 오픈소스를 제공하고. 문법이 쉽고 간단하기 때문에 쉽게 배우고 익힐 수 있습니다. 생산성이 높고 빠른 구현이 가능합니다. 스크립트 언어로써 컴파일 작업 없이 소스 코드 그대로 실행하고, 반복이 가능한 객체를 사용하여 매우 편리합니다. 들여 쓰기로 블록을 표현할 수 있고, 소스 코드가 매우 직관적이어서 읽고 쓰기가 용이합니다. 다양한 표현법을 사용할 수 있어서 소스 코드의 길이가 매우 짧고, 개발 시간을 단축할 수 있어서 개발자들이 선호하는 언어입니다 그렇기때문에 비전공자, 기초자 분들이 쉽게 접할 수 있으며가장 기초적으로 배우는 언어라고 보면 됩니다. ‣‣ 필요자격증 및 공부 C언어 -‣ 자료구조 -‣ 소켓프로그래밍 -‣ JAVA -‣ JSP -‣ MS -‣ Linux -‣ CCNA -‣ CCNP 순으로 공부를 하셔야 하고 기간은 1년정도 소요된다고 보셔야 합니다.위 교육과정에 따라 공부를 하셨다고 해도 포트폴리오, 기술서, 기술면접대비 등이 남아있습니다. IT직무분야 자격증정보보안전문가 : CISA, CISSP, CHE, 정보보안기사&산업기사프로그래밍 : OCJP, OC?WCD, OCBCD, 정보처리기사 / 산업기사서버 운영체제 : LPIC, MCITP, OCSA, OCNA, 리눅스마스터네트워크 : CCNA, CCNP, CCIE, 네트워크관리사데이터베이스 : OCA, OCP, OCM 응용프로그래머 - C/C++/C# [ C언어 기술 위주 ]시스템프로그래머 - C / C++ / C# / Linux Server / Python웹 프로그래머 - JAVA / JSP / Spring / Linux / PHP앱 프로그래머 - C / JAVA / Object-C / Android 개발 능력  처음 공부를 시작하는 단계라고 한다면,가장 먼저 배워야하는것은Python언어부터 시작을 하는게 좋습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "예은쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요! IT 교육기관 컨설턴트 멘토스쌤이에요~​질문해주신 내용을 바탕으로 답변드릴게요!​IT 직업군은 4차 산업혁명으로 정보화시대에 미래 유망직종이 되었어요~​현재 정부의 많은 지원을 받고 있으며, 일자리의 수요가 높은 직업군으로서 앞으로의 전망이 밝은 편이에요.​♥ Python ♥​프로그래밍 언어는 크게 '절차 지향', '객체 지향', '함수형 패러다임' 언어로 나누어지는데 가장 유명한 절차 지향 언어가 C언어, 객체 지향은 JAVA 등이 있어요. Python은 이러한 언어 모두 구현이 가능하답니다.​초보자부터 전문가까지 누구든 쉽게 접근할 수 있는 프로그래밍 언어로 다양한 플랫폼에서 활용이 가능하며 다양한 라이브러리 지원으로 머신러닝, 그래픽, 웹 개발 등 IT 전분야에서 꾸준히 성장하고 있어요.​영어와 문법과 사고가 비슷하여 마치 영어를 작문하는 과정처럼 느낄 수 있는 프로그래밍 언어에요.​다양한 기능을 갖고 있기에 초기에는 문법이나 제어문 등에 대해 공부를 하게 돼요.​심화로 넘어갈수록 웹 서버를 구축하거나 빅데이터에 관심이 있는 분들이라면 머신러닝을 활용하여 빅데이터 프로젝트를 진행해보는 등에 대한 심화교육이 필요하게 된답니다.​기초부터 심화까지 교육을 받을 수 있는 교육 기관에서 단기간에 배우는 게 중요해요.​KG 아이티뱅크 IT 전문교육기관에서는 Python 기초는 1달 과정으로 끝나고 웹서버 구축도 1개월 과정으로 배울 수 있어요.​또한, 머신러닝을 활용한 빅데이터 프로젝트도 운영하여 앞으로의 진로를 빅데이터나 인공지능쪽으로 생각하신 분들도 충분히 교육을 받을 수 있어요.​----------------------------------------------------------------------------------------------------------​#지역별 전문교육기관 운영(서울 종로 본점, 강남점, 구로점, 대구점, 부산점)​교육기관 자체 강의 촬영한 #인터넷강의 사이트 운영​#체계화 된 IT기술 커리큘럼으로 효율적인 실무 위주의 교육 진행​직업능력의 달 직업능력개발 유공 #국무총리표창 수상​#정보통신분야 취업률 1위 선정기관​#고용노동부 훈련이수자 평가 최고등급 획득기관​#국비지원교육 취업반 훈련생 취업률 90%이상​#아마존 AWS클라우드 미취업자 국내유일 지정교육센터​#마이크로소프트 골드파트너쉽 체결 기관(모든 PC 정품 SW 사용)​#국제공인자격증 시험센터 운영(OCP, OCJP, CCNA, MCITP, LPIC 등)​#정보통신부 단체 검정장 운영(리눅스마스터, 네트워크관리사, PC정비사 등)​#(주)바쿠츄와 일본IT취업 교육센터 운영(취업을 위한 일본어 교육, 일본 면접 실무진과의 1:1 모의 면접, 일본 정규직 취업연계 가능)​----------------------------------------------------------------------------------------------------------​개인별로 취업희망 분야와 처음 시작하는 상황이 다르기 때문에 똑같은 방법, 똑같은 기간 동안 준비하는 것은 맞지 않아요.​가장 효율적이고 시간 낭비를 하지 않는 취업 준비 방법은 본인에게 알맞은 취업 계획을 세우시고 현실적으로 준비를 해야 해요.​IT 분야로 체계적인 학습 패턴과 전문적인 커리큘럼을 가지고 취업이나 진학을 원하거나 자기개발을 준비하고 싶다면 네임카드 보고 부담갖지말고 편하게 문의하세요 ~​지식IN을 통해 문의주시는 분들에 한해 1:1 맞춤 컨설팅 무료이벤트 진행하고 있답니다! : 0\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "예은쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 저는 IT전문교육기관의 예은쌤입니다.​저는 KG아이티뱅크 종로본점, 강남점, 구로점, 부산점, 대구점 추천드립니다.​제가 추천해준것 이외 다른 곳을 보신다면 주의사항 몇가지알려드리겠습니다.​IT 교육기관이 점점 생겨나가고 있어서, 본인에게 맞는 기관인지를 따져 고르시는것을 추천드립니다. IT 교육기관이 어떤 식으로 정해야하는지 간략하게 말씀드리겠습니다. ​​1. IT 교육기관 취업률 1위인지 확인 해봐야합니다.2. IT 교육기관의 재정적인 부분을 확인해야 합니다.3. 새로 생긴 학원이면 언제 사라질지 모릅니다. (재정난 등)4. 교육기관에서학생들을 가르치는 강사진이 타임강사인지 알아야 합니다.5. 학점은행제 시스템이 제대로 구비되있는지 확인합니다.6. IT쪽으로 취업을 원하시면 확인해야할 것은 \"취업지원실\" 입니다.7. IT교육기관의 시설과 장비를 알아야합니다. ​​혹은 개인적인 사정으로 인해 강의를 참여 못하는 경우, IT교육기관과 집이 너무 멀어 다니기 어려운 상황도 있습니다.​그래서, 이런 문제점을 해결하기 위해 IT기관 홈페이지에 인터넷강의 시스템이 있는지 확인하면 좋습니다.​유튜브, 인터넷사이트가 아닌 학원 전임강사의 실무강의가 인터넷강의 시스템이 있으면 좋은점이​1. 예습, 복습이 가능합니다2. 강의 참석을 못했을 때 보충자료가 가능합니다.3. 학원을 다니지 않더라도 집에서 공부하는 등, 편리하게 이용 하실 수 있으실 겁니다.​작성자님과 맞는 곳을 선택해야하니 세세한 조건을 따지는 것이 좋습니다.​1. 교육기관에서 실제로 활동내역이 있는 취업지원실이 있는지 확인합니다.2. ( 취업과 자격증을 따기 전 학력에 대해 피드백을 줄 수 있는) 학점은행제가 갖추어져 있는지 확인합니다.3. (여러 학원을 오가는) 타임강사가 아닌, 한 학원에 정착된 전문강사가 있는지 확인합니다.​그보다 필요한 것은 작성자님의 상황에 맞는 상세하고 정확한 커리큘럼, 취업 또는 진학을 위한 \"컨설팅\"입니다​더 상세한 상담이나 궁금한거 있으시면 언제든지 문의해주세요​IT 진학http://bit.ly/2Xt4iBz​​설문상담http://bit.ly/2RPgHPe\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "예은쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요. 저는 IT전문교육기관진로·진학상담을 도와드리고 있는 교육팀장 입니다.질문해주신 내용에 도움을 주고싶어 답변남깁니다.​아래의 국내최대규모의 IT교육기관으로 추천드립니다​ http://bit.ly/2JIE8YE             누적 수강인원 9만명의 이유있는 선택 IT 교육의 중심! 아이티뱅크에서 성공하십시오! bit.ly            => 파이썬 관련된 답변드리겠습니다. 프로그래밍 공부라는건 단기간에 마스터할 수 있는게 아닙니다.단계별로 심화과정을 넘어갈 수 있는것처럼 실력을 향상시키는 것입니다.어떤 목적을 가지고, 어떤 프로그램을, 어떤 사이트를 구축할건지에 따라서배워야하는 과목과 공부과정이 틀린 것 입니다. 파이썬은 프로그래밍 언어의 한 종류이다보니무료 소프트웨어이며, 오픈소스를 제공하고. 문법이 쉽고 간단하기 때문에 쉽게 배우고 익힐 수 있습니다. 생산성이 높고 빠른 구현이 가능합니다. 스크립트 언어로써 컴파일 작업 없이 소스 코드 그대로 실행하고, 반복이 가능한 객체를 사용하여 매우 편리합니다. 들여 쓰기로 블록을 표현할 수 있고, 소스 코드가 매우 직관적이어서 읽고 쓰기가 용이합니다. 다양한 표현법을 사용할 수 있어서 소스 코드의 길이가 매우 짧고, 개발 시간을 단축할 수 있어서 개발자들이 선호하는 언어입니다 그렇기때문에 비전공자, 기초자 분들이 쉽게 접할 수 있으며가장 기초적으로 배우는 언어라고 보면 됩니다. ‣‣ 필요자격증 및 공부 C언어 -‣ 자료구조 -‣ 소켓프로그래밍 -‣ JAVA -‣ JSP -‣ MS -‣ Linux -‣ CCNA -‣ CCNP 순으로 공부를 하셔야 하고 기간은 1년정도 소요된다고 보셔야 합니다.위 교육과정에 따라 공부를 하셨다고 해도 포트폴리오, 기술서, 기술면접대비 등이 남아있습니다. IT직무분야 자격증정보보안전문가 : CISA, CISSP, CHE, 정보보안기사&산업기사프로그래밍 : OCJP, OC?WCD, OCBCD, 정보처리기사 / 산업기사서버 운영체제 : LPIC, MCITP, OCSA, OCNA, 리눅스마스터네트워크 : CCNA, CCNP, CCIE, 네트워크관리사데이터베이스 : OCA, OCP, OCM 응용프로그래머 - C/C++/C# [ C언어 기술 위주 ]시스템프로그래머 - C / C++ / C# / Linux Server / Python웹 프로그래머 - JAVA / JSP / Spring / Linux / PHP앱 프로그래머 - C / JAVA / Object-C / Android 개발 능력  처음 공부를 시작하는 단계라고 한다면,가장 먼저 배워야하는것은Python언어부터 시작을 하는게 좋습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스쿨 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요! IT 교육기관 컨설턴트 멘토스쌤이에요~​질문해주신 내용을 바탕으로 답변드릴게요!​IT 직업군은 4차 산업혁명으로 정보화시대에 미래 유망직종이 되었어요~​현재 정부의 많은 지원을 받고 있으며, 일자리의 수요가 높은 직업군으로서 앞으로의 전망이 밝은 편이에요.​♥ Python ♥​프로그래밍 언어는 크게 '절차 지향', '객체 지향', '함수형 패러다임' 언어로 나누어지는데 가장 유명한 절차 지향 언어가 C언어, 객체 지향은 JAVA 등이 있어요. Python은 이러한 언어 모두 구현이 가능하답니다.​초보자부터 전문가까지 누구든 쉽게 접근할 수 있는 프로그래밍 언어로 다양한 플랫폼에서 활용이 가능하며 다양한 라이브러리 지원으로 머신러닝, 그래픽, 웹 개발 등 IT 전분야에서 꾸준히 성장하고 있어요.​영어와 문법과 사고가 비슷하여 마치 영어를 작문하는 과정처럼 느낄 수 있는 프로그래밍 언어에요.​다양한 기능을 갖고 있기에 초기에는 문법이나 제어문 등에 대해 공부를 하게 돼요.​심화로 넘어갈수록 웹 서버를 구축하거나 빅데이터에 관심이 있는 분들이라면 머신러닝을 활용하여 빅데이터 프로젝트를 진행해보는 등에 대한 심화교육이 필요하게 된답니다.​기초부터 심화까지 교육을 받을 수 있는 교육 기관에서 단기간에 배우는 게 중요해요.​KG 아이티뱅크 IT 전문교육기관에서는 Python 기초는 1달 과정으로 끝나고 웹서버 구축도 1개월 과정으로 배울 수 있어요.​또한, 머신러닝을 활용한 빅데이터 프로젝트도 운영하여 앞으로의 진로를 빅데이터나 인공지능쪽으로 생각하신 분들도 충분히 교육을 받을 수 있어요.​----------------------------------------------------------------------------------------------------------​#지역별 전문교육기관 운영(서울 종로 본점, 강남점, 구로점, 대구점, 부산점)​교육기관 자체 강의 촬영한 #인터넷강의 사이트 운영​#체계화 된 IT기술 커리큘럼으로 효율적인 실무 위주의 교육 진행​직업능력의 달 직업능력개발 유공 #국무총리표창 수상​#정보통신분야 취업률 1위 선정기관​#고용노동부 훈련이수자 평가 최고등급 획득기관​#국비지원교육 취업반 훈련생 취업률 90%이상​#아마존 AWS클라우드 미취업자 국내유일 지정교육센터​#마이크로소프트 골드파트너쉽 체결 기관(모든 PC 정품 SW 사용)​#국제공인자격증 시험센터 운영(OCP, OCJP, CCNA, MCITP, LPIC 등)​#정보통신부 단체 검정장 운영(리눅스마스터, 네트워크관리사, PC정비사 등)​#(주)바쿠츄와 일본IT취업 교육센터 운영(취업을 위한 일본어 교육, 일본 면접 실무진과의 1:1 모의 면접, 일본 정규직 취업연계 가능)​----------------------------------------------------------------------------------------------------------​개인별로 취업희망 분야와 처음 시작하는 상황이 다르기 때문에 똑같은 방법, 똑같은 기간 동안 준비하는 것은 맞지 않아요.​가장 효율적이고 시간 낭비를 하지 않는 취업 준비 방법은 본인에게 알맞은 취업 계획을 세우시고 현실적으로 준비를 해야 해요.​IT 분야로 체계적인 학습 패턴과 전문적인 커리큘럼을 가지고 취업이나 진학을 원하거나 자기개발을 준비하고 싶다면 네임카드 보고 부담갖지말고 편하게 문의하세요 ~​지식IN을 통해 문의주시는 분들에 한해 1:1 맞춤 컨설팅 무료이벤트 진행하고 있답니다! : 0\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스쿨 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 저는 IT전문교육기관의 예은쌤입니다.​저는 KG아이티뱅크 종로본점, 강남점, 구로점, 부산점, 대구점 추천드립니다.​제가 추천해준것 이외 다른 곳을 보신다면 주의사항 몇가지알려드리겠습니다.​IT 교육기관이 점점 생겨나가고 있어서, 본인에게 맞는 기관인지를 따져 고르시는것을 추천드립니다. IT 교육기관이 어떤 식으로 정해야하는지 간략하게 말씀드리겠습니다. ​​1. IT 교육기관 취업률 1위인지 확인 해봐야합니다.2. IT 교육기관의 재정적인 부분을 확인해야 합니다.3. 새로 생긴 학원이면 언제 사라질지 모릅니다. (재정난 등)4. 교육기관에서학생들을 가르치는 강사진이 타임강사인지 알아야 합니다.5. 학점은행제 시스템이 제대로 구비되있는지 확인합니다.6. IT쪽으로 취업을 원하시면 확인해야할 것은 \"취업지원실\" 입니다.7. IT교육기관의 시설과 장비를 알아야합니다. ​​혹은 개인적인 사정으로 인해 강의를 참여 못하는 경우, IT교육기관과 집이 너무 멀어 다니기 어려운 상황도 있습니다.​그래서, 이런 문제점을 해결하기 위해 IT기관 홈페이지에 인터넷강의 시스템이 있는지 확인하면 좋습니다.​유튜브, 인터넷사이트가 아닌 학원 전임강사의 실무강의가 인터넷강의 시스템이 있으면 좋은점이​1. 예습, 복습이 가능합니다2. 강의 참석을 못했을 때 보충자료가 가능합니다.3. 학원을 다니지 않더라도 집에서 공부하는 등, 편리하게 이용 하실 수 있으실 겁니다.​작성자님과 맞는 곳을 선택해야하니 세세한 조건을 따지는 것이 좋습니다.​1. 교육기관에서 실제로 활동내역이 있는 취업지원실이 있는지 확인합니다.2. ( 취업과 자격증을 따기 전 학력에 대해 피드백을 줄 수 있는) 학점은행제가 갖추어져 있는지 확인합니다.3. (여러 학원을 오가는) 타임강사가 아닌, 한 학원에 정착된 전문강사가 있는지 확인합니다.​그보다 필요한 것은 작성자님의 상황에 맞는 상세하고 정확한 커리큘럼, 취업 또는 진학을 위한 \"컨설팅\"입니다​더 상세한 상담이나 궁금한거 있으시면 언제든지 문의해주세요​IT 진학http://bit.ly/2Xt4iBz​​설문상담http://bit.ly/2RPgHPe\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "멘토스쿨 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요. 저는 IT전문교육기관진로·진학상담을 도와드리고 있는 교육팀장 입니다.질문해주신 내용에 도움을 주고싶어 답변남깁니다.​아래의 국내최대규모의 IT교육기관으로 추천드립니다​ http://bit.ly/2JIE8YE             누적 수강인원 9만명의 이유있는 선택 IT 교육의 중심! 아이티뱅크에서 성공하십시오! bit.ly            => 파이썬 관련된 답변드리겠습니다. 프로그래밍 공부라는건 단기간에 마스터할 수 있는게 아닙니다.단계별로 심화과정을 넘어갈 수 있는것처럼 실력을 향상시키는 것입니다.어떤 목적을 가지고, 어떤 프로그램을, 어떤 사이트를 구축할건지에 따라서배워야하는 과목과 공부과정이 틀린 것 입니다. 파이썬은 프로그래밍 언어의 한 종류이다보니무료 소프트웨어이며, 오픈소스를 제공하고. 문법이 쉽고 간단하기 때문에 쉽게 배우고 익힐 수 있습니다. 생산성이 높고 빠른 구현이 가능합니다. 스크립트 언어로써 컴파일 작업 없이 소스 코드 그대로 실행하고, 반복이 가능한 객체를 사용하여 매우 편리합니다. 들여 쓰기로 블록을 표현할 수 있고, 소스 코드가 매우 직관적이어서 읽고 쓰기가 용이합니다. 다양한 표현법을 사용할 수 있어서 소스 코드의 길이가 매우 짧고, 개발 시간을 단축할 수 있어서 개발자들이 선호하는 언어입니다 그렇기때문에 비전공자, 기초자 분들이 쉽게 접할 수 있으며가장 기초적으로 배우는 언어라고 보면 됩니다. ‣‣ 필요자격증 및 공부 C언어 -‣ 자료구조 -‣ 소켓프로그래밍 -‣ JAVA -‣ JSP -‣ MS -‣ Linux -‣ CCNA -‣ CCNP 순으로 공부를 하셔야 하고 기간은 1년정도 소요된다고 보셔야 합니다.위 교육과정에 따라 공부를 하셨다고 해도 포트폴리오, 기술서, 기술면접대비 등이 남아있습니다. IT직무분야 자격증정보보안전문가 : CISA, CISSP, CHE, 정보보안기사&산업기사프로그래밍 : OCJP, OC?WCD, OCBCD, 정보처리기사 / 산업기사서버 운영체제 : LPIC, MCITP, OCSA, OCNA, 리눅스마스터네트워크 : CCNA, CCNP, CCIE, 네트워크관리사데이터베이스 : OCA, OCP, OCM 응용프로그래머 - C/C++/C# [ C언어 기술 위주 ]시스템프로그래머 - C / C++ / C# / Linux Server / Python웹 프로그래머 - JAVA / JSP / Spring / Linux / PHP앱 프로그래머 - C / JAVA / Object-C / Android 개발 능력  처음 공부를 시작하는 단계라고 한다면,가장 먼저 배워야하는것은Python언어부터 시작을 하는게 좋습니다.\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=4&dirId=40608&docId=324357176&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=7&search_sort=0&spq=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********질문 제목\n",
      "머신러닝 // 빅데이터 // 프론트&백 개발자... 문의드립니다 제발알려주세요..\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "이 세가지 직업은 모두다 다르다는 것은 알겠는데.. JAVA, 파이썬,, 알고리즘,, SW역량테스트?.. 등등각각의 직업에서 필요로하는 능력이 무엇인지구분이 하나도 안갑니다..누가 정리좀 해주시면 안될까요?...\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "KG에듀원컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 질문자님 KG 에듀원 컨설턴트입니다~우선 머신러닝과 빅데이터 같은경우는 머신러닝은 빅데이터 기반에 많이 쓰입니다~그리고 프론트엔드 & 백엔드 같은경우는 웹개발자의 종류에 속합니다~프론트 엔드는 웹페이지의 앞면을 꾸미는 직업이고백엔드는 뒷면인 웹페이지의 뼈대를 구축하는 직업입니다~그리고 이 두개를 다할 줄 아는 풀스택개발자도 있습니다~빅데이터와 머신러닝을 공부하시려면 우선은 파이썬 - R - 하둡 이렇게 공부하시면 될듯합니다~그리고 나서 머신러닝을 공부하시는것도 좋구요~그러면 빅데이터 쪽으로 진출을 하실 수 있습니다.https://blog.naver.com/dkakfndj1/221496760461 웹개발자 공부는 어떻게 하는지 제가 운영하는 블로그입니다~참고 해주시구요~궁금한점이있으시면 아래 링크로 문의주세요~http://naver.me/FA6TrelPhttps://open.kakao.com/o/suNluecb\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "KG에듀원컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 교육전문컨설턴트 전동문 학과주임선생님입니다.질문에 팩트로만 답변드리겠습니다.  간단하게 IT 직업별 업무에 대해 설명드릴테니 자세한 내용은 문의주세요.IT계열은 크게 프로그래밍 / 네트워크 / 보안 / 시스템 / 데이터베이스로나눌 수 있고 각 분야별로 세부직업군이 다양하게 존재합니다. 1. 프로그래머웹 개발자(웹 프로그래머)는 프런트엔드, 백엔드, 풀스택 개발자가 있고프런트엔드는 웹 디자이너로써 웹 사이트의 제작 및 관리하는 직업이며백엔드는 사용자가 볼 수 없는 서버를 개발 및 담당하는 직업입니다.마지막으로 풀스택 개발자는 프런트엔드와 백엔드 개발을전부 할 수 있는 직업으로 웹 개발의 만능 엔터테이너라고 보면 됩니다.응용프로그래머는 응용sw프로그램을 개발하는 직업으로예를 들어보면 컴퓨터 바탕화면에 보이는 곰플레이어, 액셀, 알약 등의실행가능한 모든 sw프로그램들을 개발하는 일을 합니다.게임프로그래머는 말 그대로 게임을 개발하는 직업으로프로그래밍, 서버, 시스템 등에 대한 지식이 필요합니다. 2. 엔지니어네트워크 엔지니어, 시스템 엔지니어, 서버 엔지니어 등이 있으며네트워크 엔지니어는 전산망과 관련된 네트워크 시스템을분석, 설계 및 구축하는 직업입니다.시스템 엔지니어는 통신장비를 시스템에 맞게 설계 및 관리하는 직업이고서버 엔지니어는 웹서버 구축 및 운영을 기술적으로 다루는 직업입니다. 3. 데이터베이스데이터베이스 분야는 데이터베이스 관리자, 솔루션 아키텍쳐,빅데이터전문가, 빅데이터 분석가가 있습니다.데이터베이스 관리자는 인터넷을 통해 수집되는 정보, 데이터들을수집 및 정리하여 분석한 다음 데이터베이스를 구축하는 직업이고솔루션 아키텍쳐는 기업의 시스템 환경을 AWS환경으로변경할 수 있도록 설계해주는 직업입니다. 4. 보안 / 해킹정보보안 분야는 디지털포렌식전문가, 보안솔루션, 악성코드전문가,모의해킹전문가, 보안관제, 침해대응 등 다양한 직업이 있습니다.디지털포렌식전문가는 손상된 데이터를 복구 및 복원하는 직업이며주로 경찰청이나 사이버 안전국에서 근무합니다.보안솔루션은 기업의 IT 정보보호를 목적으로 모의해킹을 통해취약점을 분석하고 대비하는 직업입니다.악성코드전문가는 기존에 있던 악성코드를 분석하여 백신 또는보안프로그램을 개발하는 직업입니다.모의해킹전문가는 웹이나 서버를 모의해킹하여 발생할 수 있는모든 해킹에 대비하여 취약점을 분석하는 직업입니다.보안관제는 지속적인 모니터링으로 1차적으로 해킹에 대응하는 직업이며침해대응은 해커의 침투로 피해가 생길 시 데이터를 복구하고범인을 역추적하는 역할을 하는 직업입니다. < IT 직업 >https://blog.naver.com/11glgl/221464165714  현재 무료로 진로 및 취업 컨설팅을 진행하고 있으니질문자님이 좀 더 자세한 정보와 도움을 원하거나정확한 진단을 받은 후 적절한 공부법과 계획에 대해알고 싶다면 문의주셔서 정확한 도움 받아보시기 바랍니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "KG에듀원컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요.IT정보통신기술 분야지식IN 전문가랭킹 1위취업진로지도전문가 입니다.질문에 답변드리겠습니다.JAVA, 파이썬, 알고리즘은 프로그래밍 코딩 작업을 할때 꼭 필요한 언어 입니다.머신러닝, 빅데이터, 프론트엔드&백엔드 개발자 모두 범용적으로 사용하는 기술입니다.SW역량테스트는 기업에서 입사시 지원자들을 통해 면접시에 SW 실무능력을 평가하는 시험이고 기업마다 보는 언어 및 난이도의 차이가 있습니다.지식iN 글을 보시고 연락주시는 분들 대상으로 [ 무료 진로/취업/교육 맞춤 컨설팅 ] 을 진행해드리고 있습니다. 머신러닝/빅데이터/프론트엔드&백엔드 개발자 진로, SW역량테스트에 대해서 컨설팅을 받기를 희망하신다면, 상단의 네임카드 연락처를 통해 연락주시거나, 하단의 실시간상담&배너를 클릭하셔서 문의 남겨 주시면 상세한 1:1 맞춤 컨설팅 도와드리도록 하겠습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "KG에듀원컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "프로그래밍(코딩)공부는일반적인 공부와는 틀립니다프로그래밍은 IT기술직입니다이러한 기술직 공부는책 위주의 이론식 학습이 되시면크게 도움이 안됩니다일단 프로그래밍의 개념부터잠시 짚고 넘어가보면프로그래밍이란 코딩이라고도 불리우며사람이 컴퓨터에게 명령을 하달명령을 받은 컴퓨터가동작하고 실행하는 원리입니다자! 이렇게 컴퓨터와 대화를 해야 하기위해사람과 컴퓨터를 이어주는 컴퓨터언어인프로그래밍언어를 필수적으로 공부하셔야 하는데요이러한 프로그래밍 언어도 종류가 여러 가지가 있죠?c언어, c++, c#, Java, 파이썬 등등..각자 언어들마다 만든 회사도 틀리며각 언어들마다 특장점이 틀립니다그리고 회사마다 주로 쓰는 언어가 틀리기 때문에프로그래밍 언어를 최대한 많이 섭렵하시는게 좋으십니다그리고 프로그래밍 언어만 공부했다고 끝인가? NO!프로그래밍 언어를 알았으니 활용하는 방법도 공부하셔야 합니다이러한 과정을 프로그래밍 한다고 하는데프로그래밍을 할때도 남들이 써놓은 코딩기법을 달달 외우는게 아니라직접 코딩을 하면서 에러도 많이나고이러한 에러점을 찾는 과정에서 학습이 엄청 많이 되십니다기초적인 부분은 실무기술이 가장 중요한 분야이기 때문에실무에서 어떠한 기술을 쓰는지 배우셔야 합니다지식인이라는 틀안에서 정보를 다드리기에는 한계점이 있네요ㅠㅠ저는 현재 취업율&자격증 취득율 1위인IT전문교육기관 교육부팀장으로써 많은 학생들의현실에 맞는 앞으로의 계획과 공부방법등에 대해 도움을 드리고 있습니다네이버 정책상 자세한 언급은 제한되오니궁금한 사항이 있으시다 하시면 네임카드 참고하여 연락주시면질문자님의 고민 성심성의껏 해결해드리겠습니다 :)▼ 청춘상담소 ▼▼ 오픈카톡 ▼\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "KG에듀원컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 IT 전문교육기관에 근무하고 있는 교육상담선생님 더블유쌤입니다.많은 고민들을 하셔서 개인적으로 쉽지 않은 선택을 하시는 분 또는 해당분야에 궁금한 점을 해결하기 위해 조금이나마 도움이 되고자 하는 생각에 답변을 드립니다.질문자께서 어떤 분야를 얼마나 배우셨는지 또는 학력이 어디까지인지 써 주시지 않아 더 깊게 말씀 드리지 못하는 부분에 대해서는 양해 말씀드립니다. 프로그래머는 소프트웨어 언어를 기반으로 자기 자신의 창의적인 내용을 더해프로그램을 설계, 작성하고 테스트하는 역할까지 만능으로 하는 기술자입니다. 또한 기존의 프로그램을 수정하고 유지 보수하는 일도하기 때문에 기능적인 일이기도 합니다. 프로그래머가 이런 기본적인 일들을 하려면 기본적인 프로그래밍 능력이 되어야 합니다. 프로그래밍의 언어를 숙달한 이후에 프로그래밍, 네트워크, 시스템 등의다양한 분야에 대한 기초지식을 쌓은 후 각 분야별로 전문적인 지식으로더 깊이 들어가야 하고 자격증 등을 취득하면서 기반을 만드는 것을 하셔야합니다. 프로그래머의 종류로는 응용, 시스템, 웹, 게임, 보안 프로그래머가각각 있으며 대기업으로 취업하기에 많은 도움이 되는 직업이기도 합니다. 프로그래머의 길은 다방면의 컴퓨터 기술에 능해야 하므로전문적인 기관에서 배울 필요가 있습니다.현재 상황에서 아무것도 모르는데 기초 지식 없이 다른 사람들이 글을올린 것을 바탕으로만 하여 게임, 코딩, 프로그래밍, 정보보안 등 이런 것에 손을 대면 하나도 이해 못하는 것이 당연합니다. 일반적으로 프로그래밍을 하기 위해서 C언어나 JAVA, 파이썬 등 언어 등을 먼저 시작합니다.대부분 사람들이 가장 기본으로 생각하고 있는 것이기도 하구요. 왜냐하면 C언어나 JAVA는 한글을 배움에 있어서 ㄱ,ㄴ,ㄷ,ㄹ.. 가,나,다,라를 배우는 것과 같습니다.즉 프로그래밍이 아니라 프로그래밍을 구성하는 언어이지요. 모든 프로그램을 다루기 위해서는 프로그래밍 언어를 먼저 아는 것이 중요합니다. 언어를 배우고 나서야 네트워크나 시스템 데이터 베이스 등을 배울 수 있는 근간이 됩니다. 이 언어 등을 통해 프로그램을 만드는 것을 ‘코딩’ 이라고 하며 코딩과 같은 말이 프로그래밍이라고 보시면 됩니다. 이제 여기서부터 어떻게 배우는 가입니다. 언어를 배웠으면어떻게 사용할 것인지 또는 특화 시킬 것인지가 중요하다고 볼 수 있습니다. 프로그램언어는 C언어와 JAVA를 더 심화시켜 사용하기도 합니다.예를 들어 데이터들을 어떠한 기억장치에 대해 저장, 읽기, 쓰기 등을 배우는 것을 자료구조라고 합니다. 이러한 것처럼 더욱 심화 시킨 프로그래밍 언어를 기반으로 하여단계적으로 나아가는 것이 IT 기술을 배우는 것에 기본이라고 할 수 있습니다. 프로그래밍를 배우게 되면 단계적으로 배워 나아가면서머리로 하는 것이 아니라 몸이 반응 하는 것인 ‘체화’가 될 것입니다. 하지만 이 ‘체화’가 되기까지 그리고 체화가 되고 나서그 윗 단계로 가는 것에 대해 독학 등은 상당한 무리가 있습니다.  질문에 대한 답변이외에 더 궁금하신 점이 있으시면 언제든지오픈채팅 주소 혹은 배너를 통해 문의주세요무료상담을 진행하고 있습니다 https://open.kakao.com/o/sts2Ylcbhttps://itbank-study.edueroom.co.kr/http://naver.me/5n3w6VfR\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육전문컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 질문자님 KG 에듀원 컨설턴트입니다~우선 머신러닝과 빅데이터 같은경우는 머신러닝은 빅데이터 기반에 많이 쓰입니다~그리고 프론트엔드 & 백엔드 같은경우는 웹개발자의 종류에 속합니다~프론트 엔드는 웹페이지의 앞면을 꾸미는 직업이고백엔드는 뒷면인 웹페이지의 뼈대를 구축하는 직업입니다~그리고 이 두개를 다할 줄 아는 풀스택개발자도 있습니다~빅데이터와 머신러닝을 공부하시려면 우선은 파이썬 - R - 하둡 이렇게 공부하시면 될듯합니다~그리고 나서 머신러닝을 공부하시는것도 좋구요~그러면 빅데이터 쪽으로 진출을 하실 수 있습니다.https://blog.naver.com/dkakfndj1/221496760461 웹개발자 공부는 어떻게 하는지 제가 운영하는 블로그입니다~참고 해주시구요~궁금한점이있으시면 아래 링크로 문의주세요~http://naver.me/FA6TrelPhttps://open.kakao.com/o/suNluecb\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육전문컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 교육전문컨설턴트 전동문 학과주임선생님입니다.질문에 팩트로만 답변드리겠습니다.  간단하게 IT 직업별 업무에 대해 설명드릴테니 자세한 내용은 문의주세요.IT계열은 크게 프로그래밍 / 네트워크 / 보안 / 시스템 / 데이터베이스로나눌 수 있고 각 분야별로 세부직업군이 다양하게 존재합니다. 1. 프로그래머웹 개발자(웹 프로그래머)는 프런트엔드, 백엔드, 풀스택 개발자가 있고프런트엔드는 웹 디자이너로써 웹 사이트의 제작 및 관리하는 직업이며백엔드는 사용자가 볼 수 없는 서버를 개발 및 담당하는 직업입니다.마지막으로 풀스택 개발자는 프런트엔드와 백엔드 개발을전부 할 수 있는 직업으로 웹 개발의 만능 엔터테이너라고 보면 됩니다.응용프로그래머는 응용sw프로그램을 개발하는 직업으로예를 들어보면 컴퓨터 바탕화면에 보이는 곰플레이어, 액셀, 알약 등의실행가능한 모든 sw프로그램들을 개발하는 일을 합니다.게임프로그래머는 말 그대로 게임을 개발하는 직업으로프로그래밍, 서버, 시스템 등에 대한 지식이 필요합니다. 2. 엔지니어네트워크 엔지니어, 시스템 엔지니어, 서버 엔지니어 등이 있으며네트워크 엔지니어는 전산망과 관련된 네트워크 시스템을분석, 설계 및 구축하는 직업입니다.시스템 엔지니어는 통신장비를 시스템에 맞게 설계 및 관리하는 직업이고서버 엔지니어는 웹서버 구축 및 운영을 기술적으로 다루는 직업입니다. 3. 데이터베이스데이터베이스 분야는 데이터베이스 관리자, 솔루션 아키텍쳐,빅데이터전문가, 빅데이터 분석가가 있습니다.데이터베이스 관리자는 인터넷을 통해 수집되는 정보, 데이터들을수집 및 정리하여 분석한 다음 데이터베이스를 구축하는 직업이고솔루션 아키텍쳐는 기업의 시스템 환경을 AWS환경으로변경할 수 있도록 설계해주는 직업입니다. 4. 보안 / 해킹정보보안 분야는 디지털포렌식전문가, 보안솔루션, 악성코드전문가,모의해킹전문가, 보안관제, 침해대응 등 다양한 직업이 있습니다.디지털포렌식전문가는 손상된 데이터를 복구 및 복원하는 직업이며주로 경찰청이나 사이버 안전국에서 근무합니다.보안솔루션은 기업의 IT 정보보호를 목적으로 모의해킹을 통해취약점을 분석하고 대비하는 직업입니다.악성코드전문가는 기존에 있던 악성코드를 분석하여 백신 또는보안프로그램을 개발하는 직업입니다.모의해킹전문가는 웹이나 서버를 모의해킹하여 발생할 수 있는모든 해킹에 대비하여 취약점을 분석하는 직업입니다.보안관제는 지속적인 모니터링으로 1차적으로 해킹에 대응하는 직업이며침해대응은 해커의 침투로 피해가 생길 시 데이터를 복구하고범인을 역추적하는 역할을 하는 직업입니다. < IT 직업 >https://blog.naver.com/11glgl/221464165714  현재 무료로 진로 및 취업 컨설팅을 진행하고 있으니질문자님이 좀 더 자세한 정보와 도움을 원하거나정확한 진단을 받은 후 적절한 공부법과 계획에 대해알고 싶다면 문의주셔서 정확한 도움 받아보시기 바랍니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육전문컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요.IT정보통신기술 분야지식IN 전문가랭킹 1위취업진로지도전문가 입니다.질문에 답변드리겠습니다.JAVA, 파이썬, 알고리즘은 프로그래밍 코딩 작업을 할때 꼭 필요한 언어 입니다.머신러닝, 빅데이터, 프론트엔드&백엔드 개발자 모두 범용적으로 사용하는 기술입니다.SW역량테스트는 기업에서 입사시 지원자들을 통해 면접시에 SW 실무능력을 평가하는 시험이고 기업마다 보는 언어 및 난이도의 차이가 있습니다.지식iN 글을 보시고 연락주시는 분들 대상으로 [ 무료 진로/취업/교육 맞춤 컨설팅 ] 을 진행해드리고 있습니다. 머신러닝/빅데이터/프론트엔드&백엔드 개발자 진로, SW역량테스트에 대해서 컨설팅을 받기를 희망하신다면, 상단의 네임카드 연락처를 통해 연락주시거나, 하단의 실시간상담&배너를 클릭하셔서 문의 남겨 주시면 상세한 1:1 맞춤 컨설팅 도와드리도록 하겠습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육전문컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "프로그래밍(코딩)공부는일반적인 공부와는 틀립니다프로그래밍은 IT기술직입니다이러한 기술직 공부는책 위주의 이론식 학습이 되시면크게 도움이 안됩니다일단 프로그래밍의 개념부터잠시 짚고 넘어가보면프로그래밍이란 코딩이라고도 불리우며사람이 컴퓨터에게 명령을 하달명령을 받은 컴퓨터가동작하고 실행하는 원리입니다자! 이렇게 컴퓨터와 대화를 해야 하기위해사람과 컴퓨터를 이어주는 컴퓨터언어인프로그래밍언어를 필수적으로 공부하셔야 하는데요이러한 프로그래밍 언어도 종류가 여러 가지가 있죠?c언어, c++, c#, Java, 파이썬 등등..각자 언어들마다 만든 회사도 틀리며각 언어들마다 특장점이 틀립니다그리고 회사마다 주로 쓰는 언어가 틀리기 때문에프로그래밍 언어를 최대한 많이 섭렵하시는게 좋으십니다그리고 프로그래밍 언어만 공부했다고 끝인가? NO!프로그래밍 언어를 알았으니 활용하는 방법도 공부하셔야 합니다이러한 과정을 프로그래밍 한다고 하는데프로그래밍을 할때도 남들이 써놓은 코딩기법을 달달 외우는게 아니라직접 코딩을 하면서 에러도 많이나고이러한 에러점을 찾는 과정에서 학습이 엄청 많이 되십니다기초적인 부분은 실무기술이 가장 중요한 분야이기 때문에실무에서 어떠한 기술을 쓰는지 배우셔야 합니다지식인이라는 틀안에서 정보를 다드리기에는 한계점이 있네요ㅠㅠ저는 현재 취업율&자격증 취득율 1위인IT전문교육기관 교육부팀장으로써 많은 학생들의현실에 맞는 앞으로의 계획과 공부방법등에 대해 도움을 드리고 있습니다네이버 정책상 자세한 언급은 제한되오니궁금한 사항이 있으시다 하시면 네임카드 참고하여 연락주시면질문자님의 고민 성심성의껏 해결해드리겠습니다 :)▼ 청춘상담소 ▼▼ 오픈카톡 ▼\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육전문컨설턴트 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 IT 전문교육기관에 근무하고 있는 교육상담선생님 더블유쌤입니다.많은 고민들을 하셔서 개인적으로 쉽지 않은 선택을 하시는 분 또는 해당분야에 궁금한 점을 해결하기 위해 조금이나마 도움이 되고자 하는 생각에 답변을 드립니다.질문자께서 어떤 분야를 얼마나 배우셨는지 또는 학력이 어디까지인지 써 주시지 않아 더 깊게 말씀 드리지 못하는 부분에 대해서는 양해 말씀드립니다. 프로그래머는 소프트웨어 언어를 기반으로 자기 자신의 창의적인 내용을 더해프로그램을 설계, 작성하고 테스트하는 역할까지 만능으로 하는 기술자입니다. 또한 기존의 프로그램을 수정하고 유지 보수하는 일도하기 때문에 기능적인 일이기도 합니다. 프로그래머가 이런 기본적인 일들을 하려면 기본적인 프로그래밍 능력이 되어야 합니다. 프로그래밍의 언어를 숙달한 이후에 프로그래밍, 네트워크, 시스템 등의다양한 분야에 대한 기초지식을 쌓은 후 각 분야별로 전문적인 지식으로더 깊이 들어가야 하고 자격증 등을 취득하면서 기반을 만드는 것을 하셔야합니다. 프로그래머의 종류로는 응용, 시스템, 웹, 게임, 보안 프로그래머가각각 있으며 대기업으로 취업하기에 많은 도움이 되는 직업이기도 합니다. 프로그래머의 길은 다방면의 컴퓨터 기술에 능해야 하므로전문적인 기관에서 배울 필요가 있습니다.현재 상황에서 아무것도 모르는데 기초 지식 없이 다른 사람들이 글을올린 것을 바탕으로만 하여 게임, 코딩, 프로그래밍, 정보보안 등 이런 것에 손을 대면 하나도 이해 못하는 것이 당연합니다. 일반적으로 프로그래밍을 하기 위해서 C언어나 JAVA, 파이썬 등 언어 등을 먼저 시작합니다.대부분 사람들이 가장 기본으로 생각하고 있는 것이기도 하구요. 왜냐하면 C언어나 JAVA는 한글을 배움에 있어서 ㄱ,ㄴ,ㄷ,ㄹ.. 가,나,다,라를 배우는 것과 같습니다.즉 프로그래밍이 아니라 프로그래밍을 구성하는 언어이지요. 모든 프로그램을 다루기 위해서는 프로그래밍 언어를 먼저 아는 것이 중요합니다. 언어를 배우고 나서야 네트워크나 시스템 데이터 베이스 등을 배울 수 있는 근간이 됩니다. 이 언어 등을 통해 프로그램을 만드는 것을 ‘코딩’ 이라고 하며 코딩과 같은 말이 프로그래밍이라고 보시면 됩니다. 이제 여기서부터 어떻게 배우는 가입니다. 언어를 배웠으면어떻게 사용할 것인지 또는 특화 시킬 것인지가 중요하다고 볼 수 있습니다. 프로그램언어는 C언어와 JAVA를 더 심화시켜 사용하기도 합니다.예를 들어 데이터들을 어떠한 기억장치에 대해 저장, 읽기, 쓰기 등을 배우는 것을 자료구조라고 합니다. 이러한 것처럼 더욱 심화 시킨 프로그래밍 언어를 기반으로 하여단계적으로 나아가는 것이 IT 기술을 배우는 것에 기본이라고 할 수 있습니다. 프로그래밍를 배우게 되면 단계적으로 배워 나아가면서머리로 하는 것이 아니라 몸이 반응 하는 것인 ‘체화’가 될 것입니다. 하지만 이 ‘체화’가 되기까지 그리고 체화가 되고 나서그 윗 단계로 가는 것에 대해 독학 등은 상당한 무리가 있습니다.  질문에 대한 답변이외에 더 궁금하신 점이 있으시면 언제든지오픈채팅 주소 혹은 배너를 통해 문의주세요무료상담을 진행하고 있습니다 https://open.kakao.com/o/sts2Ylcbhttps://itbank-study.edueroom.co.kr/http://naver.me/5n3w6VfR\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "취업진로지도전문가 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 질문자님 KG 에듀원 컨설턴트입니다~우선 머신러닝과 빅데이터 같은경우는 머신러닝은 빅데이터 기반에 많이 쓰입니다~그리고 프론트엔드 & 백엔드 같은경우는 웹개발자의 종류에 속합니다~프론트 엔드는 웹페이지의 앞면을 꾸미는 직업이고백엔드는 뒷면인 웹페이지의 뼈대를 구축하는 직업입니다~그리고 이 두개를 다할 줄 아는 풀스택개발자도 있습니다~빅데이터와 머신러닝을 공부하시려면 우선은 파이썬 - R - 하둡 이렇게 공부하시면 될듯합니다~그리고 나서 머신러닝을 공부하시는것도 좋구요~그러면 빅데이터 쪽으로 진출을 하실 수 있습니다.https://blog.naver.com/dkakfndj1/221496760461 웹개발자 공부는 어떻게 하는지 제가 운영하는 블로그입니다~참고 해주시구요~궁금한점이있으시면 아래 링크로 문의주세요~http://naver.me/FA6TrelPhttps://open.kakao.com/o/suNluecb\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "취업진로지도전문가 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 교육전문컨설턴트 전동문 학과주임선생님입니다.질문에 팩트로만 답변드리겠습니다.  간단하게 IT 직업별 업무에 대해 설명드릴테니 자세한 내용은 문의주세요.IT계열은 크게 프로그래밍 / 네트워크 / 보안 / 시스템 / 데이터베이스로나눌 수 있고 각 분야별로 세부직업군이 다양하게 존재합니다. 1. 프로그래머웹 개발자(웹 프로그래머)는 프런트엔드, 백엔드, 풀스택 개발자가 있고프런트엔드는 웹 디자이너로써 웹 사이트의 제작 및 관리하는 직업이며백엔드는 사용자가 볼 수 없는 서버를 개발 및 담당하는 직업입니다.마지막으로 풀스택 개발자는 프런트엔드와 백엔드 개발을전부 할 수 있는 직업으로 웹 개발의 만능 엔터테이너라고 보면 됩니다.응용프로그래머는 응용sw프로그램을 개발하는 직업으로예를 들어보면 컴퓨터 바탕화면에 보이는 곰플레이어, 액셀, 알약 등의실행가능한 모든 sw프로그램들을 개발하는 일을 합니다.게임프로그래머는 말 그대로 게임을 개발하는 직업으로프로그래밍, 서버, 시스템 등에 대한 지식이 필요합니다. 2. 엔지니어네트워크 엔지니어, 시스템 엔지니어, 서버 엔지니어 등이 있으며네트워크 엔지니어는 전산망과 관련된 네트워크 시스템을분석, 설계 및 구축하는 직업입니다.시스템 엔지니어는 통신장비를 시스템에 맞게 설계 및 관리하는 직업이고서버 엔지니어는 웹서버 구축 및 운영을 기술적으로 다루는 직업입니다. 3. 데이터베이스데이터베이스 분야는 데이터베이스 관리자, 솔루션 아키텍쳐,빅데이터전문가, 빅데이터 분석가가 있습니다.데이터베이스 관리자는 인터넷을 통해 수집되는 정보, 데이터들을수집 및 정리하여 분석한 다음 데이터베이스를 구축하는 직업이고솔루션 아키텍쳐는 기업의 시스템 환경을 AWS환경으로변경할 수 있도록 설계해주는 직업입니다. 4. 보안 / 해킹정보보안 분야는 디지털포렌식전문가, 보안솔루션, 악성코드전문가,모의해킹전문가, 보안관제, 침해대응 등 다양한 직업이 있습니다.디지털포렌식전문가는 손상된 데이터를 복구 및 복원하는 직업이며주로 경찰청이나 사이버 안전국에서 근무합니다.보안솔루션은 기업의 IT 정보보호를 목적으로 모의해킹을 통해취약점을 분석하고 대비하는 직업입니다.악성코드전문가는 기존에 있던 악성코드를 분석하여 백신 또는보안프로그램을 개발하는 직업입니다.모의해킹전문가는 웹이나 서버를 모의해킹하여 발생할 수 있는모든 해킹에 대비하여 취약점을 분석하는 직업입니다.보안관제는 지속적인 모니터링으로 1차적으로 해킹에 대응하는 직업이며침해대응은 해커의 침투로 피해가 생길 시 데이터를 복구하고범인을 역추적하는 역할을 하는 직업입니다. < IT 직업 >https://blog.naver.com/11glgl/221464165714  현재 무료로 진로 및 취업 컨설팅을 진행하고 있으니질문자님이 좀 더 자세한 정보와 도움을 원하거나정확한 진단을 받은 후 적절한 공부법과 계획에 대해알고 싶다면 문의주셔서 정확한 도움 받아보시기 바랍니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "취업진로지도전문가 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요.IT정보통신기술 분야지식IN 전문가랭킹 1위취업진로지도전문가 입니다.질문에 답변드리겠습니다.JAVA, 파이썬, 알고리즘은 프로그래밍 코딩 작업을 할때 꼭 필요한 언어 입니다.머신러닝, 빅데이터, 프론트엔드&백엔드 개발자 모두 범용적으로 사용하는 기술입니다.SW역량테스트는 기업에서 입사시 지원자들을 통해 면접시에 SW 실무능력을 평가하는 시험이고 기업마다 보는 언어 및 난이도의 차이가 있습니다.지식iN 글을 보시고 연락주시는 분들 대상으로 [ 무료 진로/취업/교육 맞춤 컨설팅 ] 을 진행해드리고 있습니다. 머신러닝/빅데이터/프론트엔드&백엔드 개발자 진로, SW역량테스트에 대해서 컨설팅을 받기를 희망하신다면, 상단의 네임카드 연락처를 통해 연락주시거나, 하단의 실시간상담&배너를 클릭하셔서 문의 남겨 주시면 상세한 1:1 맞춤 컨설팅 도와드리도록 하겠습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "취업진로지도전문가 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "프로그래밍(코딩)공부는일반적인 공부와는 틀립니다프로그래밍은 IT기술직입니다이러한 기술직 공부는책 위주의 이론식 학습이 되시면크게 도움이 안됩니다일단 프로그래밍의 개념부터잠시 짚고 넘어가보면프로그래밍이란 코딩이라고도 불리우며사람이 컴퓨터에게 명령을 하달명령을 받은 컴퓨터가동작하고 실행하는 원리입니다자! 이렇게 컴퓨터와 대화를 해야 하기위해사람과 컴퓨터를 이어주는 컴퓨터언어인프로그래밍언어를 필수적으로 공부하셔야 하는데요이러한 프로그래밍 언어도 종류가 여러 가지가 있죠?c언어, c++, c#, Java, 파이썬 등등..각자 언어들마다 만든 회사도 틀리며각 언어들마다 특장점이 틀립니다그리고 회사마다 주로 쓰는 언어가 틀리기 때문에프로그래밍 언어를 최대한 많이 섭렵하시는게 좋으십니다그리고 프로그래밍 언어만 공부했다고 끝인가? NO!프로그래밍 언어를 알았으니 활용하는 방법도 공부하셔야 합니다이러한 과정을 프로그래밍 한다고 하는데프로그래밍을 할때도 남들이 써놓은 코딩기법을 달달 외우는게 아니라직접 코딩을 하면서 에러도 많이나고이러한 에러점을 찾는 과정에서 학습이 엄청 많이 되십니다기초적인 부분은 실무기술이 가장 중요한 분야이기 때문에실무에서 어떠한 기술을 쓰는지 배우셔야 합니다지식인이라는 틀안에서 정보를 다드리기에는 한계점이 있네요ㅠㅠ저는 현재 취업율&자격증 취득율 1위인IT전문교육기관 교육부팀장으로써 많은 학생들의현실에 맞는 앞으로의 계획과 공부방법등에 대해 도움을 드리고 있습니다네이버 정책상 자세한 언급은 제한되오니궁금한 사항이 있으시다 하시면 네임카드 참고하여 연락주시면질문자님의 고민 성심성의껏 해결해드리겠습니다 :)▼ 청춘상담소 ▼▼ 오픈카톡 ▼\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "취업진로지도전문가 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 IT 전문교육기관에 근무하고 있는 교육상담선생님 더블유쌤입니다.많은 고민들을 하셔서 개인적으로 쉽지 않은 선택을 하시는 분 또는 해당분야에 궁금한 점을 해결하기 위해 조금이나마 도움이 되고자 하는 생각에 답변을 드립니다.질문자께서 어떤 분야를 얼마나 배우셨는지 또는 학력이 어디까지인지 써 주시지 않아 더 깊게 말씀 드리지 못하는 부분에 대해서는 양해 말씀드립니다. 프로그래머는 소프트웨어 언어를 기반으로 자기 자신의 창의적인 내용을 더해프로그램을 설계, 작성하고 테스트하는 역할까지 만능으로 하는 기술자입니다. 또한 기존의 프로그램을 수정하고 유지 보수하는 일도하기 때문에 기능적인 일이기도 합니다. 프로그래머가 이런 기본적인 일들을 하려면 기본적인 프로그래밍 능력이 되어야 합니다. 프로그래밍의 언어를 숙달한 이후에 프로그래밍, 네트워크, 시스템 등의다양한 분야에 대한 기초지식을 쌓은 후 각 분야별로 전문적인 지식으로더 깊이 들어가야 하고 자격증 등을 취득하면서 기반을 만드는 것을 하셔야합니다. 프로그래머의 종류로는 응용, 시스템, 웹, 게임, 보안 프로그래머가각각 있으며 대기업으로 취업하기에 많은 도움이 되는 직업이기도 합니다. 프로그래머의 길은 다방면의 컴퓨터 기술에 능해야 하므로전문적인 기관에서 배울 필요가 있습니다.현재 상황에서 아무것도 모르는데 기초 지식 없이 다른 사람들이 글을올린 것을 바탕으로만 하여 게임, 코딩, 프로그래밍, 정보보안 등 이런 것에 손을 대면 하나도 이해 못하는 것이 당연합니다. 일반적으로 프로그래밍을 하기 위해서 C언어나 JAVA, 파이썬 등 언어 등을 먼저 시작합니다.대부분 사람들이 가장 기본으로 생각하고 있는 것이기도 하구요. 왜냐하면 C언어나 JAVA는 한글을 배움에 있어서 ㄱ,ㄴ,ㄷ,ㄹ.. 가,나,다,라를 배우는 것과 같습니다.즉 프로그래밍이 아니라 프로그래밍을 구성하는 언어이지요. 모든 프로그램을 다루기 위해서는 프로그래밍 언어를 먼저 아는 것이 중요합니다. 언어를 배우고 나서야 네트워크나 시스템 데이터 베이스 등을 배울 수 있는 근간이 됩니다. 이 언어 등을 통해 프로그램을 만드는 것을 ‘코딩’ 이라고 하며 코딩과 같은 말이 프로그래밍이라고 보시면 됩니다. 이제 여기서부터 어떻게 배우는 가입니다. 언어를 배웠으면어떻게 사용할 것인지 또는 특화 시킬 것인지가 중요하다고 볼 수 있습니다. 프로그램언어는 C언어와 JAVA를 더 심화시켜 사용하기도 합니다.예를 들어 데이터들을 어떠한 기억장치에 대해 저장, 읽기, 쓰기 등을 배우는 것을 자료구조라고 합니다. 이러한 것처럼 더욱 심화 시킨 프로그래밍 언어를 기반으로 하여단계적으로 나아가는 것이 IT 기술을 배우는 것에 기본이라고 할 수 있습니다. 프로그래밍를 배우게 되면 단계적으로 배워 나아가면서머리로 하는 것이 아니라 몸이 반응 하는 것인 ‘체화’가 될 것입니다. 하지만 이 ‘체화’가 되기까지 그리고 체화가 되고 나서그 윗 단계로 가는 것에 대해 독학 등은 상당한 무리가 있습니다.  질문에 대한 답변이외에 더 궁금하신 점이 있으시면 언제든지오픈채팅 주소 혹은 배너를 통해 문의주세요무료상담을 진행하고 있습니다 https://open.kakao.com/o/sts2Ylcbhttps://itbank-study.edueroom.co.kr/http://naver.me/5n3w6VfR\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "청춘상담소 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 질문자님 KG 에듀원 컨설턴트입니다~우선 머신러닝과 빅데이터 같은경우는 머신러닝은 빅데이터 기반에 많이 쓰입니다~그리고 프론트엔드 & 백엔드 같은경우는 웹개발자의 종류에 속합니다~프론트 엔드는 웹페이지의 앞면을 꾸미는 직업이고백엔드는 뒷면인 웹페이지의 뼈대를 구축하는 직업입니다~그리고 이 두개를 다할 줄 아는 풀스택개발자도 있습니다~빅데이터와 머신러닝을 공부하시려면 우선은 파이썬 - R - 하둡 이렇게 공부하시면 될듯합니다~그리고 나서 머신러닝을 공부하시는것도 좋구요~그러면 빅데이터 쪽으로 진출을 하실 수 있습니다.https://blog.naver.com/dkakfndj1/221496760461 웹개발자 공부는 어떻게 하는지 제가 운영하는 블로그입니다~참고 해주시구요~궁금한점이있으시면 아래 링크로 문의주세요~http://naver.me/FA6TrelPhttps://open.kakao.com/o/suNluecb\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "청춘상담소 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 교육전문컨설턴트 전동문 학과주임선생님입니다.질문에 팩트로만 답변드리겠습니다.  간단하게 IT 직업별 업무에 대해 설명드릴테니 자세한 내용은 문의주세요.IT계열은 크게 프로그래밍 / 네트워크 / 보안 / 시스템 / 데이터베이스로나눌 수 있고 각 분야별로 세부직업군이 다양하게 존재합니다. 1. 프로그래머웹 개발자(웹 프로그래머)는 프런트엔드, 백엔드, 풀스택 개발자가 있고프런트엔드는 웹 디자이너로써 웹 사이트의 제작 및 관리하는 직업이며백엔드는 사용자가 볼 수 없는 서버를 개발 및 담당하는 직업입니다.마지막으로 풀스택 개발자는 프런트엔드와 백엔드 개발을전부 할 수 있는 직업으로 웹 개발의 만능 엔터테이너라고 보면 됩니다.응용프로그래머는 응용sw프로그램을 개발하는 직업으로예를 들어보면 컴퓨터 바탕화면에 보이는 곰플레이어, 액셀, 알약 등의실행가능한 모든 sw프로그램들을 개발하는 일을 합니다.게임프로그래머는 말 그대로 게임을 개발하는 직업으로프로그래밍, 서버, 시스템 등에 대한 지식이 필요합니다. 2. 엔지니어네트워크 엔지니어, 시스템 엔지니어, 서버 엔지니어 등이 있으며네트워크 엔지니어는 전산망과 관련된 네트워크 시스템을분석, 설계 및 구축하는 직업입니다.시스템 엔지니어는 통신장비를 시스템에 맞게 설계 및 관리하는 직업이고서버 엔지니어는 웹서버 구축 및 운영을 기술적으로 다루는 직업입니다. 3. 데이터베이스데이터베이스 분야는 데이터베이스 관리자, 솔루션 아키텍쳐,빅데이터전문가, 빅데이터 분석가가 있습니다.데이터베이스 관리자는 인터넷을 통해 수집되는 정보, 데이터들을수집 및 정리하여 분석한 다음 데이터베이스를 구축하는 직업이고솔루션 아키텍쳐는 기업의 시스템 환경을 AWS환경으로변경할 수 있도록 설계해주는 직업입니다. 4. 보안 / 해킹정보보안 분야는 디지털포렌식전문가, 보안솔루션, 악성코드전문가,모의해킹전문가, 보안관제, 침해대응 등 다양한 직업이 있습니다.디지털포렌식전문가는 손상된 데이터를 복구 및 복원하는 직업이며주로 경찰청이나 사이버 안전국에서 근무합니다.보안솔루션은 기업의 IT 정보보호를 목적으로 모의해킹을 통해취약점을 분석하고 대비하는 직업입니다.악성코드전문가는 기존에 있던 악성코드를 분석하여 백신 또는보안프로그램을 개발하는 직업입니다.모의해킹전문가는 웹이나 서버를 모의해킹하여 발생할 수 있는모든 해킹에 대비하여 취약점을 분석하는 직업입니다.보안관제는 지속적인 모니터링으로 1차적으로 해킹에 대응하는 직업이며침해대응은 해커의 침투로 피해가 생길 시 데이터를 복구하고범인을 역추적하는 역할을 하는 직업입니다. < IT 직업 >https://blog.naver.com/11glgl/221464165714  현재 무료로 진로 및 취업 컨설팅을 진행하고 있으니질문자님이 좀 더 자세한 정보와 도움을 원하거나정확한 진단을 받은 후 적절한 공부법과 계획에 대해알고 싶다면 문의주셔서 정확한 도움 받아보시기 바랍니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "청춘상담소 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요.IT정보통신기술 분야지식IN 전문가랭킹 1위취업진로지도전문가 입니다.질문에 답변드리겠습니다.JAVA, 파이썬, 알고리즘은 프로그래밍 코딩 작업을 할때 꼭 필요한 언어 입니다.머신러닝, 빅데이터, 프론트엔드&백엔드 개발자 모두 범용적으로 사용하는 기술입니다.SW역량테스트는 기업에서 입사시 지원자들을 통해 면접시에 SW 실무능력을 평가하는 시험이고 기업마다 보는 언어 및 난이도의 차이가 있습니다.지식iN 글을 보시고 연락주시는 분들 대상으로 [ 무료 진로/취업/교육 맞춤 컨설팅 ] 을 진행해드리고 있습니다. 머신러닝/빅데이터/프론트엔드&백엔드 개발자 진로, SW역량테스트에 대해서 컨설팅을 받기를 희망하신다면, 상단의 네임카드 연락처를 통해 연락주시거나, 하단의 실시간상담&배너를 클릭하셔서 문의 남겨 주시면 상세한 1:1 맞춤 컨설팅 도와드리도록 하겠습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "청춘상담소 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "프로그래밍(코딩)공부는일반적인 공부와는 틀립니다프로그래밍은 IT기술직입니다이러한 기술직 공부는책 위주의 이론식 학습이 되시면크게 도움이 안됩니다일단 프로그래밍의 개념부터잠시 짚고 넘어가보면프로그래밍이란 코딩이라고도 불리우며사람이 컴퓨터에게 명령을 하달명령을 받은 컴퓨터가동작하고 실행하는 원리입니다자! 이렇게 컴퓨터와 대화를 해야 하기위해사람과 컴퓨터를 이어주는 컴퓨터언어인프로그래밍언어를 필수적으로 공부하셔야 하는데요이러한 프로그래밍 언어도 종류가 여러 가지가 있죠?c언어, c++, c#, Java, 파이썬 등등..각자 언어들마다 만든 회사도 틀리며각 언어들마다 특장점이 틀립니다그리고 회사마다 주로 쓰는 언어가 틀리기 때문에프로그래밍 언어를 최대한 많이 섭렵하시는게 좋으십니다그리고 프로그래밍 언어만 공부했다고 끝인가? NO!프로그래밍 언어를 알았으니 활용하는 방법도 공부하셔야 합니다이러한 과정을 프로그래밍 한다고 하는데프로그래밍을 할때도 남들이 써놓은 코딩기법을 달달 외우는게 아니라직접 코딩을 하면서 에러도 많이나고이러한 에러점을 찾는 과정에서 학습이 엄청 많이 되십니다기초적인 부분은 실무기술이 가장 중요한 분야이기 때문에실무에서 어떠한 기술을 쓰는지 배우셔야 합니다지식인이라는 틀안에서 정보를 다드리기에는 한계점이 있네요ㅠㅠ저는 현재 취업율&자격증 취득율 1위인IT전문교육기관 교육부팀장으로써 많은 학생들의현실에 맞는 앞으로의 계획과 공부방법등에 대해 도움을 드리고 있습니다네이버 정책상 자세한 언급은 제한되오니궁금한 사항이 있으시다 하시면 네임카드 참고하여 연락주시면질문자님의 고민 성심성의껏 해결해드리겠습니다 :)▼ 청춘상담소 ▼▼ 오픈카톡 ▼\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "청춘상담소 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 IT 전문교육기관에 근무하고 있는 교육상담선생님 더블유쌤입니다.많은 고민들을 하셔서 개인적으로 쉽지 않은 선택을 하시는 분 또는 해당분야에 궁금한 점을 해결하기 위해 조금이나마 도움이 되고자 하는 생각에 답변을 드립니다.질문자께서 어떤 분야를 얼마나 배우셨는지 또는 학력이 어디까지인지 써 주시지 않아 더 깊게 말씀 드리지 못하는 부분에 대해서는 양해 말씀드립니다. 프로그래머는 소프트웨어 언어를 기반으로 자기 자신의 창의적인 내용을 더해프로그램을 설계, 작성하고 테스트하는 역할까지 만능으로 하는 기술자입니다. 또한 기존의 프로그램을 수정하고 유지 보수하는 일도하기 때문에 기능적인 일이기도 합니다. 프로그래머가 이런 기본적인 일들을 하려면 기본적인 프로그래밍 능력이 되어야 합니다. 프로그래밍의 언어를 숙달한 이후에 프로그래밍, 네트워크, 시스템 등의다양한 분야에 대한 기초지식을 쌓은 후 각 분야별로 전문적인 지식으로더 깊이 들어가야 하고 자격증 등을 취득하면서 기반을 만드는 것을 하셔야합니다. 프로그래머의 종류로는 응용, 시스템, 웹, 게임, 보안 프로그래머가각각 있으며 대기업으로 취업하기에 많은 도움이 되는 직업이기도 합니다. 프로그래머의 길은 다방면의 컴퓨터 기술에 능해야 하므로전문적인 기관에서 배울 필요가 있습니다.현재 상황에서 아무것도 모르는데 기초 지식 없이 다른 사람들이 글을올린 것을 바탕으로만 하여 게임, 코딩, 프로그래밍, 정보보안 등 이런 것에 손을 대면 하나도 이해 못하는 것이 당연합니다. 일반적으로 프로그래밍을 하기 위해서 C언어나 JAVA, 파이썬 등 언어 등을 먼저 시작합니다.대부분 사람들이 가장 기본으로 생각하고 있는 것이기도 하구요. 왜냐하면 C언어나 JAVA는 한글을 배움에 있어서 ㄱ,ㄴ,ㄷ,ㄹ.. 가,나,다,라를 배우는 것과 같습니다.즉 프로그래밍이 아니라 프로그래밍을 구성하는 언어이지요. 모든 프로그램을 다루기 위해서는 프로그래밍 언어를 먼저 아는 것이 중요합니다. 언어를 배우고 나서야 네트워크나 시스템 데이터 베이스 등을 배울 수 있는 근간이 됩니다. 이 언어 등을 통해 프로그램을 만드는 것을 ‘코딩’ 이라고 하며 코딩과 같은 말이 프로그래밍이라고 보시면 됩니다. 이제 여기서부터 어떻게 배우는 가입니다. 언어를 배웠으면어떻게 사용할 것인지 또는 특화 시킬 것인지가 중요하다고 볼 수 있습니다. 프로그램언어는 C언어와 JAVA를 더 심화시켜 사용하기도 합니다.예를 들어 데이터들을 어떠한 기억장치에 대해 저장, 읽기, 쓰기 등을 배우는 것을 자료구조라고 합니다. 이러한 것처럼 더욱 심화 시킨 프로그래밍 언어를 기반으로 하여단계적으로 나아가는 것이 IT 기술을 배우는 것에 기본이라고 할 수 있습니다. 프로그래밍를 배우게 되면 단계적으로 배워 나아가면서머리로 하는 것이 아니라 몸이 반응 하는 것인 ‘체화’가 될 것입니다. 하지만 이 ‘체화’가 되기까지 그리고 체화가 되고 나서그 윗 단계로 가는 것에 대해 독학 등은 상당한 무리가 있습니다.  질문에 대한 답변이외에 더 궁금하신 점이 있으시면 언제든지오픈채팅 주소 혹은 배너를 통해 문의주세요무료상담을 진행하고 있습니다 https://open.kakao.com/o/sts2Ylcbhttps://itbank-study.edueroom.co.kr/http://naver.me/5n3w6VfR\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육컨설턴트 욱쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요~ 질문자님 KG 에듀원 컨설턴트입니다~우선 머신러닝과 빅데이터 같은경우는 머신러닝은 빅데이터 기반에 많이 쓰입니다~그리고 프론트엔드 & 백엔드 같은경우는 웹개발자의 종류에 속합니다~프론트 엔드는 웹페이지의 앞면을 꾸미는 직업이고백엔드는 뒷면인 웹페이지의 뼈대를 구축하는 직업입니다~그리고 이 두개를 다할 줄 아는 풀스택개발자도 있습니다~빅데이터와 머신러닝을 공부하시려면 우선은 파이썬 - R - 하둡 이렇게 공부하시면 될듯합니다~그리고 나서 머신러닝을 공부하시는것도 좋구요~그러면 빅데이터 쪽으로 진출을 하실 수 있습니다.https://blog.naver.com/dkakfndj1/221496760461 웹개발자 공부는 어떻게 하는지 제가 운영하는 블로그입니다~참고 해주시구요~궁금한점이있으시면 아래 링크로 문의주세요~http://naver.me/FA6TrelPhttps://open.kakao.com/o/suNluecb\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육컨설턴트 욱쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 교육전문컨설턴트 전동문 학과주임선생님입니다.질문에 팩트로만 답변드리겠습니다.  간단하게 IT 직업별 업무에 대해 설명드릴테니 자세한 내용은 문의주세요.IT계열은 크게 프로그래밍 / 네트워크 / 보안 / 시스템 / 데이터베이스로나눌 수 있고 각 분야별로 세부직업군이 다양하게 존재합니다. 1. 프로그래머웹 개발자(웹 프로그래머)는 프런트엔드, 백엔드, 풀스택 개발자가 있고프런트엔드는 웹 디자이너로써 웹 사이트의 제작 및 관리하는 직업이며백엔드는 사용자가 볼 수 없는 서버를 개발 및 담당하는 직업입니다.마지막으로 풀스택 개발자는 프런트엔드와 백엔드 개발을전부 할 수 있는 직업으로 웹 개발의 만능 엔터테이너라고 보면 됩니다.응용프로그래머는 응용sw프로그램을 개발하는 직업으로예를 들어보면 컴퓨터 바탕화면에 보이는 곰플레이어, 액셀, 알약 등의실행가능한 모든 sw프로그램들을 개발하는 일을 합니다.게임프로그래머는 말 그대로 게임을 개발하는 직업으로프로그래밍, 서버, 시스템 등에 대한 지식이 필요합니다. 2. 엔지니어네트워크 엔지니어, 시스템 엔지니어, 서버 엔지니어 등이 있으며네트워크 엔지니어는 전산망과 관련된 네트워크 시스템을분석, 설계 및 구축하는 직업입니다.시스템 엔지니어는 통신장비를 시스템에 맞게 설계 및 관리하는 직업이고서버 엔지니어는 웹서버 구축 및 운영을 기술적으로 다루는 직업입니다. 3. 데이터베이스데이터베이스 분야는 데이터베이스 관리자, 솔루션 아키텍쳐,빅데이터전문가, 빅데이터 분석가가 있습니다.데이터베이스 관리자는 인터넷을 통해 수집되는 정보, 데이터들을수집 및 정리하여 분석한 다음 데이터베이스를 구축하는 직업이고솔루션 아키텍쳐는 기업의 시스템 환경을 AWS환경으로변경할 수 있도록 설계해주는 직업입니다. 4. 보안 / 해킹정보보안 분야는 디지털포렌식전문가, 보안솔루션, 악성코드전문가,모의해킹전문가, 보안관제, 침해대응 등 다양한 직업이 있습니다.디지털포렌식전문가는 손상된 데이터를 복구 및 복원하는 직업이며주로 경찰청이나 사이버 안전국에서 근무합니다.보안솔루션은 기업의 IT 정보보호를 목적으로 모의해킹을 통해취약점을 분석하고 대비하는 직업입니다.악성코드전문가는 기존에 있던 악성코드를 분석하여 백신 또는보안프로그램을 개발하는 직업입니다.모의해킹전문가는 웹이나 서버를 모의해킹하여 발생할 수 있는모든 해킹에 대비하여 취약점을 분석하는 직업입니다.보안관제는 지속적인 모니터링으로 1차적으로 해킹에 대응하는 직업이며침해대응은 해커의 침투로 피해가 생길 시 데이터를 복구하고범인을 역추적하는 역할을 하는 직업입니다. < IT 직업 >https://blog.naver.com/11glgl/221464165714  현재 무료로 진로 및 취업 컨설팅을 진행하고 있으니질문자님이 좀 더 자세한 정보와 도움을 원하거나정확한 진단을 받은 후 적절한 공부법과 계획에 대해알고 싶다면 문의주셔서 정확한 도움 받아보시기 바랍니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육컨설턴트 욱쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요.IT정보통신기술 분야지식IN 전문가랭킹 1위취업진로지도전문가 입니다.질문에 답변드리겠습니다.JAVA, 파이썬, 알고리즘은 프로그래밍 코딩 작업을 할때 꼭 필요한 언어 입니다.머신러닝, 빅데이터, 프론트엔드&백엔드 개발자 모두 범용적으로 사용하는 기술입니다.SW역량테스트는 기업에서 입사시 지원자들을 통해 면접시에 SW 실무능력을 평가하는 시험이고 기업마다 보는 언어 및 난이도의 차이가 있습니다.지식iN 글을 보시고 연락주시는 분들 대상으로 [ 무료 진로/취업/교육 맞춤 컨설팅 ] 을 진행해드리고 있습니다. 머신러닝/빅데이터/프론트엔드&백엔드 개발자 진로, SW역량테스트에 대해서 컨설팅을 받기를 희망하신다면, 상단의 네임카드 연락처를 통해 연락주시거나, 하단의 실시간상담&배너를 클릭하셔서 문의 남겨 주시면 상세한 1:1 맞춤 컨설팅 도와드리도록 하겠습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육컨설턴트 욱쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "프로그래밍(코딩)공부는일반적인 공부와는 틀립니다프로그래밍은 IT기술직입니다이러한 기술직 공부는책 위주의 이론식 학습이 되시면크게 도움이 안됩니다일단 프로그래밍의 개념부터잠시 짚고 넘어가보면프로그래밍이란 코딩이라고도 불리우며사람이 컴퓨터에게 명령을 하달명령을 받은 컴퓨터가동작하고 실행하는 원리입니다자! 이렇게 컴퓨터와 대화를 해야 하기위해사람과 컴퓨터를 이어주는 컴퓨터언어인프로그래밍언어를 필수적으로 공부하셔야 하는데요이러한 프로그래밍 언어도 종류가 여러 가지가 있죠?c언어, c++, c#, Java, 파이썬 등등..각자 언어들마다 만든 회사도 틀리며각 언어들마다 특장점이 틀립니다그리고 회사마다 주로 쓰는 언어가 틀리기 때문에프로그래밍 언어를 최대한 많이 섭렵하시는게 좋으십니다그리고 프로그래밍 언어만 공부했다고 끝인가? NO!프로그래밍 언어를 알았으니 활용하는 방법도 공부하셔야 합니다이러한 과정을 프로그래밍 한다고 하는데프로그래밍을 할때도 남들이 써놓은 코딩기법을 달달 외우는게 아니라직접 코딩을 하면서 에러도 많이나고이러한 에러점을 찾는 과정에서 학습이 엄청 많이 되십니다기초적인 부분은 실무기술이 가장 중요한 분야이기 때문에실무에서 어떠한 기술을 쓰는지 배우셔야 합니다지식인이라는 틀안에서 정보를 다드리기에는 한계점이 있네요ㅠㅠ저는 현재 취업율&자격증 취득율 1위인IT전문교육기관 교육부팀장으로써 많은 학생들의현실에 맞는 앞으로의 계획과 공부방법등에 대해 도움을 드리고 있습니다네이버 정책상 자세한 언급은 제한되오니궁금한 사항이 있으시다 하시면 네임카드 참고하여 연락주시면질문자님의 고민 성심성의껏 해결해드리겠습니다 :)▼ 청춘상담소 ▼▼ 오픈카톡 ▼\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "교육컨설턴트 욱쌤 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "안녕하세요 IT 전문교육기관에 근무하고 있는 교육상담선생님 더블유쌤입니다.많은 고민들을 하셔서 개인적으로 쉽지 않은 선택을 하시는 분 또는 해당분야에 궁금한 점을 해결하기 위해 조금이나마 도움이 되고자 하는 생각에 답변을 드립니다.질문자께서 어떤 분야를 얼마나 배우셨는지 또는 학력이 어디까지인지 써 주시지 않아 더 깊게 말씀 드리지 못하는 부분에 대해서는 양해 말씀드립니다. 프로그래머는 소프트웨어 언어를 기반으로 자기 자신의 창의적인 내용을 더해프로그램을 설계, 작성하고 테스트하는 역할까지 만능으로 하는 기술자입니다. 또한 기존의 프로그램을 수정하고 유지 보수하는 일도하기 때문에 기능적인 일이기도 합니다. 프로그래머가 이런 기본적인 일들을 하려면 기본적인 프로그래밍 능력이 되어야 합니다. 프로그래밍의 언어를 숙달한 이후에 프로그래밍, 네트워크, 시스템 등의다양한 분야에 대한 기초지식을 쌓은 후 각 분야별로 전문적인 지식으로더 깊이 들어가야 하고 자격증 등을 취득하면서 기반을 만드는 것을 하셔야합니다. 프로그래머의 종류로는 응용, 시스템, 웹, 게임, 보안 프로그래머가각각 있으며 대기업으로 취업하기에 많은 도움이 되는 직업이기도 합니다. 프로그래머의 길은 다방면의 컴퓨터 기술에 능해야 하므로전문적인 기관에서 배울 필요가 있습니다.현재 상황에서 아무것도 모르는데 기초 지식 없이 다른 사람들이 글을올린 것을 바탕으로만 하여 게임, 코딩, 프로그래밍, 정보보안 등 이런 것에 손을 대면 하나도 이해 못하는 것이 당연합니다. 일반적으로 프로그래밍을 하기 위해서 C언어나 JAVA, 파이썬 등 언어 등을 먼저 시작합니다.대부분 사람들이 가장 기본으로 생각하고 있는 것이기도 하구요. 왜냐하면 C언어나 JAVA는 한글을 배움에 있어서 ㄱ,ㄴ,ㄷ,ㄹ.. 가,나,다,라를 배우는 것과 같습니다.즉 프로그래밍이 아니라 프로그래밍을 구성하는 언어이지요. 모든 프로그램을 다루기 위해서는 프로그래밍 언어를 먼저 아는 것이 중요합니다. 언어를 배우고 나서야 네트워크나 시스템 데이터 베이스 등을 배울 수 있는 근간이 됩니다. 이 언어 등을 통해 프로그램을 만드는 것을 ‘코딩’ 이라고 하며 코딩과 같은 말이 프로그래밍이라고 보시면 됩니다. 이제 여기서부터 어떻게 배우는 가입니다. 언어를 배웠으면어떻게 사용할 것인지 또는 특화 시킬 것인지가 중요하다고 볼 수 있습니다. 프로그램언어는 C언어와 JAVA를 더 심화시켜 사용하기도 합니다.예를 들어 데이터들을 어떠한 기억장치에 대해 저장, 읽기, 쓰기 등을 배우는 것을 자료구조라고 합니다. 이러한 것처럼 더욱 심화 시킨 프로그래밍 언어를 기반으로 하여단계적으로 나아가는 것이 IT 기술을 배우는 것에 기본이라고 할 수 있습니다. 프로그래밍를 배우게 되면 단계적으로 배워 나아가면서머리로 하는 것이 아니라 몸이 반응 하는 것인 ‘체화’가 될 것입니다. 하지만 이 ‘체화’가 되기까지 그리고 체화가 되고 나서그 윗 단계로 가는 것에 대해 독학 등은 상당한 무리가 있습니다.  질문에 대한 답변이외에 더 궁금하신 점이 있으시면 언제든지오픈채팅 주소 혹은 배너를 통해 문의주세요무료상담을 진행하고 있습니다 https://open.kakao.com/o/sts2Ylcbhttps://itbank-study.edueroom.co.kr/http://naver.me/5n3w6VfR\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=1&dirId=104&docId=291801870&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=8&search_sort=0&spq=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********질문 제목\n",
      "인공지능(딥러닝,머신러닝)을 만들기 위해\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "제가 인공지능을 구현해 보고 싶어서 프로그래밍(c언어)공부를 시작했습니다 그런데 찾아보니 인공지능에 관한 라이브러리?등은대부분 파이썬이 더라구요1.C언어 만으로 인공지능을 구현하기엔 무리인가요?2.그리고 인공지능을 만들기 위해 필요한 것은 무엇인가요?3.인공지능은 서버를 필요로 하나요?(글 맥락이 좀 이상해도 양해 부탁드려요....)\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "soro**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "(1)글쎄요. 친구따라 강남간다고 볼 수도 있는 건데요.개발의 어떤 헤게모니를 python이 장악을 했기 때문에 인공지능에 python을 많이 사용하는 거겠지요.python만이 인공 지능을 구현할 수 있는 게 아니라 쓸만한 AI library들이 python 기반인 경우가 많아서 python을 쓰는 겁니다.예를 들어 누군가가 요즘 핫한 tensorflow가 python이니까 python을 쓴다라고 말해도 할말이 없는 겁니다.http://aikorea.org/blog/dl-libraries/위 링크를 가 보시면 옛날 자료지만, 각 언어별 ai library 목록들이 있습니다.타 언어도 가능하다는 뜻입니다.C를 물어보셨는데요, C가 타입이나 메모리 관리 등, 세부적 부분에 신경쓸 게 많아서 AI 에는 조금 안어울릴 것 같은 느낌이 듭니다.(2)인공 지능을 구현하려면 결과적으로 해당 AI library의 활용법을 거꾸로 배워야 하는 입장이고요,물론 바닥부터 다 만들 수도 있겠지만, 기존의 라이브러리를 이용하는 것이 시간이나 안정성에서 우월하기 때문에요.사실상 AI에 자신만의 러닝 로직을 작성하는게 유저의 몫이라고 봅니다.(3)러닝을 하는 것이 짧게 툭 끝나는 것이 있을 수도 있지만오랜 기간 지속적으로 뭔가 계산을 해야 하는 경우가 많을 것이기 때문에서버를 띄워서 지속적으로 동작시키는 경우가 대부분일 겁니다.\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=1&dirId=10402&docId=305231543&qb=7YyM7J207I2sIOudvOydtOu4jOufrOumrCDrqLjsi6Drn6zri50=&enc=utf8§ion=kin&rank=9&search_sort=0&spq=0\n",
      "**********질문 제목\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select_one'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e2c8a7f7600e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mcontent_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqna_questions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div.c-heading > div.c-heading__content\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontent_q\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select_one'"
     ]
    }
   ],
   "source": [
    "site = \"https://kin.naver.com/search/list.nhn?\"\n",
    "tag = input(\"검색어를 입력해주십시오-->\")\n",
    "num = input(\"검색할 페이지 번호를 입력하십시오-->\")\n",
    "temp_url = site + \"sort=none&query=\" + tag \n",
    "temp_url_encode=parse.urlparse(temp_url)\n",
    "query = parse.parse_qs(temp_url_encode.query)\n",
    "query_encode = parse.urlencode(query, doseq=True)\n",
    "\n",
    "url = site+query_encode+\"&section=kin&page=\"+ num\n",
    "\n",
    "#\n",
    "\n",
    "page = urlopen(url) \n",
    "document = page.read()\n",
    "soup = BeautifulSoup(document.decode(\"utf-8\"), \"html.parser\")\n",
    "\n",
    "temp_qna = soup.find(class_=\"basic1\")\n",
    "qna_list = temp_qna.select(\"li > dl > dt > a\")\n",
    "\n",
    "for qna in qna_list:\n",
    "    \n",
    "    # 링크 생성\n",
    "    qna_url = qna.attrs[\"href\"]\n",
    "    print(\"=\"*30+\"링크\"+\"=\"*30)\n",
    "    print(qna_url)\n",
    "    \n",
    "    # 링크 읽기\n",
    "    qna_page = urlopen(qna_url.encode(\"ascii\",\"ignore\").decode(\"ascii\",\"ignore\")) \n",
    "    qna_document = qna_page.read()\n",
    "    \n",
    "    # 객체 생성\n",
    "    soup_qna = BeautifulSoup(qna_document.decode(\"utf-8\"), \"html.parser\")\n",
    "    \n",
    "    # 질문\n",
    "    print(\"*\"*10 + \"질문 제목\")\n",
    "    qna_questions = soup_qna.find(class_=\"question-content__inner\")\n",
    "    \n",
    "    if qna_questions == None:\n",
    "        pass\n",
    "    else:\n",
    "        title_q = qna_questions.select_one(\"div.c-heading > div.c-heading__title > div.c-heading__title-inner > div.title\").text\n",
    "        print(title_q.strip())\n",
    "    \n",
    "    content_q = qna_questions.select_one(\"div.c-heading > div.c-heading__content\")\n",
    "    if content_q == None:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"\\n\")\n",
    "        print(\"*\"*10 + \"질문 내용\")\n",
    "        print(content_q.text.strip())\n",
    "        \n",
    "    # 답변\n",
    "    qna_answers = soup_qna.find(class_=\"answer-content__inner\")\n",
    "    \n",
    "    title_q_list = qna_answers.select(\"div.c-heading-answer__title > p\")\n",
    "    content_q_list = qna_answers.select(\"div._endContentsText\")\n",
    "    \n",
    "    if title_q == None:\n",
    "        print(\"답변이 없습니다\")\n",
    "    else:\n",
    "        for title_q in title_q_list:\n",
    "            for content_q in content_q_list:\n",
    "                print(\"\\n\")\n",
    "                print(\"+\"*10 + \"답변자\" + \"+\"*10)\n",
    "                print(title_q.text.strip())\n",
    "                print(\"+\"*26)\n",
    "                print(\"\\n\")\n",
    "                print(\"-\"*10 + \"답변 내용\" + \"-\"*10)\n",
    "                print(content_q.text.strip())\n",
    "                print(\"-\"*29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력하십시오-->python\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How do you import a .txt (or other file type) into a Google Colab notebook directly from your own Drive?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085245/how-do-you-import-a-txt-or-other-file-type-into-a-google-colab-notebook-direc\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm trying to import a couple .txt files into Colab from my own Google Drive. The colab notebook I'm using is in the same folder as the files I want to upload within that notebook. While there is documentation on file I/O on External data: Local Files, Drive, Sheets, and Cloud Storage within Colab, the descriptions do not seem to fit what I'm looking for (e.g. I could upload these files directly from my local directory each time I use the notebook, but instead, I would like to have a cell written that loads them once and keeps them loaded via a connection to my own Drive). Also, the .txt files I want to upload contain samples of plain text to use for training a deep learning algorithm on sentiment analysis, and thus does not appear to be convertable to a .csv for functionally equivalent application (which might be solved by Google Sheets).\n",
      "From External data: Local Files, Drive, Sheets, and Cloud Storage, the options are 1) uploading from a local directory, 2) mounting to Drive locally/using an API, or 3) importing from Google Sheets or Google Cloud Storage. Of these, I have tried 1 and some of 2, but because I am still relatively new to Python and Colab, the documentation of 2 and 3 are confusing and do not clearly direct me to a solution. Thus, it is possible that my problem is an interpretation problem, in which case showing how 2/3 could solve it would be quite helpful.\n",
      "I'm thinking it should be possible to refer to these .txt files within Colab since both the colab notebook and .txt files are in the same Drive folder, but perhaps I'm overly comparing the cell functionality of Colab with navigating files in a directory in your everyday terminal.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 PyCall does not build - Fails at “Downloading miniconda installer”\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085221/pycall-does-not-build-fails-at-downloading-miniconda-installer\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Using Windows 10, Julia 1.1.1 (2019-05-16).\n",
      "I've searched around online for various solutions, and have even resorted to totally removing Julia and reinstalling it, only to make no progress. Conda builds inside of Julia perfectly fine.\n",
      "I'm calling the following code:\n",
      "julia> import Pkg\n",
      "julia> ENV[\"PYTHON\"]=\"\"\n",
      "julia> Pkg.build(\"PyCall\")\n",
      "\n",
      "I have tried setting the python environment to a local Conda environment via ENV[\"PYTHON\"]=\"conda activate py37 & python\" which works in a command window to load the correct python environment activated (Note: I don't mind whether I use this externally created environment, or the default one that PyCall is meant to make via a standard build). I've also tried running Julia in administrator mode. Every time I keep getting the same response (though with extra path related content when I try specifying the external Conda environment):\n",
      "julia> Pkg.build(\"PyCall\")\n",
      "  Building Conda ─→ `C:\\Users\\Student\\.julia\\packages\\Conda\\kLXeC\\deps\\build.log`\n",
      "  Building PyCall → `C:\\Users\\Student\\.julia\\packages\\PyCall\\ttONZ\\deps\\build.log`\n",
      "┌ Error: Error building `PyCall`:\n",
      "│ ┌ Info: Using the Python distribution in the Conda package by default.\n",
      "│ └ To use a different Python version, set ENV[\"PYTHON\"]=\"pythoncommand\" and re-run Pkg.build(\"PyCall\").\n",
      "│ [ Info: Downloading miniconda installer ...\n",
      "│ ERROR: LoadError: failed process: Process(`'C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe' -Version 3 -NoProfile -Command \"[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]::Tls12; (New-Object System.Net.Webclient).DownloadFile('https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Windows-x86_64.exe', 'C:\\Users\\Student\\.julia\\conda\\3\\installer.exe')\"`, ProcessExited(3221225477)) [3221225477]\n",
      "│ Stacktrace:\n",
      "│  [1] error(::String, ::Base.Process, ::String, ::Int64, ::String) at .\\error.jl:42\n",
      "│  [2] pipeline_error at .\\process.jl:785 [inlined]\n",
      "│  [3] download(::String, ::String) at .\\download.jl:20\n",
      "│  [4] _install_conda(::String, ::Bool) at C:\\Users\\Student\\.julia\\packages\\Conda\\kLXeC\\src\\Conda.jl:160\n",
      "│  [5] _install_conda(::String) at C:\\Users\\Student\\.julia\\packages\\Conda\\kLXeC\\src\\Conda.jl:152\n",
      "│  [6] runconda(::Cmd, ::String) at C:\\Users\\Student\\.julia\\packages\\Conda\\kLXeC\\src\\Conda.jl:111\n",
      "│  [7] #add#1(::String, ::Function, ::String, ::String) at C:\\Users\\Student\\.julia\\packages\\Conda\\kLXeC\\src\\Conda.jl:184\n",
      "│  [8] add at C:\\Users\\Student\\.julia\\packages\\Conda\\kLXeC\\src\\Conda.jl:183 [inlined] (repeats 2 times)\n",
      "│  [9] top-level scope at C:\\Users\\Student\\.julia\\packages\\PyCall\\ttONZ\\deps\\build.jl:84\n",
      "│  [10] include at .\\boot.jl:326 [inlined]\n",
      "│  [11] include_relative(::Module, ::String) at .\\loading.jl:1038\n",
      "│  [12] include(::Module, ::String) at .\\sysimg.jl:29\n",
      "│  [13] include(::String) at .\\client.jl:403\n",
      "│  [14] top-level scope at none:0\n",
      "│ in expression starting at C:\\Users\\Student\\.julia\\packages\\PyCall\\ttONZ\\deps\\build.jl:43\n",
      "└ @ Pkg.Operations C:\\cygwin\\home\\Administrator\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.1\\Pkg\\src\\Operations.jl:1075\n",
      "\n",
      "Any thoughts?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 dlib functions to parse ImgLab files\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085210/dlib-functions-to-parse-imglab-files\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have used dlib's ImgLab tool to annotate some images, so its produced an XML annotation file. I then used this for dlib's simple object detector but it did quite bad. So I want to train my own SVM where I have more control over feature extraction. \n",
      "I want to use the same annotation XML file for my own feature extraction. Does dlib have functions to parse the XML file and extract the boxes/annotations? \n",
      "I know I can write my own XML parser but I would much prefer to not reinvent the wheel is dlib has functions to extract these. I have searched the dlib site but cannot find dlib python functions for parsing - anyone know a link?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Websockets - trying to convert Phython Code to Node\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085197/websockets-trying-to-convert-phython-code-to-node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm trying to copy the features of this library https://github.com/rnbguy/pysphere/blob/master/misphere.py which connects via websockets to the Mi Sphere 360 camera. \n",
      "The important code is here\n",
      "ms_ip = '192.168.42.1'\n",
      "ms_tcp_port = 7878\n",
      "ms_uk_port = 8787\n",
      "ms_fs_port = 50422\n",
      "\n",
      "def init(self):\n",
      "    self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "    self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
      "    self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
      "    self.socket.connect((self.ms_ip, self.ms_tcp_port))\n",
      "    self.socket_1 = socket.socket()\n",
      "    self.recv_handle_live = True\n",
      "    self.recv_thread = threading.Thread(target=self.recv_handler)\n",
      "    self.recv_thread.daemon = True\n",
      "    self.session = Session()\n",
      "    self.session.conf = {}\n",
      "    self.session.locks = {}\n",
      "    self.last_send = 0\n",
      "\n",
      "I'm trying to do this with the ws library in Node\n",
      "const ip = '192.168.42.1'\n",
      "const port = '7878'\n",
      "const url = `ws://${ip}:${port}`\n",
      "const websocket = new WebSocket(url)\n",
      "\n",
      "However I'm not getting a connection to the camera.  (It's connected via Wifi, but the websocket never sends a connection message)\n",
      "I'm wondering if it's to do with the protocols, which are listed in the Phython code. \n",
      "I see in the websocket documentation you can define protocols in the connection, but I can't find any documentation about what those protocols are and how you add them. \n",
      "i.e. \n",
      "self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
      "self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
      "\n",
      "Anyone know how I add something like this in the Node websocket connection? \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "A plain TCP socket is not the same as a webSocket.  Your Python code appears to be making a plain TCP socket connection.  \n",
      "Your node.js code is attempting to make a webSocket connection.  A webSocket connection can only be made to a webSocket server that speaks the webSocket protocol (which runs on top of a plain TCP socket).\n",
      "If your Python code is working, then you apparently need to make just a plain TCP socket connection which you can see how to do in the Net module.\n",
      "For further description of the differences between a plain TCP connection and a webSocket connection see these:\n",
      "Computer refuses websocket connections\n",
      "What's the difference between WebSocket and plain socket communication?\n",
      "Is there a way to do a tcp connection to an IP with javascript?\n",
      "Socket server not connect with JavaScript socket client\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to assign value to a pandas dataframe, when subset by complex index and boolean based conditions?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085178/how-to-assign-value-to-a-pandas-dataframe-when-subset-by-complex-index-and-bool\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I would like to replace values in a pandas dataframe, with a complex subsetting pattern. \n",
      "With the loc. accessor, I was only able to subset by chaining multiple conditions, because some of the conditions are index based. But it seems I can not assign values after such a chain of subsetting.\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame({'a': range(10), 'b': range(10)}, index=pd.date_range('2019-01-01','2019-01-10'))\n",
      "\n",
      "df.loc[df['a'] % 2 == 0, 'b'].loc[pd.to_datetime(['2019-01-05','2019-01-09'])] = np.nan\n",
      "\n",
      "df\n",
      "\n",
      "Result:\n",
      "            a   b\n",
      "2019-01-01  0   0\n",
      "2019-01-02  1   1\n",
      "2019-01-03  2   2\n",
      "2019-01-04  3   3\n",
      "2019-01-05  4   4\n",
      "2019-01-06  5   5\n",
      "2019-01-07  6   6\n",
      "2019-01-08  7   7\n",
      "2019-01-09  8   8\n",
      "2019-01-10  9   9\n",
      "\n",
      "Expected:\n",
      "            a   b\n",
      "2019-01-01  0   0\n",
      "2019-01-02  1   1\n",
      "2019-01-03  2   2\n",
      "2019-01-04  3   3\n",
      "2019-01-05  4   NaN\n",
      "2019-01-06  5   5\n",
      "2019-01-07  6   6\n",
      "2019-01-08  7   7\n",
      "2019-01-09  8   NaN\n",
      "2019-01-10  9   9\n",
      "\n",
      "I have tried alternative approaches like:\n",
      "df.loc[df['a'] % 2 == 0 and df.index.isin(['2019-01-05','2019-01-09']), 'b']\n",
      "\n",
      "which drops:\n",
      "ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "\n",
      "Not even this works, as the isin returns an array without the date based indexing:\n",
      "df['a'] % 2 == 0 and pd.Series(df.index.isin(['2019-01-05','2019-01-09']))\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "The idea here is that you only want to perform assignment on those labels if they satisfy the previous condition given. You can perform boolean indexing to filter the index, then perform an intersection with the desired labels.\n",
      "idx = df.index[df['a'] % 2 == 0].intersection(['2019-01-05', '2019-01-09'])\n",
      "df.loc[idx, 'b'] = pd.np.nan                               \n",
      "df                                                                                                                          \n",
      "\n",
      "            a    b\n",
      "2019-01-01  0  0.0\n",
      "2019-01-02  1  1.0\n",
      "2019-01-03  2  2.0\n",
      "2019-01-04  3  3.0\n",
      "2019-01-05  4  NaN\n",
      "2019-01-06  5  5.0\n",
      "2019-01-07  6  6.0\n",
      "2019-01-08  7  7.0\n",
      "2019-01-09  8  NaN\n",
      "2019-01-10  9  9.0\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 DWA (Dynamic Windows Approach ) research paper, how to implement the algorithm using python\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085169/dwa-dynamic-windows-approach-research-paper-how-to-implement-the-algorithm-u\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am new in the robotics field and been requested to do a project for a differential drive robot.in the project, I need to implement obstacle avoidance algorithm with a stereo camera.\n",
      "one of the suggested algorithms is DWA which need to be implemented using Python.\n",
      "I have read the published research paper about DWA(1997). I think before starting with the paper I need to build a kinematic model for the differential drive robot is this correct if it's correct how to do it .differnetial drive robot will have angular velocity, linear velocity and heading angle (Theta).\n",
      "Please, I will appreciate any help or suggestions on how to start implementing a scientific paper related to a robot with python.\n",
      "Please let me know if any extra information is needed.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 “Error: bad transparency mask” when converting PNG's with alpha transparency to JPEG's\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085147/error-bad-transparency-mask-when-converting-pngs-with-alpha-transparency-to\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm writing a short function that converts PNG's with alpha transparent background to JPEG's with white backgrounds. Despite the error, the code works by successfully finding all PNG's within a directory and saving JPEG's. Why am I getting this error?\n",
      "The commented-out line of code shows a previous method I tried, which without running error converted PNG's to JPEG's using PIL's \"convert\" function. However it also converted alpha values to black, giving the JPEG's black backgrounds instead of white ones as desired.\n",
      "import os, sys, fnmatch\n",
      "\n",
      "path_in = 'path_in/'\n",
      "dir = os.listdir(path_in)\n",
      "\n",
      "for image in dir:\n",
      "    #checks if image is png\n",
      "    if fnmatch.fnmatch(image, '*.png'):\n",
      "        png_image = Image.open(path_in + image)\n",
      "        f, e = os.path.splitext(path_out + image)\n",
      "\n",
      "        #creates new all-white jpg based on size of jpg\n",
      "        jpg_image = Image.new('RGB', png_image.size, (255,255,255))\n",
      "        #pastes png over jpg\n",
      "        jpg_image.paste(png_image, (0,0), png_image)\n",
      "\n",
      "        #code that made transparent backgrounds black, so not in use\n",
      "        #jpg_image = jpg_image.convert('RGB')\n",
      "\n",
      "        #saves as jpg to same path\n",
      "        jpg_image.save(f + '.jpg', 'JPEG')\n",
      "\n",
      "I want to know why this code is producing an error message despite functioning properly. Otherwise, I would like to use the simpler method of \"convert\" built into PIL, but converting alpha values of PNG's to white instead of black.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Problem to convert object to str in phyton\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085127/problem-to-convert-object-to-str-in-phyton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I need convert object to string \n",
      "import pandas as pd\n",
      "import locale\n",
      "import numpy as np\n",
      "\n",
      "df_csv = pd.read_csv('df_empenho.csv', sep=';', encoding='utf-8')\n",
      "\n",
      "df_csv['valor2'] = df_csv['Empenhado'].astype(str)\n",
      "\n",
      "df_csv['valor2'].astype(str).apply(lambda x : x.replace('.',''))\n",
      "\n",
      "df_csv['valor2'].head()\n",
      "\n",
      "Output\n",
      "0    13.188,15\n",
      "1     8.492,40\n",
      "2     3.570,00\n",
      "3     1.486,20\n",
      "4     2.660,67\n",
      "Name: valor2, dtype: object\n",
      "I expect dtype = string\n",
      "Thanks to help\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Importing a text file with multiple keys\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085125/importing-a-text-file-with-multiple-keys\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am trying to import a text file with multiple 'sets' of values, some lists, some integers. \n",
      "I am struggling to figure out what the best course of action to do this would be. \n",
      "I have provided an example of the text file I am attempting to import and manipulate below. Basically, each line of the text file holds 6 values, and all these values relate to one another. My end goal is to be able to have almost a \"list of list\" set up so that I can define each set of six values almost as one.\n",
      "Does anyone have any thoughts on this? \n",
      "house, brick, 876, no, yes, 3\n",
      "apartment, wood, 345, yes, yes, 1\n",
      "condominium, brick, 453, no, yes, 8\n",
      "etc.. \n",
      "my intended end point is to be able to categorize each variable (for example, building type = house, material = brick, etc. and be able to search through these\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Equivalent matlab function mod in numpy or python\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085118/equivalent-matlab-function-mod-in-numpy-or-python\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I was converting some codes from Matlab to Python.\n",
      "In matlab there is the function mod which gives the modulo operation.\n",
      "For example the following example shows different results between the matlab mod and the equivalent numpy remainder operation:\n",
      "Matlab:\n",
      ">> mod(6, 0.05)\n",
      "\n",
      "ans =\n",
      "\n",
      "     0\n",
      "\n",
      "Numpy:\n",
      "np.remainder(6, 0.05) \n",
      "0.04999999999999967\n",
      "\n",
      "np.mod(6, 0.05)\n",
      "0.04999999999999967\n",
      "\n",
      "Python modulus operator gives the same results as numpy.\n",
      "6%0.05\n",
      "0.04999999999999967\n",
      "\n",
      "Is there anything in python which gives the same mod operation as the one in Matlab? preferably can be operated on numpy 2d/3d arrays.\n",
      "numpy documentation says that numpy.mod is equivalent to matlab mod.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Please how do i sum up all the integer elements from a sqlite3 table row\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085074/please-how-do-i-sum-up-all-the-integer-elements-from-a-sqlite3-table-row\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to calculate the sum total of the elements of a row using user data from a sqlite3 database table\n",
      "i used cursor.fetchall() to get the amount row in my database table and tried using the for loop to sum the elements of the table but i am getting an error\n",
      "db = sqlite3.connect('my.db')\n",
      "cursor = db.cursor()\n",
      "cursor.execute('SELECT amount FROM expenses')\n",
      "result = cursor.fetchall()\n",
      "c = 0\n",
      "for x in result:\n",
      "    c += x\n",
      "print(c)\n",
      "\n",
      "i got\n",
      "TypeError: unsupported operand type(s) for +=: 'int' and 'tuple'\n",
      "i expected to get the sum total of the amount\n",
      "I am aware of the sqlite3 SUM command. But I would like to learn how to solve this problem using Tkinter\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Use \n",
      "c = 0\n",
      "for x in result:\n",
      "    c += x[0]\n",
      "\n",
      "cursor.fetchall() returns a list of tuples, with the tuple containing the column values per row. In your case, there happens to be only one column, but it's still a (one element) tuple.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Struggling to see why reverse sort doesnt take into account 100\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085064/struggling-to-see-why-reverse-sort-doesnt-take-into-account-100\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am importing a file into a multidimensional array. I am reverse sorting based on highest score - however when I run the code it currently ignores scores over 100 - can anyone help?\n",
      "scores = []\n",
      "file = open(\"hScores.txt\",\"r\")\n",
      "for line in file:\n",
      "    scores.append(line.strip(\"\\n\").split(\",\"))\n",
      "scores.sort(key=lambda x: x[1])\n",
      "scores.reverse()\n",
      "print(scores)\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Everything you read from a file is read as a string. You need to convert the numbers to ints before using them to compare as you expect. You can accomplish this by just changing the following line:\n",
      "scores.sort(key=lambda x: int(x[1]))\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Python pandas - Dropping grouped rows based on missing territory code\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085063/python-pandas-dropping-grouped-rows-based-on-missing-territory-code\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "In the df below, we have two \"Mixes\" as indicated by the Mix_Name and Mix_ID columns. And within each of these mixes are multiple tracks with unique Track_ID's that contain different territories (see Territories column).\n",
      "\n",
      "What I'm hoping to do here find out which tracks do not have the US territory, and if any of the tracks do not have the US territory, I will want to drop the entire mix from my dataframe. With the resulting dataframe looking like this because \"Coachella Mix Vol 2\" is missing the US territory in one of its tracks:\n",
      "\n",
      "I know that I need to Groupby: 'Mix_ID', 'Track', and 'Artist' but I'm unsure of how to search the territories column to see if it contains the \"US\" territory. Any help would be much appreciated!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "df.groupby(['Mix_Name', 'Track', 'Artist']).filter(lambda x: (x['Territories'].str.contains('US').any()))\n",
      "\n",
      "You do your groupby, then filter the groups to check if the territories column contains 'US'.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Selenium Python Iterate Through Web Table Columns, Stop after condition is True, Throw Error if Condition is False\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085050/selenium-python-iterate-through-web-table-columns-stop-after-condition-is-true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am trying to validate whether or not data values (which include dollars and cents amounts) are being populated in certain columns of an HTML table. I have written a Selenium Python Script that uses a For Loop to iterate through the HTML table. I have added an IF / ELSE in the For Loop to check if a decimal point is found in any of the text in the columns on the table. If a values with a decimal point is found, then variable \"values_filled\" is set to True (if not, then \"values_filled\" is set to False). My thinking is that if a value with a decimal point is found, then the column is being populated successfully with dollars and cents values. If no decimal point is found, then the column is NOT being populated and this should trigger a \"FAIL\".\n",
      "The code that I have written successfully iterates through the rows and columns of the HTML table. Further, the IF / ELSE section is correctly flagging \"values_filled\" = True if there is a value with a decimal point and values_filled = False if no values found. Last, I have a TRY / EXCEPT section where an exception is thrown if the values_filled is \"False\".\n",
      "What I would like to do next is when the first instance of the \"values_filled\" = True is found, I want to end the loop. However, the loop continues even when the values_filled value is true.\n",
      "account_balances_table = driver.find_element(By.XPATH, \"/html/body/div[1]/div[3]/div/div/ui-view/div/div/div/div[4]/div[1]/table\")\n",
      "rows = account_balances_table.find_elements(By.TAG_NAME, \"tr\")\n",
      "for row in rows:\n",
      "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
      "    for col in cols:\n",
      "        text_found = cols[1].text\n",
      "        if (\".\" in text_found):\n",
      "            values_filled = True\n",
      "            if values_filled == True:\n",
      "                break\n",
      "            else:\n",
      "                continue\n",
      "        else:\n",
      "            values_filled = False\n",
      "\n",
      "        try:\n",
      "            assert values_filled is True\n",
      "        except AssertionError:\n",
      "            screenshot_name = \"FAIL\" + \"_\" + test_case_ID + \"_\" + browser + \"_\" + env + \"_\" + time_stamp + \".png\"\n",
      "            saved_screenshot_location = str(screenshot_directory / screenshot_name)\n",
      "            driver.get_screenshot_as_file(saved_screenshot_location)\n",
      "            raise\n",
      "\n",
      "The problem I am encountering is that the For Loop continues even when values_filled is \"true\". I inserted a break but the loop does not stop. This is a problem as the script continues and falsely flags an exception for blank values found in other cells. I would like the loop to end after finding the first instance of values_filled = true.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Linear Regression - Unexpected Results (Python)\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085013/linear-regression-unexpected-results-python\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am getting unexpected results for the implementation of linear regression I coded.\n",
      "Sometimes I get out of memory error, squaring errors, multiplication errors, basically that I've run out of size.\n",
      "The code seems pretty okay to me, and I'm unable to identify why it fails to work.\n",
      "X = np.array([ 6.1101,  5.5277,  8.5186,  7.0032,  5.8598,  8.3829, 7.4764,\n",
      "        8.5781,  6.4862,  5.0546,  5.7107, 14.164 ,  5.734 ,  8.4084,\n",
      "        5.6407,  5.3794,  6.3654,  5.1301,  6.4296,  7.0708,  6.1891,\n",
      "       20.27  ,  5.4901,  6.3261,  5.5649, 18.945 , 12.828 , 10.957 ,\n",
      "       13.176 , 22.203 ,  5.2524,  6.5894,  9.2482,  5.8918,  8.2111,\n",
      "        7.9334,  8.0959,  5.6063, 12.836 ,  6.3534,  5.4069,  6.8825,\n",
      "       11.708 ,  5.7737,  7.8247,  7.0931,  5.0702,  5.8014, 11.7   ,\n",
      "        5.5416,  7.5402,  5.3077,  7.4239,  7.6031,  6.3328,  6.3589,\n",
      "        6.2742,  5.6397,  9.3102,  9.4536,  8.8254,  5.1793, 21.279 ,\n",
      "       14.908 , 18.959 ,  7.2182,  8.2951, 10.236 ,  5.4994, 20.341 ,\n",
      "       10.136 ,  7.3345,  6.0062,  7.2259,  5.0269,  6.5479,  7.5386,\n",
      "        5.0365, 10.274 ,  5.1077,  5.7292,  5.1884,  6.3557,  9.7687,\n",
      "        6.5159,  8.5172,  9.1802,  6.002 ,  5.5204,  5.0594,  5.7077,\n",
      "        7.6366,  5.8707,  5.3054,  8.2934, 13.394 ,  5.4369])\n",
      "y = np.array([17.592  ,  9.1302 , 13.662  , 11.854  ,  6.8233 , 11.886  ,\n",
      "        4.3483 , 12.     ,  6.5987 ,  3.8166 ,  3.2522 , 15.505  ,\n",
      "        3.1551 ,  7.2258 ,  0.71618,  3.5129 ,  5.3048 ,  0.56077,\n",
      "        3.6518 ,  5.3893 ,  3.1386 , 21.767  ,  4.263  ,  5.1875 ,\n",
      "        3.0825 , 22.638  , 13.501  ,  7.0467 , 14.692  , 24.147  ,\n",
      "       -1.22   ,  5.9966 , 12.134  ,  1.8495 ,  6.5426 ,  4.5623 ,\n",
      "        4.1164 ,  3.3928 , 10.117  ,  5.4974 ,  0.55657,  3.9115 ,\n",
      "        5.3854 ,  2.4406 ,  6.7318 ,  1.0463 ,  5.1337 ,  1.844  ,\n",
      "        8.0043 ,  1.0179 ,  6.7504 ,  1.8396 ,  4.2885 ,  4.9981 ,\n",
      "        1.4233 , -1.4211 ,  2.4756 ,  4.6042 ,  3.9624 ,  5.4141 ,\n",
      "        5.1694 , -0.74279, 17.929  , 12.054  , 17.054  ,  4.8852 ,\n",
      "        5.7442 ,  7.7754 ,  1.0173 , 20.992  ,  6.6799 ,  4.0259 ,\n",
      "        1.2784 ,  3.3411 , -2.6807 ,  0.29678,  3.8845 ,  5.7014 ,\n",
      "        6.7526 ,  2.0576 ,  0.47953,  0.20421,  0.67861,  7.5435 ,\n",
      "        5.3436 ,  4.2415 ,  6.7981 ,  0.92695,  0.152  ,  2.8214 ,\n",
      "        1.8451 ,  4.2959 ,  7.2029 ,  1.9869 ,  0.14454,  9.0551 ,\n",
      "        0.61705])\n",
      "theta = np.array([0,0]) #Initial values of theta_0 and theta_1\n",
      "\n",
      "#Calculates cost function for given theta\n",
      "def costFunction(X,y,theta):\n",
      "    m = y.size\n",
      "    hypothesis = (X * theta[1]) + theta[0]\n",
      "    return (1/m) * sum((hypothesis - y) ** 2)\n",
      "\n",
      "#Calculates the partial derivatives of theta_0 and theta_1\n",
      "def slope(X,y,theta):\n",
      "    m=y.size\n",
      "    hypothesis = (X * theta[1]) + theta[0]\n",
      "    theta_0 = 2/(m) * sum(hypothesis - y) \n",
      "    theta_1 = 2/(m) * sum((hypothesis - y) * X)\n",
      "    return np.array([theta_0,theta_1])\n",
      "\n",
      "#running the gradient descent with 200 iters with learning rate 0.1\n",
      "for i in range(200):\n",
      "    theta = theta - 0.1*slope(X,y,theta)\n",
      "\n",
      "costFunction(X,y,theta) # Prints inf\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Your learning rate is too large and GD does not converge. Try changing it to 0.01 and run it for more epochs, it worked for me.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Remove list type in columns while preserving list structure\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085000/remove-list-type-in-columns-while-preserving-list-structure\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have two columns that from the way my data was pulled are in lists. This may be a really easy question, I just haven't found the exactly correct way to create the result I'm looking for.\n",
      "I need the \"c_u\" column to be a string without the [] and the \"tawgs.db_id\" column to be integers separated by a column if that's possible. \n",
      "I've tried this code:\n",
      "df['c_u'] = df['c_u'].astype(str)\n",
      "\n",
      "to convert c_u to a string: but it failed and outputs:\n",
      "\n",
      "What I need the output to look like is:\n",
      "c_u                              tawgs.db_id\n",
      "hbhprecision.com         10813,449,6426,6427\n",
      "thomsonreuters.com            12519,510,6426\n",
      "\n",
      "etc.\n",
      "Please help and thank you very much in advance!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "for the first part, removing the brackets [ ] \n",
      "df['c_u'].apply(lambda x : x.strip(\"['\").strip(\"']\"))\n",
      "\n",
      "for the second part (assuming you removed your brackets as well), splitting the values across columns:\n",
      "df['tawgs.db_id'].str.split(',',  expand=True)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How can i say “if error occurs do not implement this code”\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084986/how-can-i-say-if-error-occurs-do-not-implement-this-code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to write a code in python that basically says, \" if error occurs while implementing int(a) print('valid character')      elif no error occurs while implementing int(a)  print('invalid character').\" a is an input\n",
      "I wanna make a simple hangman game and if the input is not a letter i want a certain message to be displayed. I tried using \"if a==int()\" but inputs are always a string\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Normally you would use a Try, Except clause to handle errors, but because you're not actually getting an error -- you just want to know if the input is an alphabetical character or not, you would use the string.isalpha function.\n",
      "guess = input()\n",
      "if not guess.isalpha():\n",
      "    print('You must supply an alphabetical character')\n",
      "else:\n",
      "    #the rest of your code would go here\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 What is the proper way to code the main.py in python?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084983/what-is-the-proper-way-to-code-the-main-py-in-python\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Using PyQt designer, what is the proper way to lay out your main.py? I have multiple windows I've created using QtDesigner. I've already coded all the window funtions, as in buttons and menu bar funtions. But I've coded all the funtions in the main.py. Should all the window funtions be coded in their own .py file? Or is it ok with coding them all in the main? It doesn't look good to me, but everything is working properly. \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "In general, you should try to keep your main.py as empty as possible:\n",
      "import stuff\n",
      "\n",
      "def main():\n",
      "    class_object = Class_object()\n",
      "    class_object.run()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "Something like this is clear. Obviously you can initiate logging, import configs and such, but only do the most general stuff in the main.py .\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 i try to update odoo code but i get strptime() argument 1 must be str, not bool\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084929/i-try-to-update-odoo-code-but-i-get-strptime-argument-1-must-be-str-not-bool\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "The objective is to take the sale date and create a plan for automatic visits, 2 visits per year for three years, and i can do it in older version of odoo but now i get this error.\n",
      "that worked in openerp 7, but now i want to do it in Odoo 11.0 Python 3, really i don't get it what i missed\n",
      "\n",
      "    class garantias(models.Model):\n",
      "        _name = 'itriplee.garantias'\n",
      "\n",
      "        equipo = fields.Many2one('itriplee.equipos', 'Equipo')\n",
      "        fecha_de_venta = fields.Date('Fecha de Venta', related='equipo.venta', readonly=True)\n",
      "\n",
      "     @api.model\n",
      "        def create(self, vals):\n",
      "            obj_visita = self.pool.get('itriplee.servicio')\n",
      "            obj = self.env['itriplee.garantias']\n",
      "            cliente = obj.cliente.id\n",
      "            fecha_compra = obj.fecha_de_venta\n",
      "            fm = ('%Y-%m-%d')\n",
      "            cantidad_meses = 6\n",
      "            ind = 0\n",
      "            now = datetime.now()\n",
      "            now_str = now.strftime(fm)\n",
      "            now_int = datetime.strptime(now_str, fm)\n",
      "            # fecha_compra_original = datetime.strptime(fecha_compra, fm)\n",
      "            fecha_compra_inicial = datetime.strptime(fecha_compra, fm)\n",
      "            while ind < cantidad_meses:\n",
      "                fecha_6_meses = fecha_compra_inicial + relativedelta(months=6)\n",
      "                if fecha_6_meses >= now_int:\n",
      "                    obj_visita.create({'cliente':cliente,'visita':fecha_6_meses,'estado':'confirmar','visitas':obj.id},context=None)\n",
      "                ind = ind + 1\n",
      "                fecha_compra_inicial = fecha_6_meses\n",
      "            return True\n",
      "\n",
      "\n",
      "and get this error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 651, in _handle_exception\n",
      "    return super(JsonRequest, self)._handle_exception(exception)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 310, in _handle_exception\n",
      "    raise pycompat.reraise(type(exception), exception, sys.exc_info()[2])\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/tools/pycompat.py\", line 87, in reraise\n",
      "    raise value\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 693, in dispatch\n",
      "    result = self._call_function(**self.params)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 342, in _call_function\n",
      "    return checked_call(self.db, *args, **kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/service/model.py\", line 97, in wrapper\n",
      "    return f(dbname, *args, **kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 335, in checked_call\n",
      "    result = self.endpoint(*a, **kw)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 937, in __call__\n",
      "    return self.method(*args, **kw)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/http.py\", line 515, in response_wrap\n",
      "    response = f(*args, **kw)\n",
      "  File \"/home/openerp/odoo-dev/odoo/addons/web/controllers/main.py\", line 934, in call_kw\n",
      "    return self._call_kw(model, method, args, kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/addons/web/controllers/main.py\", line 926, in _call_kw\n",
      "    return call_kw(request.env[model], method, args, kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/api.py\", line 687, in call_kw\n",
      "    return call_kw_model(method, model, args, kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/odoo/api.py\", line 672, in call_kw_model\n",
      "    result = method(recs, *args, **kwargs)\n",
      "  File \"/home/openerp/odoo-dev/odoo/addons/itriplee/models/garantias.py\", line 53, in create\n",
      "    fecha_compra_inicial = datetime.strptime(fecha_compra, fm).date()\n",
      "TypeError: strptime() argument 1 must be str, not bool\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Why is the output 2 when i run this code?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084912/why-is-the-output-2-when-i-run-this-code\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Doing a bioinformatic course on coursera, for the function below, the input is GCGCG, the output should be 2 but nothing puts up when i try to run the code.\n",
      "def PatternCount(Text, Pattern):\n",
      " count = 0\n",
      " for i in range(len(Text)-len(Pattern)+1):\n",
      "    if Text[i:i+len(Pattern)] == Pattern:\n",
      "        count = count+1\n",
      " return count \n",
      "Text= \"GCGCG\"\n",
      "Pattern= \"GCG\"\n",
      "print PatternCount(Text, Pattern)\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Check out SyntaxVoid comment. => Convert to python 3\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Telethon issue, need to read messages from channels\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084901/telethon-issue-need-to-read-messages-from-channels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I've read a lot of answers here and tried their code, but I cannot get the examples to work for me and I don't understand why.\n",
      "I want to capture messages from telegram channels but noone of telehon methods works for me...\n",
      "I'm using telethon but it is not important how. I just need to capture these messages.\n",
      "from telethon import TelegramClient, events,utils\n",
      "from telethon.tl.functions.messages import GetFullChatRequest\n",
      "from telethon.tl.functions.messages import GetHistoryRequest\n",
      "from telethon.tl.functions.channels import GetChannelsRequest\n",
      "from telethon.tl.functions.contacts import ResolveUsernameRequest\n",
      "from telethon.tl.types import PeerUser, PeerChat, PeerChannel , sync\n",
      "import re\n",
      "\n",
      "\n",
      "client = TelegramClient('session_name', api id, api hash )\n",
      "client.start()\n",
      "my_self=client.get_me()\n",
      "print(myself)\n",
      "\n",
      "This is the output I get:\n",
      " C:/Users/Riccardo/Desktop/yobitpumpR.py:15: RuntimeWarning: coroutine \n",
      " 'AuthMethods._start' was never awaited\n",
      "\n",
      " C:/Users/Riccardo/Desktop/yobitpumpR.py:19: RuntimeWarning: coroutine \n",
      " 'get_me' was never awaited\n",
      " <coroutine object get_me at 0x00000083F93E9410>\n",
      "\n",
      "How should I proceed?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to modify np array by slicing through the broadcast of an index vector?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084899/how-to-modify-np-array-by-slicing-through-the-broadcast-of-an-index-vector\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have this m x n numpy array which I want to apply certain operation over the row elements. Although, it must be cast only on those elements whose index is prior to those specified by the entries on a vector of indexes.\n",
      "I've already gone through the classic for-loop way, but I was expecting something more NumPythonic.\n",
      "The following code would complete the job:\n",
      "for i,j in enumerate(x):\n",
      "    M[i, 0:j] = 2*M[i, 0:j]\n",
      "\n",
      "But I was look for a broadcast, no-for approach. Any Ideas?\n",
      "For example, lets say that\n",
      "M = [[ 1, 2, 3, 4, 5],\n",
      "     [ 6, 7, 8, 9,10]]\n",
      "\n",
      "x = [2, 3]\n",
      "\n",
      "and our application is to double certain element. According to the indexes specified in x we should have the resulting array:\n",
      "M = [[ 2, 4, 3, 4, 5],\n",
      "     [12,14,16, 9,10]]\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Considering the fact that your operations are not being applied uniformly to the whole array, I do not think it is feasible to accomplish this with broadcasting. \n",
      "I have gotten as far as making the multiplication for each row but the rest does not work with just numpy. If the changes you were making were always on the first, let's say, 2 numbers for each array, then I could find a way but this seems highly unlikely to be possible. This might be possible with Pandas however. \n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to use Python create the chart below? or what library I should use?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084898/how-to-use-python-create-the-chart-below-or-what-library-i-should-use\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to create a chart for companies interlocks as below (but I don't know how this chart was created...) Does anyone have clues about what library or data I should use? \n",
      "Connected companies chart\n",
      "I used Python Networkx package to build a graph but didn't get the same output.\n",
      "\n",
      "g = nx.from_pandas_edgelist(df, source='CompanyList1', target='CompanyList2')\n",
      "Any thoughts or suggestions about how to create the chart above using Python?\n",
      "Thanks!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How can I improve my function to be more memory efficient?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084884/how-can-i-improve-my-function-to-be-more-memory-efficient\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm writing an AWS Lambda in Python 3.6\n",
      "I have a large amount of large space separated text files and I need to loop through these files and pull out the first N (in this case 1000) lines of text. Once I have those lines I need to put them in to a new file and upload that to S3.\n",
      "I'm also not a python developer, so the language and environment is new to me.\n",
      "Right now I'm collecting the S3 object summaries, and for each of those, I'm running a check on them and then getting the object's data, opening that as a file-like object and also opening the output variable as a file-like object, and then doing my processing.\n",
      "I've given my Lambda 3GB RAM but the lambda is running out of memory before it can process any files (Each file is about 800MB and there are about 210 of them).\n",
      "    for item in object_summary:\n",
      "        # Check if the object exists, and skip it if so\n",
      "        try:\n",
      "            head_object_response = s3Client.head_object(Bucket=target_bucket_name, Key=item)\n",
      "            logger.info('%s: Key alredy exists.' % item)\n",
      "        except:\n",
      "            # if the key does not exist, we need to swallow the 404 that comes from boto3\n",
      "            pass\n",
      "\n",
      "        # and then do our logic to headify the files\n",
      "        logger.info('Key does not exist in target, headifying: %s' % item)\n",
      "\n",
      "        # If the file doesn't exist, get the full object\n",
      "        s3_object = s3Client.get_object(Bucket=inputBucketName, Key=item)\n",
      "        long_file = s3_object['Body']._raw_stream.data\n",
      "        file_name = item\n",
      "        logger.info('%s: Processing 1000 lines of input.' % file_name)\n",
      "\n",
      "        '''\n",
      "        Looks like the Lambda hits a memory limit on the line below.\n",
      "        It crashes with 2500MB of memory used, the file it's trying \n",
      "        to open at that stage is 800MB large which puts it over the \n",
      "        max allocation of 3GB\n",
      "        '''\n",
      "        try:\n",
      "            with open(long_file, 'r') as input_file, open(file_name, 'w') as output_file:\n",
      "                for i in range(1000):\n",
      "                    output_file.write(input_file.readline())\n",
      "        except OSError as exception:\n",
      "            if exception.errno ==36:\n",
      "                logger.error('File name: %s' %exception.filename)\n",
      "                logger.error(exception.__traceback__)\n",
      "\n",
      "I put the whole function for completeness above, but I think that the specific area I can improve it is the try: while: block that handles the file processing. \n",
      "Have I got that right? Is there anywhere else I can improve it?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 4D plot in python matplotlib\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084861/4d-plot-in-python-matplotlib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am quite new with python programming so i am a bit lost with it. \n",
      "I am trying to create a scatter3D plot which plots X, Y, Z coordinates for time position and use the color as 4 dimension to plot the value. \n",
      "The problem i am facing is that... it seems impossible to me to add the variable as colour and add a colorbar for help. \n",
      "I explain the background here: I have created a meshgrid for X, Y, Z and an array for computing the value in each position for X, Y, Z coordinates (which is an trimensional array). \n",
      "It follows an loop for filling the array with the values for each X,Y,Z combination and then i try to create the graph. \n",
      "x = np.linspace(0, 15, 15)\n",
      "y = np.linspace(0, 30, 15)\n",
      "z = np. linspace(5, 45, 15)\n",
      "\n",
      "X, Y, Z = np.meshgrid(x, y, z)\n",
      "\n",
      "sumatorio = np.zeros(shape=(len(X[0, :, 0]), len(Y[:, 0, 0])))\n",
      "\n",
      "sumatorio_1 = np.zeros(shape=(len(Y[:, 0, 0]), len(Z[0, 0, :])))\n",
      "\n",
      "sum_total = np.zeros(shape=(len(X[:, 0, 0]), len(Y[0, :, 0]), len(Z[0, 0, \n",
      ":])))\n",
      "\n",
      "c = np.array([])\n",
      "\n",
      "for i in range(len(X[0, :, 0])):\n",
      "    for j in range(len(Y[:, 0, 0])):\n",
      "        sum_1 = X[0, i, 0] + Y[j, 0, 0]\n",
      "        sumatorio[i, j] = sum_1\n",
      "            for k in range(len(Z[0, 0, :])):\n",
      "            sum_2 = sum_1 ** 2.0 + Z[0, 0, k] + X[0, i, 0]\n",
      "            sumatorio_1[j, k] = sum_2\n",
      "            sum_total[i, j, k] = sumatorio[i, j] + sumatorio_1[j, k]\n",
      "fig = plt.figure(figsize=(50.0, 50.0))\n",
      "ax = fig.add_subplot(111, projection='3d')\n",
      "\n",
      "ax.scatter3D(X, Y, Z, c=sum_total, cmap='coolwarm', depthshade=0)\n",
      "fig.colorbar(sum_total)\n",
      "\n",
      "plt.title(\"DV at departure from Earth\")\n",
      "ax.set_xlabel(\"Beginning\")\n",
      "ax.set_ylabel(\"Time of flight\")\n",
      "ax.set_zlabel(\"Time of flight 2\")\n",
      "\n",
      "plt.show()\n",
      "\n",
      "When i execute the code, the following error comes into view: \n",
      "'c' argument has 15 elements, which is not acceptable for use with 'x' with size 3375, 'y' with size 3375.\n",
      "\n",
      "It seems when i put the array sum_total here, it did not recognize the shape of it.\n",
      "I try to add the color as c=np.ravel(sum_total) which returns me the graphic with color, but i think it is not given the appropiate color to each point.\n",
      "Also, when i create the graphic with np.ravel(sum_total), the colorbar gives the following error: \" 'numpy.ndarray' object has no attribute 'get_array'\"\n",
      "Thank you in advance! I will appreciate the help as i wrote, i am quite new with python.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to install mysqlclient?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084842/how-to-install-mysqlclient\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I try to install mysqlclient on Windows 10 with \n",
      "pip install mysqlclient\n",
      "I have visual studio 2019 and python 3.7.4 also the build tools the last one and I update the pip\n",
      "But I get this error:\n",
      "ERROR: Complete output from command 'c:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import setuptools, tokenize;__file__='\"'\"'C:\\\\Users\\\\ponce\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yg2gzqfi\\\\mysqlclient\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\ponce\\AppData\\Local\\Temp\\pip-record-34g102m_\\install-record.txt' --single-version-externally-managed --compile:\n",
      "    ERROR: running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win32-3.7\n",
      "    creating build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\__init__.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\_exceptions.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\compat.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\connections.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\converters.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\cursors.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\release.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    copying MySQLdb\\times.py -> build\\lib.win32-3.7\\MySQLdb\n",
      "    creating build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\__init__.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\CLIENT.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\CR.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\ER.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\FIELD_TYPE.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    copying MySQLdb\\constants\\FLAG.py -> build\\lib.win32-3.7\\MySQLdb\\constants\n",
      "    running build_ext\n",
      "    building 'MySQLdb._mysql' extension\n",
      "    creating build\\temp.win32-3.7\n",
      "    creating build\\temp.win32-3.7\\Release\n",
      "    creating build\\temp.win32-3.7\\Release\\MySQLdb\n",
      "    C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.21.27702\\bin\\HostX86\\x86\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,4,2,'post',1) -D__version__=1.4.2.post1 \"-IC:\\Program Files (x86)\\MySQL\\MySQL Connector C 6.1\\include\\mariadb\" -Ic:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\include -Ic:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.21.27702\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\" /TcMySQLdb/_mysql.c /Fobuild\\temp.win32-3.7\\Release\\MySQLdb/_mysql.obj /Zl /D_CRT_SECURE_NO_WARNINGS\n",
      "    _mysql.c\n",
      "    MySQLdb/_mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory\n",
      "    error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.21.27702\\\\bin\\\\HostX86\\\\x86\\\\cl.exe' failed with exit status 2\n",
      "    ----------------------------------------\n",
      "ERROR: Command \"'c:\\users\\ponce\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import setuptools, tokenize;__file__='\"'\"'C:\\\\Users\\\\ponce\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yg2gzqfi\\\\mysqlclient\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\ponce\\AppData\\Local\\Temp\\pip-record-34g102m_\\install-record.txt' --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\ponce\\AppData\\Local\\Temp\\pip-install-yg2gzqfi\\mysqlclient\\\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to fix a “i in range(n)” when it refuses to repeat\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084817/how-to-fix-a-i-in-rangen-when-it-refuses-to-repeat\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am trying to copy the code from a Python tutorial https://youtu.be/BfS2H1y6tzQ?t=156 and copied it word for word, but it isn't working like in the video.\n",
      "import random\n",
      "\n",
      "def random_walk(n):\n",
      "    \"\"\"Return coodrinates after 'n' block random walk.\"\"\"\n",
      "    x = 0\n",
      "    y = 0\n",
      "    for i in range(n):\n",
      "        step = random.choice(['N','S','E','W'])\n",
      "        if step == 'N':\n",
      "            y = y + 1\n",
      "        elif step == 'S':\n",
      "            y = y - 1\n",
      "        elif step == 'E':\n",
      "            x = x + 1\n",
      "        elif step == 'W':\n",
      "            x = x - 1\n",
      "        return (x, y)\n",
      "\n",
      "I expected the \"for i in range(n):\" to repeat the next line for the n amount of times, but it seems to only try it once, thus diving the new coordinates a one number difference.  I was expecting a result more like the one at https://youtu.be/BfS2H1y6tzQ?t=176. For reference, I'm using Python 3.7.3.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Your return statement has a wrong intent, and you have to delete some blanks before it. Below is the correct code, and be careful of the last line.\n",
      "import random\n",
      "\n",
      "def random_walk(n):\n",
      "    \"\"\"Return coodrinates after 'n' block random walk.\"\"\"\n",
      "    x = 0\n",
      "    y = 0\n",
      "    for i in range(n):\n",
      "        step = random.choice(['N','S','E','W'])\n",
      "        if step == 'N':\n",
      "            y = y + 1\n",
      "        elif step == 'S':\n",
      "            y = y - 1\n",
      "        elif step == 'E':\n",
      "            x = x + 1\n",
      "        elif step == 'W':\n",
      "            x = x - 1\n",
      "\n",
      "    return (x, y)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Changing Marker Size in pyplot.plot\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084787/changing-marker-size-in-pyplot-plot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I wish to change the marker size in a matplotlib.pyplot.plot NOT a scatter.  The documentation has the kwarg \"markersize\", but adding that to the command has not changed the markersize.  I know this can be done in a scatter, but I don't want a scatter plot, I want to still be able to connect the points.  As far as I can tell, points can only be connected in a pyplot.plot and not a scatter.\n",
      "import matplotlib as plt\n",
      "size = 30.\n",
      "plt.plot(xArray,yArray, alpha = 0.75, marker = 'o', color = 'b', ls = '--', markersize = size)\n",
      "\n",
      "Changing size does not change the size of the points in my plot.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Change weights on base of mean prediction keras\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084773/change-weights-on-base-of-mean-prediction-keras\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm setting up a few shot image segmentation with keras. My first network (VGG-like) outputs a feature vector with the shape [2,1024] that separates information of foreground and background.\n",
      "I want to compare one prototypical output (fix image) with the mean feature vector of all the other images. My loss function is a nearest neighbor implementation to update the weights.\n",
      "My GPU can only take 4 images max. with an accurate resolution.\n",
      "My question is, how to calculate the losses based on a mean output of all images while training.\n",
      "Would be greateful for every hint.\n",
      "The input is an image with separated marked foreground and marked background with shape [2,512,512,1]\n",
      "    def conv_model(self, input):\n",
      "        c1 = Conv2D(self.feat_lev_1, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (input) # added input shape\n",
      "        c1 = BatchNormalization()(c1)\n",
      "        c1 = Conv2D(self.feat_lev_1, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
      "        c1 = BatchNormalization()(c1)\n",
      "        p1 = MaxPooling2D((2, 2)) (c1)\n",
      "\n",
      "        c2 = Conv2D(self.feat_lev_2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
      "        c2 = BatchNormalization()(c2)\n",
      "        c2 = Conv2D(self.feat_lev_2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
      "        c2 = BatchNormalization()(c2)\n",
      "        p2 = MaxPooling2D((2, 2)) (c2)\n",
      "\n",
      "        c3 = Conv2D(self.feat_lev_3, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
      "        c3 = BatchNormalization()(c3)\n",
      "        c3 = Conv2D(self.feat_lev_3, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
      "        c3 = BatchNormalization()(c3)\n",
      "        p3 = MaxPooling2D((2, 2)) (c3)\n",
      "\n",
      "        c4 = Conv2D(self.feat_lev_4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
      "        c4 = BatchNormalization()(c4)\n",
      "        c4 = Conv2D(self.feat_lev_4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
      "        c4 = BatchNormalization()(c4)\n",
      "        # added\n",
      "        #d4 = Dropout(0.5)(c4)\n",
      "        p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
      "\n",
      "        c5 = Conv2D(self.feat_lev_5, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
      "        c5 = BatchNormalization()(c5)\n",
      "        c5 = Conv2D(self.feat_lev_5, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
      "        c5 = BatchNormalization()(c5)\n",
      "        #c5 = Dropout(0.3) (c5) # 0.5\n",
      "        result = GlobalAveragePooling2D()(c5)# activation='softmax'\n",
      "\n",
      "        return result\n",
      "\n",
      "I already used the predict function of keras to extract the mean vector. But running the network always uses the input images separately.\n",
      "    # the prototypical image\n",
      "    prototype = model.predict(xv)\n",
      "\n",
      "    learner_train = DataGenerator_Learner(X, Y, prototype)\n",
      "\n",
      "    # create mean prototype\n",
      "    prototypes = []\n",
      "    for i in range(len(X)):\n",
      "        element = learner_train.get_data(i)\n",
      "        prototypes.append(model.predict(element))\n",
      "    mean_prototype = tf.add_n(prototypes) / len(X)\n",
      "\n",
      "    result = model.fit_generator(generator=learner_train, epochs=num_epochs, callbacks=[checkpointer, earlystopper], steps_per_epoch=1)\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Have a string and trying to input that input a tableview\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084762/have-a-string-and-trying-to-input-that-input-a-tableview\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "So here's the problem that I am having, I have to read from multiple different files and each file has a different format of how it represents the data, so for example one file could be:\n",
      "1 2 3 4 5\n",
      "and another could be:\n",
      "0\n",
      "and another from the file can be is:\n",
      "col1 col2 col3\n",
      "\n",
      "0    2     1\n",
      "\n",
      "how can I use a tableview to output this type of data.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Mechanize / requests post image encoded picture\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084760/mechanize-requests-post-image-encoded-picture\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am trying to upload an image to a webpage, where the image file gets encoded while uploading.\n",
      "The encoded picture looks like that:\n",
      "\n",
      "EDIT: The encoding should be ISO 8859-9, but how can i encode my image?\n",
      "I have tryed it like this:\n",
      "with open(r'C:/Users/Oli/Google Drive/IMG_20190616_153823.jpg', 'rb') as img_file:\n",
      "    img_file = str(img_file.read()).encode('iso-8859-1')\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Don't understand how a “while not” loop works\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084757/dont-understand-how-a-while-not-loop-works\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am new to programming. Sorry if I break any rules of asking. I am currently trying to learn python. I was going through Al Sweigart's Automate the Boring stuff with Python. In his example of while loops, he uses a not condition with his while loop (as shown in the code below).\n",
      "name = ''\n",
      "while not name != '':\n",
      "    print('Enter your name:')\n",
      "    name = input()\n",
      "print('How many guests will you have?')\n",
      "numOfGuests = int(input())\n",
      "if numOfGuests !=0:\n",
      "    print('Be sure to have enough room for all your guests.')\n",
      "print('Done')\n",
      "\n",
      "This code works fine. I am confused about how this works though. We set name to '' (blank value) and then in the while loop we have while not name !=''. Why does this not work with while name != ''?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "The while loop will only loop as long as the condition after it remains true. Putting a not before the condition inverts it. not True == False, not False == True\n",
      "while not name != '' will loop as long as (not (name != '')) is True.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to send WebKitFormBoundary post via python\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084730/how-to-send-webkitformboundary-post-via-python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Hello there I am facing with this issue and I am having a real hard time \n",
      "I am trying to make a nice webclient of a 4chan board and ways \n",
      "the problem that I am facing is that the payload is WebKitFormBoundary\n",
      "Now the post-payload is \n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"resto\"\n",
      "\n",
      "804890085\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"com\"\n",
      "\n",
      "Hello world this is a test\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"mode\"\n",
      "\n",
      "regist\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"pwd\"\n",
      "\n",
      "_HwWOVhsZy08kMr1Gmq75BLc293fANR64\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "Content-Disposition: form-data; name=\"g-recaptcha-response\"\n",
      "\n",
      "03AOLTBLTaPVx7Y96wcfYaraLNGRPA_-sXCt6Bu5O3n8SgsnBRgc0zi38L9L6BMf3azYoV2GsjWy9Epm7YBCY0ciHLVVPE3o8rmNef24DQajXHqp21yRcw1k4z0qTR2PvUNSXr7BCQkWzk5h5bay8nCXX7dWf6BKKa3aR4nid2umKXciIrqhp5zfAtdM2BOASdfNg1NiPRX_pJKhkL6AQNbjsRPyL0VSNZLC8Qcn4wu8ITcbc7Pdnzz2kxl_KzOjqh7vbYdawPQgVF\n",
      "------WebKitFormBoundary7O0A7Bnnrv3G98vW--\n",
      "\n",
      "the headers is \n",
      ":authority: sys.4chan.org\n",
      ":method: POST\n",
      ":path: /b/post\n",
      ":scheme: https\n",
      "accept: */*\n",
      "accept-encoding: gzip, deflate, br\n",
      "accept-language: en-US,en;q=0.9\n",
      "content-length: 848\n",
      "content-type: multipart/form-data; boundary=----WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "cookie: __cfduid=d822b2332925f67ed3a21d3fcd15ce42d1563307415; 4chan_pass=_HwWOVhsZy08kMr1Gmq75BLc293fANR64\n",
      "origin: https://boards.4chan.org\n",
      "referer: https://boards.4chan.org/\n",
      "user-agent: Mozilla/5.0 (X11; U; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3690.133 Safari/537.36\n",
      "\n",
      "Now I want to focus on \n",
      "content-length: 848\n",
      "content-type: multipart/form-data; boundary=----WebKitFormBoundary7O0A7Bnnrv3G98vW\n",
      "\n",
      "And also how to I get 'boundary' / 'WebKitFormBoundary'?\n",
      "and I how to I get the right amount of 'content-length'?\n",
      "Can someone can hit me with a solution please ? I am have a real hard time solving it no meter what I do it seem like I miss something and the post doesn't work, lol trying no meter what but I cant figure out how to deal with it thanks! I never had to deal with this kind of system before it seem kind of weird I think I might miss some values ... what should I do to get it right.\n",
      "would like to get a semi demo python file that deals with it and explication about how to deal with it in python ! \n",
      "Thanks for everyone who is willing to help or even try ! much appreciated!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Enumerating columns of large data using NumPy\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084723/enumerating-columns-of-large-data-using-numpy\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a large data, and I want to name the columns, for instance '1', '2', ... . For a small data, I can do\n",
      "np.random.randint(5, size=(50, 2))   # synthesis data\n",
      "A = A.ravel().view([('1','i8'),('2','i8'),]).astype([('1','i4'),('2','i8'),])\n",
      "\n",
      "and then call an individual column using\n",
      "print(A['2'])\n",
      "\n",
      "I was wondering if there is a way to automate this for any random size and column numbers. My preference is to use NumPy, not Pandas. Thanks!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Extending from your work, you can use a list comprehension to accomplish this. It will automatically create the required number of columns with the proper labels:\n",
      "A = np.random.randint(5, size=(10, 10))\n",
      "B = A.ravel().view([ (str(x),'i4') for x in range(1, len(A[0])+1) ])\n",
      "\n",
      "Then you can do print(A['2']) from 1 to 10 in this case.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Programing how do i ask questions am new-operator\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084710/programing-how-do-i-ask-questions-am-new-operator\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Pls can I know how to ask questions on this platform concerning python program I am new in this site and I need help on this and in programing. Tnks\n",
      "Adei\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to combine desired values from multiple CSV table\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084704/how-to-combine-desired-values-from-multiple-csv-table\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have numerous amounts of csv files that contains a table with longitude, latitude, date, and wave height. I want to create a table that contains the date and wave heights of values that lie within a certain longitude/longitude from all csv files. \n",
      "For example, if I have two csv files that each contains 2 values that falls within the desired longitude/latitude, then I want to create a table that has the 4 values with their corresponding dates and wave height. This table would be very large, so I prefer to convert this into excel.\n",
      "How would I approach this with python? \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Capturing Picture of webview display of raspberry pi camera using HTML\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084664/capturing-picture-of-webview-display-of-raspberry-pi-camera-using-html\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am live-streaming my raspberry pi camera with some python code, and I created an app to access the webview of the stream with android studio. I want to be able to take a picture using the raspberry pi camera with the click of a button.\n",
      "import io\n",
      "import picamera\n",
      "import logging\n",
      "import socketserver\n",
      "from threading import Condition\n",
      "from http import server\n",
      "\n",
      "PAGE=\"\"\"\\\n",
      "<html>\n",
      "<head>\n",
      "<title>Raspberry Pi - Surveillance Camera</title>\n",
      "</head>\n",
      "<body>\n",
      "<center><h1>Raspberry Pi - Surveillance Camera</h1></center>\n",
      "<center><img src=\"stream.mjpg\" width=\"640\" height=\"480\"></center>\n",
      "</body>\n",
      "</html>\n",
      "\"\"\"\n",
      "\n",
      "class StreamingOutput(object):\n",
      "    def __init__(self):\n",
      "        self.frame = None\n",
      "        self.buffer = io.BytesIO()\n",
      "        self.condition = Condition()\n",
      "\n",
      "    def write(self, buf):\n",
      "        if buf.startswith(b'\\xff\\xd8'):\n",
      "            # New frame, copy the existing buffer's content and notify all\n",
      "            # clients it's available\n",
      "            self.buffer.truncate()\n",
      "            with self.condition:\n",
      "                self.frame = self.buffer.getvalue()\n",
      "                self.condition.notify_all()\n",
      "            self.buffer.seek(0)\n",
      "        return self.buffer.write(buf)\n",
      "\n",
      "class StreamingHandler(server.BaseHTTPRequestHandler):\n",
      "    def do_GET(self):\n",
      "        if self.path == '/':\n",
      "            self.send_response(301)\n",
      "            self.send_header('Location', '/index.html')\n",
      "            self.end_headers()\n",
      "        elif self.path == '/index.html':\n",
      "            content = PAGE.encode('utf-8')\n",
      "            self.send_response(200)\n",
      "            self.send_header('Content-Type', 'text/html')\n",
      "            self.send_header('Content-Length', len(content))\n",
      "            self.end_headers()\n",
      "            self.wfile.write(content)\n",
      "        elif self.path == '/stream.mjpg':\n",
      "            self.send_response(200)\n",
      "            self.send_header('Age', 0)\n",
      "            self.send_header('Cache-Control', 'no-cache, private')\n",
      "            self.send_header('Pragma', 'no-cache')\n",
      "            self.send_header('Content-Type', 'multipart/x-mixed-replace; boundary=FRAME')\n",
      "            self.end_headers()\n",
      "            try:\n",
      "                while True:\n",
      "                    with output.condition:\n",
      "                        output.condition.wait()\n",
      "                        frame = output.frame\n",
      "                    self.wfile.write(b'--FRAME\\r\\n')\n",
      "                    self.send_header('Content-Type', 'image/jpeg')\n",
      "                    self.send_header('Content-Length', len(frame))\n",
      "                    self.end_headers()\n",
      "                    self.wfile.write(frame)\n",
      "                    self.wfile.write(b'\\r\\n')\n",
      "            except Exception as e:\n",
      "                logging.warning(\n",
      "                    'Removed streaming client %s: %s',\n",
      "                    self.client_address, str(e))\n",
      "        else:\n",
      "            self.send_error(404)\n",
      "            self.end_headers()\n",
      "\n",
      "class StreamingServer(socketserver.ThreadingMixIn, server.HTTPServer):\n",
      "    allow_reuse_address = True\n",
      "    daemon_threads = True\n",
      "\n",
      "with picamera.PiCamera(resolution='640x480', framerate=24) as camera:\n",
      "    output = StreamingOutput()\n",
      "    #Uncomment the next line to change your Pi's Camera rotation (in degrees)\n",
      "    #camera.rotation = 90\n",
      "    camera.start_recording(output, format='mjpeg')\n",
      "    try:\n",
      "        address = ('', 8000)\n",
      "        server = StreamingServer(address, StreamingHandler)\n",
      "        server.serve_forever()\n",
      "    finally:\n",
      "        camera.stop_recording()\n",
      "\n",
      "I want to be able to take a picture with the raspberry pi and be able to access it on either my phone gallery or in the web view itself.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 What does Python regard as “true”? [duplicate]\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084649/what-does-python-regard-as-true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I just came to know there are Truthy and Falsy values in python which are different from the normal True and False?\n",
      "Can someone please explain in depth what truthy and falsy values are? \n",
      "Where should I use them?\n",
      "What is the difference between truthy and True values and falsy and False values ?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "As the comments described, it just refers to values which are evaluated to True or False.\n",
      "For instance, to see if a list is not empty, instead of checking like this:\n",
      "if len(my_list) != 0:\n",
      "   print \"Not empty!\"\n",
      "\n",
      "You can simply do this:\n",
      "if my_list:\n",
      "   print \"Not empty!\"\n",
      "\n",
      "This is because some values, such as empty lists, are considered False when evaluated for a boolean value. Non-empty lists are True.\n",
      "Similarly for the integer 0, the empty string \"\", and so on, for False, and non-zero integers, non-empty strings, and so on, for True.\n",
      "The idea of terms like \"truthy\" and \"falsy\" simply refer to those values which are considered True in cases like those described above, and those which are considered False.\n",
      "For example, an empty list ([]) is considered \"falsy\", and a non-empty list (for example, [1]) is considered \"truthy\".\n",
      "See also this section of the documentation.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to remove unnecessary variables of a dataframe to predict a binary output?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084632/how-to-remove-unnecessary-variables-of-a-dataframe-to-predict-a-binary-output\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a data set with a huge amount of variables for an output that can either be A or not A. How can I remove the variables that are useless to predict the output.\n",
      "Example\n",
      "I have a data set of loans which status loan_status are currently Fully Paid or Charged OffSince the data set is very large, I would like to keep only the essential variables that would help me predicting the output.\n",
      ">>> subset.head()\n",
      "\n",
      "    id  member_id   loan_amnt   funded_amnt funded_amnt_inv term    int_rate    installment grade   sub_grade   emp_title   emp_length  home_ownership  annual_inc  verification_status issue_d loan_status pymnt_plan  url desc    purpose title   zip_code    addr_state  dti delinq_2yrs earliest_cr_line    inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc    pub_rec revol_bal   revol_util  total_acc   initial_list_status out_prncp   out_prncp_inv   total_pymnt total_pymnt_inv total_rec_prncp total_rec_int   total_rec_late_fee  recoveries  collection_recovery_fee last_pymnt_d    last_pymnt_amnt next_pymnt_d    last_credit_pull_d  collections_12_mths_ex_med  mths_since_last_major_derog policy_code application_type    annual_inc_joint    dti_joint   verification_status_joint   acc_now_delinq  tot_coll_amt    tot_cur_bal open_acc_6m open_act_il open_il_12m open_il_24m mths_since_rcnt_il  total_bal_il    il_util open_rv_12m open_rv_24m max_bal_bc  all_util    total_rev_hi_lim    inq_fi  total_cu_tl inq_last_12m    acc_open_past_24mths    avg_cur_bal bc_open_to_buy  bc_util chargeoff_within_12_mths    delinq_amnt mo_sin_old_il_acct  mo_sin_old_rev_tl_op    mo_sin_rcnt_rev_tl_op   mo_sin_rcnt_tl  mort_acc    mths_since_recent_bc    mths_since_recent_bc_dlq    mths_since_recent_inq   mths_since_recent_revol_delinq  num_accts_ever_120_pd   num_actv_bc_tl  num_actv_rev_tl num_bc_sats num_bc_tl   num_il_tl   num_op_rev_tl   num_rev_accts   num_rev_tl_bal_gt_0 num_sats    num_tl_120dpd_2m    num_tl_30dpd    num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75    pub_rec_bankruptcies    tax_liens   tot_hi_cred_lim total_bal_ex_mort   total_bc_limit  total_il_high_credit_limit  revol_bal_joint sec_app_earliest_cr_line    sec_app_inq_last_6mths  sec_app_mort_acc    sec_app_open_acc    sec_app_revol_util  sec_app_open_act_il sec_app_num_rev_accts   sec_app_chargeoff_within_12_mths    sec_app_collections_12_mths_ex_med  sec_app_mths_since_last_major_derog hardship_flag   hardship_type   hardship_reason hardship_status deferral_term   hardship_amount hardship_start_date hardship_end_date   payment_plan_start_date hardship_length hardship_dpd    hardship_loan_status    orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  hardship_last_payment_amount    debt_settlement_flag    debt_settlement_flag_date   settlement_status   settlement_date settlement_amount   settlement_percentage   settlement_term\n",
      "11  NaN NaN 10000   10000   10000.0 60 months   14.07%  233.05  C   C3  Teacher 4 years RENT    42000.0 Source Verified Mar-2018    Fully Paid  n   NaN NaN major_purchase  Major purchase  341xx   FL  24.69   0   Oct-2004    0   32.0    NaN 17  0   707 15.7%   34  w   0.0 0.0 11153.669505    11153.67    10000.00    1153.67 0.0 0.0 0.0 Mar-2019    10.38   NaN Jun-2019    0   40.0    1   Individual  NaN NaN NaN 0   0   93913   0   15  0   0   54.0    93206   116.0   0   1   707 111.0   4500    0   0   0   1   5524.0  3793.0  15.7    0   0   161.0   88  18  18  0   18.0    32.0    18.0    32.0    14  1   1   2   4   30  2   4   1   17  0.0 0   0   0   43.8    0.0 0   0   84930   93913   4500    80430   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "16  NaN NaN 7000    7000    7000.0  36 months   11.98%  232.44  B   B5  Parole  < 1 year    MORTGAGE    40000.0 Verified    Mar-2018    Fully Paid  n   NaN NaN home_improvement    Home improvement    797xx   TX  20.25   0   Mar-2007    0   60.0    NaN 13  0   5004    36% 29  w   0.0 0.0 7693.314943 7693.31 7000.00 693.31  0.0 0.0 0.0 Mar-2019    5364.25 NaN Mar-2019    0   60.0    1   Individual  NaN NaN NaN 0   0   131726  1   6   0   2   16.0    126722  102.0   2   2   3944    90.0    13900   2   1   4   4   10977.0 4996.0  50.0    0   0   122.0   132 1   1   0   10.0    64.0    5.0 60.0    3   2   2   3   4   19  7   10  2   13  0.0 0   0   2   89.7    33.3    0   0   132817  131726  10000   118917  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "17  NaN NaN 20000   20000   20000.0 60 months   26.77%  607.97  E   E5  Mental Health Provider  3 years RENT    33500.0 Not Verified    Mar-2018    Charged Off n   NaN NaN house   Home buying 604xx   IL  24.40   0   Aug-2008    1   NaN NaN 27  0   7364    46% 34  w   0.0 0.0 7236.150000 7236.15 2195.37 5040.78 0.0 0.0 0.0 Apr-2019    607.97  NaN Jun-2019    0   NaN 1   Individual  NaN NaN NaN 0   308 160804  0   21  0   0   29.0    153440  118.0   0   2   2607    110.0   16000   0   0   2   2   5956.0  2767.0  68.6    0   0   115.0   115 20  20  0   26.0    NaN 5.0 NaN 0   3   6   3   3   27  6   7   6   27  0.0 0   0   0   100.0   33.3    0   0   146514  160804  8800    130514  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "20  NaN NaN 21000   21000   21000.0 60 months   20.39%  560.94  D   D4  Machine operator    10+ years   OWN 85000.0 Source Verified Mar-2018    Fully Paid  n   NaN NaN house   Home buying 135xx   NY  15.76   1   Nov-2008    0   2.0 NaN 15  0   14591   34.2%   27  w   0.0 0.0 24217.170915    24217.17    21000.00    3217.17 0.0 0.0 0.0 Feb-2019    183.26  NaN May-2019    0   NaN 1   Individual  NaN NaN NaN 0   0   128270  1   1   2   2   7.0 37076   NaN 2   5   5354    34.0    42700   6   4   13  8   8551.0  16684.0 38.4    0   0   67.0    112 4   4   3   4.0 NaN 0.0 2.0 0   5   7   6   10  3   12  21  7   15  0.0 0   0   4   92.6    16.7    0   0   172433  51667   27100   39733   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN N   NaN NaN NaN NaN NaN NaN\n",
      "...\n",
      "\n",
      "My attempt\n",
      "I wasn't able to find a way to reduce the number of useless variables. I only used dataframe.describe() to know  a bit more about them. But even if you try to summarize all the loans, first, I'm not sure it helps me summarize, since I still have many variables, and, second, I'm not sure I do the calculation correctly. In fact, as you can read there :\n",
      "    id  member_id   loan_amnt   funded_amnt funded_amnt_inv term    int_rate    installment grade   sub_grade   emp_title   emp_length  home_ownership  annual_inc  verification_status issue_d loan_status pymnt_plan  url desc    purpose title   zip_code    addr_state  dti delinq_2yrs earliest_cr_line    inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc    pub_rec revol_bal   revol_util  total_acc   initial_list_status out_prncp   out_prncp_inv   total_pymnt total_pymnt_inv total_rec_prncp total_rec_int   total_rec_late_fee  recoveries  collection_recovery_fee last_pymnt_d    last_pymnt_amnt next_pymnt_d    last_credit_pull_d  collections_12_mths_ex_med  mths_since_last_major_derog policy_code application_type    annual_inc_joint    dti_joint   verification_status_joint   acc_now_delinq  tot_coll_amt    tot_cur_bal open_acc_6m open_act_il open_il_12m open_il_24m mths_since_rcnt_il  total_bal_il    il_util open_rv_12m open_rv_24m max_bal_bc  all_util    total_rev_hi_lim    inq_fi  total_cu_tl inq_last_12m    acc_open_past_24mths    avg_cur_bal bc_open_to_buy  bc_util chargeoff_within_12_mths    delinq_amnt mo_sin_old_il_acct  mo_sin_old_rev_tl_op    mo_sin_rcnt_rev_tl_op   mo_sin_rcnt_tl  mort_acc    mths_since_recent_bc    mths_since_recent_bc_dlq    mths_since_recent_inq   mths_since_recent_revol_delinq  num_accts_ever_120_pd   num_actv_bc_tl  num_actv_rev_tl num_bc_sats num_bc_tl   num_il_tl   num_op_rev_tl   num_rev_accts   num_rev_tl_bal_gt_0 num_sats    num_tl_120dpd_2m    num_tl_30dpd    num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75    pub_rec_bankruptcies    tax_liens   tot_hi_cred_lim total_bal_ex_mort   total_bc_limit  total_il_high_credit_limit  revol_bal_joint sec_app_earliest_cr_line    sec_app_inq_last_6mths  sec_app_mort_acc    sec_app_open_acc    sec_app_revol_util  sec_app_open_act_il sec_app_num_rev_accts   sec_app_chargeoff_within_12_mths    sec_app_collections_12_mths_ex_med  sec_app_mths_since_last_major_derog hardship_flag   hardship_type   hardship_reason hardship_status deferral_term   hardship_amount hardship_start_date hardship_end_date   payment_plan_start_date hardship_length hardship_dpd    hardship_loan_status    orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  hardship_last_payment_amount    debt_settlement_flag    debt_settlement_flag_date   settlement_status   settlement_date settlement_amount   settlement_percentage   settlement_term\n",
      "count   0.0 0.0 110909.000000   110909.000000   110909.000000   110909  110909  110909.000000   110909  110909  101148  101310  110909  1.109090e+05    110909  110909  110909  110909  0.0 0.0 110909  110909  110909  110909  110616.000000   110909.000000   110909  110909.000000   50074.000000    16811.000000    110909.000000   110909.000000   1.109090e+05    110739  110909.000000   110909  110909.0    110909.0    110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110201  110909.000000   0.0 110909  110909.000000   27474.000000    110909.0    110909  14193.000000    14193.000000    13876   110909.000000   1.109090e+05    1.109090e+05    110909.000000   110909.000000   110909.000000   110909.000000   106966.000000   1.109090e+05    92546.000000    110909.000000   110909.000000   110909.000000   110877.000000   1.109090e+05    110909.000000   110909.000000   110909.000000   110909.000000   110903.000000   109104.000000   109017.000000   110909.000000   110909.000000   106966.000000   110909.000000   110909.000000   110909.000000   110909.000000   109225.000000   23011.000000    101505.000000   32915.000000    110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   110909.000000   107934.0    110909.000000   110909.000000   110909.000000   110908.000000   109104.000000   110909.000000   110909.000000   1.109090e+05    1.109090e+05    110909.000000   1.109090e+05    14193.000000    14193   14193.000000    14193.000000    14193.000000    13921.000000    14193.000000    14193.000000    14193.000000    14193.000000    5062.000000 110909  131 131 131 131.0   131.000000  131 131 131 131.0   131.000000  131 26.000000   131.000000  131.000000  110909  1470    1470    1470    1470.000000 1470.000000 1470.000000\n",
      "unique  NaN NaN NaN NaN NaN 2   74  NaN 7   34  22652   11  4   NaN 3   6   2   1   NaN NaN 13  12  867 50  NaN NaN 614 NaN NaN NaN NaN NaN NaN 1042    NaN 2   NaN NaN NaN NaN NaN NaN NaN NaN NaN 18  NaN 0.0 20  NaN NaN NaN 2   NaN NaN 3   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 510 NaN NaN NaN NaN NaN NaN NaN NaN NaN 1   1   8   2   NaN NaN 11  9   9   NaN NaN 4   NaN NaN NaN 2   12  3   13  NaN NaN NaN\n",
      "top NaN NaN NaN NaN NaN 36 months   11.98%  NaN B   B5  Manager 10+ years   MORTGAGE    NaN Source Verified Apr-2018    Fully Paid  n   NaN NaN debt_consolidation  Debt consolidation  945xx   CA  NaN NaN Aug-2005    NaN NaN NaN NaN NaN NaN 0%  NaN w   NaN NaN NaN NaN NaN NaN NaN NaN NaN Jan-2019    NaN NaN Jun-2019    NaN NaN NaN Individual  NaN NaN Not Verified    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aug-2005    NaN NaN NaN NaN NaN NaN NaN NaN NaN N   INTEREST ONLY-3 MONTHS DEFERRAL UNEMPLOYMENT    BROKEN  NaN NaN Mar-2019    Apr-2019    Mar-2019    NaN NaN Late (16-30 days)   NaN NaN NaN N   Jun-2019    ACTIVE  May-2019    NaN NaN NaN\n",
      "freq    NaN NaN NaN NaN NaN 81370   3485    NaN 30833   6964    2030    36774   54812   NaN 44735   29655   87515   110909  NaN NaN 56217   56217   1223    15847   NaN NaN 987 NaN NaN NaN NaN NaN NaN 1843    NaN 92893   NaN NaN NaN NaN NaN NaN NaN NaN NaN 9952    NaN NaN 47569   NaN NaN NaN 96716   NaN NaN 6353    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 164 NaN NaN NaN NaN NaN NaN NaN NaN NaN 110909  131 55  105 NaN NaN 27  27  34  NaN NaN 68  NaN NaN NaN 109439  374 1182    320 NaN NaN NaN\n",
      "mean    NaN NaN 15133.124228    15133.124228    15129.913485    NaN NaN 451.703280  NaN NaN NaN NaN NaN 7.866525e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 18.658485   0.240945    NaN 0.543842    36.703838   82.156683   11.373595   0.162151    1.414609e+04    NaN 23.704154   NaN 0.0 0.0 13093.720734    13090.819511    11834.579108    1065.864157 1.193620    192.083862  34.107295   NaN 8988.970489 NaN NaN 0.018060    45.958725   1.0 NaN 123220.229856   19.046298   NaN 0.000045    3.007498e+02    1.487752e+05    1.048427    2.624494    0.845035    1.801720    18.464428   3.570383e+04    71.071878   1.399805    2.941727    5191.785689 53.011716   3.665310e+04    1.330749    1.696274    2.404187    5.083699    14573.002065    15451.859776    45.630734   0.008728    3.745034    121.109044  171.096953  13.539704   7.627740    1.476652    23.266752   40.444831   6.617723    37.548321   0.499932    3.305349    4.951492    4.712936    7.286199    8.664689    8.084673    13.359015   4.904345    11.346311   0.0 0.000045    0.066108    2.404593    94.636698   29.690390   0.145534    0.016464    1.886942e+05    5.006460e+04    25196.060734    4.486298e+04    32620.767914    NaN 0.753047    1.609244    11.366589   55.994512   2.893328    12.913619   0.052913    0.083492    36.830304   NaN NaN NaN NaN 3.0 194.040611  NaN NaN NaN 3.0 15.007634   NaN 677.778462  14977.221908    197.404962  NaN NaN NaN NaN 7235.872456 51.259034   18.638776\n",
      "std NaN NaN 10063.626492    10063.626492    10062.605730    NaN NaN 291.529754  NaN NaN NaN NaN NaN 8.273838e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 20.105508   0.759381    NaN 0.791792    21.864945   23.731842   5.938104    0.445418    2.109029e+04    NaN 12.636174   NaN 0.0 0.0 10677.374875    10675.875613    10274.588973    1127.992027 8.430334    844.626003  150.986459  NaN 9417.206405 NaN NaN 0.149077    21.220192   0.0 NaN 61899.006621    8.171933    NaN 0.006714    2.447485e+04    1.670928e+05    1.212451    2.877455    1.041107    1.708049    23.830165   4.476172e+04    22.776782   1.595896    2.734340    5728.066547 22.057862   3.675638e+04    1.715969    2.911280    2.682558    3.482714    17992.870300    20176.943227    29.912941   0.107913    340.178661  54.487416   97.531613   17.183313   8.446866    1.798038    32.073651   22.185204   5.688223    22.098123   1.393569    2.312572    3.307021    3.167090    4.704991    7.612502    4.949893    8.197814    3.218891    5.928676    0.0 0.006714    0.438064    2.050780    8.906956    34.368549   0.359703    0.254046    1.863843e+05    5.154504e+04    25322.554746    4.691188e+04    28067.364794    NaN 1.108586    1.836520    6.574821    26.571334   3.085046    8.499611    0.510133    0.386138    24.069226   NaN NaN NaN NaN 0.0 144.144733  NaN NaN NaN 0.0 8.734103    NaN 445.752949  9097.533521 207.337176  NaN NaN NaN NaN 5009.981486 8.231359    6.806635\n",
      "min NaN NaN 1000.000000 1000.000000 1000.000000 NaN NaN 30.120000   NaN NaN NaN NaN NaN 0.000000e+00    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.000000    0.000000    NaN 0.000000    0.000000    3.000000    0.000000    0.000000    0.000000e+00    NaN 2.000000    NaN 0.0 0.0 0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    NaN 0.000000    NaN NaN 0.000000    0.000000    1.0 NaN 15400.000000    0.190000    NaN 0.000000    0.000000e+00    0.000000e+00    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000e+00    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000e+00    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    1.000000    2.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    2.000000    0.000000    0.000000    0.0 0.000000    0.000000    0.000000    12.500000   0.000000    0.000000    0.000000    0.000000e+00    0.000000e+00    0.000000    0.000000e+00    0.000000    NaN 0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    NaN NaN NaN NaN 3.0 5.950000    NaN NaN NaN 3.0 0.000000    NaN 151.860000  424.110000  0.070000    NaN NaN NaN NaN 413.930000  29.920000   1.000000\n",
      "25% NaN NaN 7200.000000 7200.000000 7200.000000 NaN NaN 232.750000  NaN NaN NaN NaN NaN 4.595300e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 10.400000   0.000000    NaN 0.000000    18.000000   67.000000   7.000000    0.000000    4.085000e+03    NaN 15.000000   NaN 0.0 0.0 5007.935312 5007.250000 3500.000000 292.490000  0.000000    0.000000    0.000000    NaN 823.590000  NaN NaN 0.000000    29.000000   1.0 NaN 83862.000000    13.000000   NaN 0.000000    0.000000e+00    2.739200e+04    0.000000    1.000000    0.000000    1.000000    6.000000    8.377000e+03    58.000000   0.000000    1.000000    1747.000000 38.000000   1.530000e+04    0.000000    0.000000    1.000000    3.000000    2958.000000 2983.000000 19.900000   0.000000    0.000000    83.000000   104.000000  4.000000    3.000000    0.000000    6.000000    23.000000   2.000000    20.000000   0.000000    2.000000    3.000000    3.000000    4.000000    4.000000    5.000000    8.000000    3.000000    7.000000    0.0 0.000000    0.000000    1.000000    92.300000   0.000000    0.000000    0.000000    5.299800e+04    1.849600e+04    8900.000000 1.466100e+04    14026.000000    NaN 0.000000    0.000000    7.000000    36.500000   1.000000    7.000000    0.000000    0.000000    15.000000   NaN NaN NaN NaN 3.0 90.360000   NaN NaN NaN 3.0 7.000000    NaN 338.100000  7428.245000 40.040000   NaN NaN NaN NaN 3466.597500 45.000000   18.000000\n",
      "50% NaN NaN 12000.000000    12000.000000    12000.000000    NaN NaN 368.690000  NaN NaN NaN NaN NaN 6.500000e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 16.780000   0.000000    NaN 0.000000    34.000000   86.000000   10.000000   0.000000    9.061000e+03    NaN 22.000000   NaN 0.0 0.0 10297.070860    10294.240000    10000.000000    696.570000  0.000000    0.000000    0.000000    NaN 6119.670000 NaN NaN 0.000000    46.000000   1.0 NaN 110000.000000   18.560000   NaN 0.000000    0.000000e+00    8.415200e+04    1.000000    2.000000    1.000000    1.000000    11.000000   2.329500e+04    74.000000   1.000000    2.000000    3859.000000 54.000000   2.710000e+04    1.000000    0.000000    2.000000    4.000000    7869.000000 8439.000000 43.100000   0.000000    0.000000    128.000000  154.000000  8.000000    5.000000    1.000000    13.000000   39.000000   5.000000    35.000000   0.000000    3.000000    4.000000    4.000000    6.000000    7.000000    7.000000    12.000000   4.000000    10.000000   0.0 0.000000    0.000000    2.000000    100.000000  18.200000   0.000000    0.000000    1.248450e+05    3.653800e+04    17800.000000    3.328500e+04    25326.000000    NaN 0.000000    1.000000    10.000000   57.800000   2.000000    11.000000   0.000000    0.000000    35.000000   NaN NaN NaN NaN 3.0 146.390000  NaN NaN NaN 3.0 17.000000   NaN 367.950000  15652.360000    146.070000  NaN NaN NaN NaN 5891.670000 45.230000   20.000000\n",
      "75% NaN NaN 20000.000000    20000.000000    20000.000000    NaN NaN 620.660000  NaN NaN NaN NaN NaN 9.500000e+04    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 24.122500   0.000000    NaN 1.000000    53.000000   101.000000  14.000000   0.000000    1.725500e+04    NaN 31.000000   NaN 0.0 0.0 18766.242514    18755.060000    17500.000000    1443.380000 0.000000    0.000000    0.000000    NaN 13912.100000    NaN NaN 0.000000    63.000000   1.0 NaN 148000.000000   24.650000   NaN 0.000000    0.000000e+00    2.285040e+05    2.000000    3.000000    1.000000    3.000000    21.000000   4.672800e+04    87.000000   2.000000    4.000000    6943.000000 69.000000   4.640000e+04    2.000000    2.000000    3.000000    7.000000    20477.000000    19997.000000    70.500000   0.000000    0.000000    152.000000  220.000000  17.000000   10.000000   2.000000    27.000000   57.000000   10.000000   53.000000   0.000000    4.000000    6.000000    6.000000    9.000000    11.000000   10.000000   17.000000   6.000000    14.000000   0.0 0.000000    0.000000    3.000000    100.000000  50.000000   0.000000    0.000000    2.771000e+05    6.409300e+04    33000.000000    6.074100e+04    41986.000000    NaN 1.000000    3.000000    15.000000   77.300000   4.000000    17.000000   0.000000    0.000000    57.000000   NaN NaN NaN NaN 3.0 279.065000  NaN NaN NaN 3.0 23.000000   NaN 1193.032500 22114.265000    291.190000  NaN NaN NaN NaN 10090.202500    60.000000   24.000000\n",
      "max NaN NaN 40000.000000    40000.000000    40000.000000    NaN NaN 1628.080000 NaN NaN NaN NaN NaN 9.300000e+06    NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 999.000000  19.000000   NaN 5.000000    226.000000  126.000000  86.000000   52.000000   1.113293e+06    NaN 129.000000  NaN 0.0 0.0 51653.389338    51653.390000    40000.000000    13842.210000    320.700000  33122.070000    5961.972600 NaN 41353.670000    NaN NaN 8.000000    226.000000  1.0 NaN 1000000.000000  39.980000   NaN 1.000000    6.214661e+06    4.151547e+06    13.000000   49.000000   6.000000    16.000000   383.000000  1.378570e+06    309.000000  22.000000   28.000000   389468.000000   175.000000  1.680300e+06    32.000000   40.000000   67.000000   33.000000   393082.000000   331957.000000   146.600000  9.000000    65000.000000    518.000000  806.000000  368.000000  260.000000  24.000000   551.000000  152.000000  24.000000   152.000000  36.000000   50.000000   72.000000   59.000000   66.000000   105.000000  72.000000   94.000000   65.000000   82.000000   0.0 1.000000    17.000000   23.000000   100.000000  100.000000  5.000000    52.000000   4.358152e+06    1.394335e+06    460900.000000   1.380346e+06    280272.000000   NaN 6.000000    17.000000   58.000000   212.600000  35.000000   75.000000   20.000000   11.000000   117.000000  NaN NaN NaN NaN 3.0 649.970000  NaN NaN NaN 3.0 29.000000   NaN 1369.860000 32300.260000    1072.990000 NaN NaN NaN NaN 28503.000000    80.000000   24.000000\n",
      "\n",
      "It seems unlikely that the loan_amnt count is 110909.000000 when you see the first 4 lines (and there are many).\n",
      "Annex : how to recreate the dataset\n",
      "I used a csv file that I downloaded here (bank loans for 2018. They are divided into four quarters). Using Python 3 can be obtained by doing:\n",
      "import pandas as pd \n",
      "# Control delimiters, rows, column names with read_csv (see later) \n",
      "data_Q1 = pd.read_csv(\"LoanStats_2018Q1.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "data_Q2 = pd.read_csv(\"LoanStats_2018Q2.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "data_Q3 = pd.read_csv(\"LoanStats_2018Q2.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "data_Q4 = pd.read_csv(\"LoanStats_2018Q2.csv\", skiprows=1, skipfooter=2, engine='python')\n",
      "frames = [data_Q1,data_Q2,data_Q3,data_Q4]\n",
      "\n",
      "result = pd.concat(frames)\n",
      "subset = result.loc[result[\"loan_status\"].isin(['Charged Off','Fully Paid'])]\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Printing into a txt file\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084629/printing-into-a-txt-file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to print in a txt file only then a specific event, that is my code:\n",
      "txt = tutto()\n",
      "file = open(\"psw.txt\", \"w\")\n",
      "file.write(str(txt))\n",
      "file.close()\n",
      "\n",
      "tutto() is a def ( def tutto(): ). In tutto there are a lot of print commands. How can I redirect all these print commands only after tutto() is started?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "print accepts a file parameter that designates where the output will be printed to. By default, the location is sys.stdout (which is normally your terminal). You can modify the definition of tutto so that it uses a special print function. functools.partial is used to help you create your own print function so that you don't have to type print(\"...\", file = fout) everywhere. Once tutto ends, print will return to its normal behavior since you are only changing it's definition in the scope of tutto. \n",
      "import sys\n",
      "from functools import partial\n",
      "\n",
      "def tutto(fout = sys.stdout):\n",
      "    print = partial(__builtins__.print, file = fout)\n",
      "    print(\"Here are all\")\n",
      "    print(\"my print statements\")\n",
      "    print(\"They will automatically show up in\")\n",
      "    print(\"The file designated by fout...\")\n",
      "    return\n",
      "\n",
      "with open(\"Sample.txt\", \"w\") as f:\n",
      "    print(\"Calling tutto\")\n",
      "    tutto(f)\n",
      "    print(\"Tutto is finished, and I'll appear in your terminal.\")\n",
      "\n",
      "Outputs:\n",
      "Terminal:\n",
      "Calling tutto\n",
      "Tutto is finished, and I'll appear in your terminal.\n",
      "\n",
      "Sample.txt:\n",
      "Here are all\n",
      "my print statements\n",
      "They will automatically show up in\n",
      "The file designated by fout...\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 What is the behavior of sorted(arr) when arr is a numpy array with nans? [duplicate]\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084592/what-is-the-behavior-of-sortedarr-when-arr-is-a-numpy-array-with-nans\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "sorted([2, float('nan'), 1]) returns [2, nan, 1]\n",
      "(At least on Activestate Python 3.1 implementation.)\n",
      "I understand nan is a weird object, so I wouldn't be surprised if it shows up in random places in the sort result. But it also messes up the sort for the non-nan numbers in the container, which is really unexpected.\n",
      "I asked a related question about max, and based on that I understand why sort works like this. But should this be considered a bug?\n",
      "Documentation just says \"Return a new sorted list [...]\" without specifying any details.\n",
      "EDIT: \n",
      "I now agree that this isn't in violation of the IEEE standard. However, it's a bug from any common sense viewpoint, I think. Even  Microsoft, which isn't known to admit their mistakes often, has recognized this one as a bug, and fixed it in the latest version: http://connect.microsoft.com/VisualStudio/feedback/details/363379/bug-in-list-double-sort-in-list-which-contains-double-nan.\n",
      "Anyway, I ended up following @khachik's answer:\n",
      "sorted(list_, key = lambda x : float('-inf') if math.isnan(x) else x)\n",
      "\n",
      "I suspect it results in a performance hit compared to the language doing that by default, but at least it works (barring any bugs that I introduced).\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "The previous answers are useful, but perhaps not clear regarding the root of the problem.\n",
      "In any language, sort applies a given ordering, defined by a comparison function or in some other way, over the domain of the input values.  For example, less-than, a.k.a. operator <, could be used throughout if and only if less than defines a suitable ordering over the input values.\n",
      "But this is specifically NOT true for floating point values and less-than:\n",
      "\"NaN is unordered: it is not equal to, greater than, or less than anything, including itself.\" (Clear prose from GNU C manual, but applies to all modern IEEE754 based floating point)\n",
      "So the possible solutions are:\n",
      "\n",
      "\n",
      "remove the NaNs first, making the input domain well defined via <\n",
      "  (or the other sorting function being used)\n",
      "define a custom comparison function (a.k.a. predicate) that does\n",
      "  define an ordering for NaN, such as less than any number, or greater\n",
      "  than any number.\n",
      "\n",
      "\n",
      "Either approach can be used, in any language.\n",
      "Practically, considering python, I would prefer to remove the NaNs if you either don't care much about fastest performance or if removing NaNs is a desired behavior in context. \n",
      "Otherwise you could use a suitable predicate function via \"cmp\" in older python versions, or via this and  functools.cmp_to_key().  The latter is a bit more awkward, naturally, than removing the NaNs first. And care will be required to avoid worse performance, when defining this predicate function.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 APScheduler not remembering jobs after Heroku sleeps\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084554/apscheduler-not-remembering-jobs-after-heroku-sleeps\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am currently trying to implement APScheduler through Heroku. I have the scheduler working through Heroku with Python just as expected when the Heroku is running. The issue I am running into is when the Heroku dyno goes to sleep after 30 minutes of inactivity. I want to be able to make Heroku active again and have the scheduler remember all of the jobs that were not triggered before it went to sleep. I am using the SQLAlchemyJobStore which is supposed to remember jobs after the scheduler is shut down.\n",
      "Here is the code that I am using currently:\n",
      "from apscheduler.schedulers.blocking import BlockingScheduler\n",
      "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
      "\n",
      "jobstores = {\n",
      "    'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
      "}\n",
      "job_defaults = {\n",
      "    'coalesce': False,\n",
      "    'max_instances': 1\n",
      "}\n",
      "\n",
      "sched = BlockingScheduler()\n",
      "sched.configure(jobstores=jobstores, job_defaults=job_defaults, timezone='America/New_York')\n",
      "\n",
      "def my_job(text):\n",
      "    print(text)\n",
      "\n",
      "def add_jobs():\n",
      "    print(\"scheduling\")\n",
      "    sched.add_job(my_job, 'date', run_date='2019-7-17 15:25:00', args=['date job firing'], id = \"Job1\", misfire_grace_time = 18000)\n",
      "    sched.add_job(my_job, 'date', run_date='2019-7-17 15:30:00', args=['date to run'], id = \"Job2\", misfire_grace_time=18000)\n",
      "    sched.add_job(my_job, 'date', run_date='2019-7-17 16:30:00', args=['job after shutdown'], id = 'Job3', misfire_grace_time = 18000)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    add_jobs()\n",
      "    #sched.print_jobs()\n",
      "    sched.start()\n",
      "\n",
      "In this case, I commit and push the code to heroku, which runs this file in heroku. The results I get are as followed:\n",
      "2019-07-17T19:24:38.000000+00:00 app[api]: Build started by user\n",
      "2019-07-17T19:25:03.445241+00:00 heroku[clock.1]: State changed from down to starting\n",
      "2019-07-17T19:25:03.227462+00:00 app[api]: Release v268 created by user \n",
      "2019-07-17T19:25:03.227462+00:00 app[api]: Deploy 6d599479 by user\n",
      "2019-07-17T19:25:09.364735+00:00 heroku[clock.1]: Starting process with command `python clock.py`\n",
      "2019-07-17T19:25:10.118949+00:00 heroku[clock.1]: State changed from starting to up\n",
      "2019-07-17T19:25:13.928223+00:00 app[clock.1]: scheduling\n",
      "2019-07-17T19:25:14.142912+00:00 app[clock.1]: date job firing\n",
      "2019-07-17T19:25:14.000000+00:00 app[api]: Build succeeded\n",
      "2019-07-17T19:30:00.051064+00:00 app[clock.1]: date to run\n",
      "\n",
      "30 minutes after the the previous job triggered, Heroku sleeps. This is when I ping Heroku to wake up and then the last job at 16:30:00 does not trigger at the time it should. \n",
      "I believe the issue might have to do with the database. I am not sure what is in the database at any certain time or if the scheduler is pulling the non executed jobs every time Heroku goes to sleep. \n",
      "If anyone knows why this is happening that would be very helpful.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You are using an SQLite database to store your jobs. This doesn't play nicely with Heroku's ephemeral filesystem, which loses all changes every time your dyno restarts. This happens frequently (at least once per day) and likely occurs when your app sleeps and then wakes back up.\n",
      "You should use a client-server database instead. Heroku's Postgres service might be a better choice. It's supported out of the box and the base plan is free.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How do I override QListWidget dragged items?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084553/how-do-i-override-qlistwidget-dragged-items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a QListWidget populated with file paths (i.e \"C:/Folder/file.ext\"). I have the list set to drag and drop via qlistwidget.setDragDropMode(QAbstractItemView.DragDrop)\n",
      "And it works, but it drags the text it looks like?\n",
      "I want to convert the path to a URL, so I can drag the file in the listwidget into other programs, an explorer window, etc.\n",
      "Is there an event I can override to override the content of the drag?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "not sure what the correct way to do it is, but here's what worked for me:\n",
      "I created a new MyListWidget class like used here:\n",
      "Catch which mousebutton is pressed on item\n",
      "and created a new dragLeaveEvent functionm, and made it create a QMimeData element like described here:\n",
      "https://wiki.python.org/moin/PyQt/Exporting%20a%20file%20to%20other%20applications\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 migrating problems when porting django project to python 3 and django 2\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084552/migrating-problems-when-porting-django-project-to-python-3-and-django-2\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I've been porting a Django project to python 3 and Django 2. I have had to add on_delete to all my models with foreign keys as required in Django 2. Now I have tried to make migrations for those changes have been getting TypeError: __init__() missing 1 required positional argument: 'on_delete' the file it references is the 0002 migration file not the models file which has been updated. I am not sure how to go about fixing this. I have tried faking the migrations and I still get the same error. I am not sure why it thinks the database doesn't exist, I have checked and everything is intact and working in postgres. Any ideas?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Since django-2.0 ForeignKey fields [Django-doc] and OneToOneField fields fields now have a required on_delete parameter.\n",
      "This is specified in the release notes of Django-2.0 under Features removed in 2.0:\n",
      "\n",
      "The on_delete argument for ForeignKey and OneToOneField is now required in models and migrations. Consider squashing migrations so that you have fewer of them to update.\n",
      "\n",
      "You thus should inspect your migration files for ForeignKeys and OneToOneFields, and add an on_delete parameter, like:\n",
      "class Migration(migrations.Migration):\n",
      "\n",
      "    initial = False\n",
      "\n",
      "    dependencies = [\n",
      "        ('app', '0001_initial'),\n",
      "    ]\n",
      "\n",
      "    operations = [\n",
      "        migrations.CreateModel(\n",
      "            name='Model',\n",
      "            fields=[\n",
      "                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n",
      "                ('some_foreignkey', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='app.OtherModel')),\n",
      "            ],\n",
      "        ),\n",
      "    ]\n",
      "You should inspect the documentation on the on_delete parameter to see what deletion strategy is the best for each situation. The options are, at the time of writing CASCADE, PROTECT, SET_NULL, SET_DEFAULT, SET(..), DO_NOTHING.\n",
      "If you did not specify the on_delete in the pre-django-2.0 versions, it made a default to CASCADE. So if you want the same behavior, you should add on_delete=django.db.models.deletion.CASCADE. This is noted in the 1.11 version of the documentation on on_delete:\n",
      "\n",
      "Deprecated since version 1.9: on_delete will become a required argument in Django 2.0. In older versions it defaults to CASCADE.\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Importing modules is consuming too much memory\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084532/importing-modules-is-consuming-too-much-memory\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a pretty big application, that uses a lot of different libraries.\n",
      "The result is that memory consumption is a bit high. Just an import of python pandas consumes over 50mb on my system. In total, the process needs more than 200mb to run. That's too much, especially if multiple processes are started. \n",
      "Is there some way to optimize memory consumption? Or is it normal that just an import of python pandas already needs more than 50 MB?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Creating multiple dataframes (2) based off value in one column and stop when another value is observed\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084530/creating-multiple-dataframes-2-based-off-value-in-one-column-and-stop-when-ano\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "A dataframe structure looks like so:\n",
      "Date   Amount1     Amount2     Amount3\n",
      "\n",
      "NaN     Port        NaN        thing1\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3      \n",
      "NaN     Port        NaN        thing2\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3  \n",
      "Total     Nan        NaN        NaN\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3  \n",
      "NaN        Nan       Nan         Nan\n",
      "\n",
      "I am interested in creating 2 dataframes.\n",
      "1.One dataframe collects the rows after thing2 is observed in the amount3 column and stops the row before total is observed in the date column.\n",
      "2.The 2nd dataframe will start on the row after total is observed and stop when a NaN (null) value is observed. \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can do with idxmax and .iloc \n",
      "df1=df.loc[df.Amount3.eq('thing2').idxmax():df.Date.eq('Total').idxmax()-1].copy()\n",
      "df2=df.loc[df.Date.eq('Total').idxmax():]\n",
      "df2=df2.loc[:df2.Date.isnull().idxmax()-1]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Scrapy: ROBOTSTXT_OBEY=True gives an error\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084508/scrapy-robotstxt-obey-true-gives-an-error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "When setting ROBOTSTXT_OBEY=True in scrapy settings I get the following error: \n",
      "    TypeError: to_bytes must receive a unicode, str or bytes object, got list.\n",
      "I've tried multiple websites to see if it was an issue with the robots.txt file, but i get the same error for all websites.  Even for google \n",
      "    scrapy shell https://www.google.com/ --set=\"ROBOTSTXT_OBEY=True\"\n",
      "TypeError: to_bytes must receive a unicode, str or bytes object, got list\n",
      "\n",
      "How can I respect robots.txt and crawl using scrapy?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Found out the issue.  When USER_AGENT is set I get the error.  If no USER_AGENT is set then ROBOTSTXT_OBEY=True works without error.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Return rows with max/min values at bottom of dataframe (python/pandas)\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084473/return-rows-with-max-min-values-at-bottom-of-dataframe-python-pandas\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to write a function that can look at a dataframe, find the max or min value in a specified column, then return the entire datafrane with the row(s) containing the max or min value at the bottom.\n",
      "I have made it so that the rows with the max or min value alone get returned.\n",
      "def findAggregate(df, transType, columnName=None):\n",
      "\n",
      "    if transType == 'max1Column':\n",
      "        return df[df[columnName] == df[columnName].max()]\n",
      "\n",
      "    elif transType == 'min1Column':\n",
      "        return df[df[columnName] == df[columnName].min()]\n",
      "\n",
      "Given the dataframe below, I want to check col2 for the MIN value\n",
      "Original Dataframe:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "orange   18       cat\n",
      "black    6        fish\n",
      "\n",
      "Expected output:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "orange   18       cat\n",
      "black    6        fish\n",
      "blue     2        dog\n",
      "\n",
      "Actual output:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Use idxmin or idxmax:\n",
      "df.append(df.iloc[df['col2'].idxmin()], ignore_index=True)\n",
      "\n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "3    blue     2   dog\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to fix 'ModuleNotFoundError: ' for a Python Service?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084443/how-to-fix-modulenotfounderror-for-a-python-service\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Module name is PService.py.\n",
      "PService (_svc_name_) and Reader (_svc_display_name_)\n",
      "Using python PService.py install, I've made a service and shows up in the services.msc.\n",
      "Going through lots of different suggestions, spent almost all day trying to fix, however, I was not able to solve ModuleNotFoundError: No module named 'PService'..\n",
      "I've read through all resources that I can possibly find, some had same error, but there was no solutions..\n",
      "This is running on Windows Server 2012 R2, I am using Python 3.6.\n",
      "\n",
      "Y:\\>python PService.py debug\n",
      "Debugging service PService - press Ctrl+C to stop.\n",
      "Error 0xC000000A - Python could not construct the class instance\n",
      "\n",
      "<Error getting traceback - traceback.print_exception() failed\n",
      "\n",
      "(null): (null)\n",
      "\n",
      "\n",
      "Services:\n",
      "Windows could not start the SiteReader on Local Computer. For more information, review the System Event log... refer to service-specific error code 1.\n",
      "\n",
      "System Event Properties:\n",
      "The Reader service terminated with the following service-specific error: \n",
      "Incorrect function.\n",
      "\n",
      "Application Event Properties: (Source: Python Service)\n",
      "Python could not import the service's module \n",
      "ModuleNotFoundError: No module named 'PService' \n",
      "%2: %3\n",
      "Event Id: 4\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to Reduce the Install Size of a Spacy based Python Application\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084413/how-to-reduce-the-install-size-of-a-spacy-based-python-application\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm building a docker container that contains a the Python library spacy. I'm now trying to reduce the size of this container, and spacy appears to be the main contributor to the disk size.\n",
      "Without any models installed, and without any other code/dependencies etc, spacy consumes around 500MB of disk when installed! Does anyone have any useful hints/tips on installing spacy in a disk-space-friendly manner. \n",
      "My repro steps are:\n",
      "mkdir foo1                  # create a folder \n",
      "cd foo1                     # change directory\n",
      "python3 -m venv .venv       # create virtual environment\n",
      "source .venv/bin/activate   # activate virtual environment\n",
      "pip install --upgrade pip   # upgrade pip\n",
      "pip install spacy           # install spacy\n",
      "\n",
      "After doing this, I then navigate into the following folder...\n",
      "foo1/.venv/lib/python3.7/site-packages\n",
      "... and can see that the spacy folder is very large:\n",
      "$ du -sh spacy\n",
      "425M    spacy\n",
      "\n",
      "Specifically, it's the language folder that's large:\n",
      "$ du -sh spacy/lang\n",
      "401M spacy/lang\n",
      "\n",
      "There are 52 languages in that folder, and for many situations I only care about one or two languages. Specifically, for my current situation, that's English.\n",
      "When I look at the sizes, English is the 14th largest (only showing the top 14 in this list)...\n",
      "$ du -sH spacy/lang/* | sort -n -r \n",
      "\n",
      "142024 spacy/lang/tr\n",
      "86608 spacy/lang/pt\n",
      "78368 spacy/lang/nb\n",
      "76592 spacy/lang/da\n",
      "74840 spacy/lang/sv\n",
      "60672 spacy/lang/ca\n",
      "50880 spacy/lang/es\n",
      "48296 spacy/lang/fr\n",
      "41688 spacy/lang/de\n",
      "36960 spacy/lang/nl\n",
      "34008 spacy/lang/it\n",
      "32632 spacy/lang/ro\n",
      "24160 spacy/lang/lt\n",
      "8712 spacy/lang/en  <--- THE ONLY ONE I WANT\n",
      "\n",
      "Is there a spacy-specifc way of installing spacy without all of these languages?\n",
      "I can hack around post-install, but is there a safer way to install fewer languages?\n",
      "Versions installed, on MacOS, by the above steps are as follows:\n",
      "$ pip freeze\n",
      "blis==0.2.4\n",
      "certifi==2019.6.16\n",
      "chardet==3.0.4\n",
      "cymem==2.0.2\n",
      "idna==2.8\n",
      "murmurhash==1.0.2\n",
      "numpy==1.16.4\n",
      "plac==0.9.6\n",
      "preshed==2.0.1\n",
      "requests==2.22.0\n",
      "spacy==2.1.6\n",
      "srsly==0.0.7\n",
      "thinc==7.0.8\n",
      "tqdm==4.32.2\n",
      "urllib3==1.25.3\n",
      "wasabi==0.2.2\n",
      "\n",
      "$ python --version\n",
      "Python 3.7.4\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "I raised this as an issue against the spacy project on GitHub, and it looks like this is a known issue, and that there are plans to address the size of spacy installs.\n",
      "https://github.com/explosion/spaCy/issues/3983\n",
      "So, at this time, there isn't a supported/recommended way to reduce the size of the package install.\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# In[1]:\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "from urllib.request import urlopen\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# 검색어 설정\n",
    "tag = input(\"검색어를 입력하십시오-->\")\n",
    "\n",
    "# url 설정 및 url 읽기\n",
    "url = \"https://stackoverflow.com/questions/tagged/\" + tag\n",
    "page = urlopen(url) \n",
    "document = page.read()\n",
    "\n",
    "# 객체 생성 및 파일 읽기\n",
    "soup = BeautifulSoup(document, \"html.parser\")\n",
    "questions = soup.find(id=\"questions\")\n",
    "questions_list=questions.find_all(\"a\", class_=\"question-hyperlink\")\n",
    "\n",
    "# 메인 알고리즘\n",
    "for questions in questions_list:\n",
    "    \n",
    "    print(\"@\"*50, \"제목\", \"@\"*50)\n",
    "    \n",
    "    # Q&A 질문 요약\n",
    "    print(\"질문\",questions.get_text()) # 조금 더 가벼움!\n",
    "    \n",
    "    # Q&A 링크 생성\n",
    "    print(\"=\"*100)\n",
    "    print(\"링크:\",\"https://stackoverflow.com\"+questions.get(\"href\"))\n",
    "          \n",
    "    \n",
    "    # Q&A 링크 읽기\n",
    "    qna_url=\"https://stackoverflow.com\"+questions.get(\"href\")\n",
    "    qna_page = urlopen(qna_url) \n",
    "    qna_document = qna_page.read()\n",
    "    \n",
    "    # 객체 생성 및 파일 읽기\n",
    "    soup_qna = BeautifulSoup(qna_document, \"html.parser\")\n",
    "    qna_questions = soup_qna.find(class_=\"question\")\n",
    "    \n",
    "    # 질문 내용 출력\n",
    "    print(\"*\"*50 ,\"질문 내용\", \"*\"*50)\n",
    "    print(qna_questions.select(\"div.post-text\")[0].text)\n",
    "    \n",
    "    # 답변 내용 출력\n",
    "    print(\"*\"*50 ,\"답변 내용\", \"*\"*50)\n",
    "    qna_answers = soup_qna.find(id=\"answers\")\n",
    "    if qna_answers.select_one(\"div.answer > div.post-layout > div.answercell > div.post-text\") == None:\n",
    "        print(\"#\"*50,\"답변이 없습니다\",\"#\"*50)\n",
    "    else:\n",
    "        print(qna_answers.select_one(\"div.answer > div.post-layout > div.answercell > div.post-text\").text)\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력하십시오-->pandas\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to assign value to a pandas dataframe, when subset by complex index and boolean based conditions?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085178/how-to-assign-value-to-a-pandas-dataframe-when-subset-by-complex-index-and-bool\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I would like to replace values in a pandas dataframe, with a complex subsetting pattern. \n",
      "With the .loc accessor, I was only able to subset by chaining multiple conditions, because some of the conditions are index based. But it seems I can not assign values after such a chain of subsetting.\n",
      "UPDATE: A further problem is caused by the duplicated indicies. I have updated the example accordingly.\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame({'a': ['foo'] * 10 + ['bar'] * 10, 'b': range(20)}, index=pd.date_range('2019-01-01','2019-01-10').append(pd.date_range('2019-01-01','2019-01-10')))\n",
      "\n",
      "df.loc[df['a'] == 'foo', 'b'].loc[pd.to_datetime(['2019-01-05','2019-01-09'])] = np.nan\n",
      "\n",
      "df\n",
      "\n",
      "Result:\n",
      "              a     b\n",
      "2019-01-01  foo     0\n",
      "2019-01-02  foo     1\n",
      "2019-01-03  foo     2\n",
      "2019-01-04  foo     3\n",
      "2019-01-05  foo     4\n",
      "2019-01-06  foo     5\n",
      "2019-01-07  foo     6\n",
      "2019-01-08  foo     7\n",
      "2019-01-09  foo     8\n",
      "2019-01-10  foo     9\n",
      "2019-01-01  bar     10\n",
      "2019-01-02  bar     11\n",
      "2019-01-03  bar     12\n",
      "2019-01-04  bar     13\n",
      "2019-01-05  bar     14\n",
      "2019-01-06  bar     15\n",
      "2019-01-07  bar     16\n",
      "2019-01-08  bar     17\n",
      "2019-01-09  bar     18\n",
      "2019-01-10  bar     19\n",
      "\n",
      "Expected:\n",
      "              a     b\n",
      "2019-01-01  foo     0\n",
      "2019-01-02  foo     1\n",
      "2019-01-03  foo     2\n",
      "2019-01-04  foo     3\n",
      "2019-01-05  foo     NaN\n",
      "2019-01-06  foo     5\n",
      "2019-01-07  foo     6\n",
      "2019-01-08  foo     7\n",
      "2019-01-09  foo     NaN\n",
      "2019-01-10  foo     9\n",
      "2019-01-01  bar     10\n",
      "2019-01-02  bar     11\n",
      "2019-01-03  bar     12\n",
      "2019-01-04  bar     13\n",
      "2019-01-05  bar     14\n",
      "2019-01-06  bar     15\n",
      "2019-01-07  bar     16\n",
      "2019-01-08  bar     17\n",
      "2019-01-09  bar     18\n",
      "2019-01-10  bar     19\n",
      "\n",
      "I have tried alternative approaches like:\n",
      "df.loc[df['a'] == 'foo' and df.index.isin(['2019-01-05','2019-01-09']), 'b']\n",
      "\n",
      "which drops:\n",
      "ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "\n",
      "Not even this works, as the isin returns an array without the date based indexing:\n",
      "df['a'] == 'foo' and pd.Series(df.index.isin(['2019-01-05','2019-01-09']))\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can do with one .loc chain of loc assignment will be not safe\n",
      "df.loc[df.index.isin(['2019-01-05','2019-01-09'])&df.a.eq('foo'),'b']=np.nan\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Combining corresponding columns between two separate dataframes into new dataframe\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085151/combining-corresponding-columns-between-two-separate-dataframes-into-new-datafra\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have two dataframes looking like below: \n",
      "df1\n",
      "\n",
      "Column 1   Column 2  Column 3 \n",
      "0.2         0.4       0.5 \n",
      "0.25        0.44      0.45 \n",
      "0.26        0.32      0.33\n",
      "\n",
      "df2\n",
      "\n",
      "Column 1   Column 2  Column 3 \n",
      "340         350       360\n",
      "410         400       350\n",
      "234         324       450\n",
      "\n",
      "How can I combine df1 and df2 to make a dataframe df3 that has Columns of the same # side-by-side, i.e. \n",
      "df3 \n",
      "Column 1  Column 1  Column 2  Column 2  Column 3  Column 3\n",
      "0.2         340       0.4       350      0.5       360\n",
      "0.25        410       0.44      400      0.45      350 \n",
      "0.26        234       0.32      324      0.33      450\n",
      "\n",
      "Thank you! \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You could try a more fancy way of ordering here: Pandas concatenate alternating columns\n",
      "But it would be much easier to read the code if the dataframes were combined explicitly in the desired way.\n",
      "First declare new column names:\n",
      "dataCols = ['c1', 'c2', 'c3', 'c4', 'c5', 'c6']\n",
      "\n",
      "Then alternate the Series:\n",
      "dataSeries = [df1.Column1, df2.Column1, df1.Column2, df2.Column2, df1.Column3, df2.Column3]\n",
      "\n",
      "(Use df1['Column 1'] if there are spaces in your current column names)\n",
      "Then combine into a dictionary and create a dataframe:\n",
      "dataDict = dict(list(zip(dataCols, dataSeries)\n",
      "newDf = pd.DataFrame(dataDict)\n",
      "\n",
      "This will create a dataframe with alternating columns.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Problem to convert object to str in Python\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085127/problem-to-convert-object-to-str-in-python\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I need convert object to string \n",
      "import pandas as pd\n",
      "import locale\n",
      "import numpy as np\n",
      "\n",
      "df_csv = pd.read_csv('df_empenho.csv', sep=';', encoding='utf-8')\n",
      "\n",
      "df_csv['valor2'] = df_csv['Empenhado'].astype(str)\n",
      "\n",
      "df_csv['valor2'].astype(str).apply(lambda x : x.replace('.',''))\n",
      "\n",
      "df_csv['valor2'].head()\n",
      "\n",
      "Output\n",
      "0    13.188,15\n",
      "1     8.492,40\n",
      "2     3.570,00\n",
      "3     1.486,20\n",
      "4     2.660,67\n",
      "\n",
      "Name: valor2, dtype: object\n",
      "\n",
      "I expect dtype = string\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Python pandas - Dropping grouped rows based on missing territory code\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085063/python-pandas-dropping-grouped-rows-based-on-missing-territory-code\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "In the df below, we have two \"Mixes\" as indicated by the Mix_Name and Mix_ID columns. And within each of these mixes are multiple tracks with unique Track_ID's that contain different territories (see Territories column).\n",
      "\n",
      "What I'm hoping to do here find out which tracks do not have the US territory, and if any of the tracks do not have the US territory, I will want to drop the entire mix from my dataframe. With the resulting dataframe looking like this because \"Coachella Mix Vol 2\" is missing the US territory in one of its tracks:\n",
      "\n",
      "I know that I need to Groupby: 'Mix_ID', 'Track', and 'Artist' but I'm unsure of how to search the territories column to see if it contains the \"US\" territory. Any help would be much appreciated!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "df.groupby(['Mix_Name', 'Track', 'Artist']).filter(lambda x: (x['Territories'].str.contains('US').any()))\n",
      "\n",
      "You do your groupby, then filter the groups to check if the territories column contains 'US'.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Remove list type in columns while preserving list structure\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57085000/remove-list-type-in-columns-while-preserving-list-structure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have two columns that from the way my data was pulled are in lists. This may be a really easy question, I just haven't found the exactly correct way to create the result I'm looking for.\n",
      "I need the \"c_u\" column to be a string without the [] and the \"tawgs.db_id\" column to be integers separated by a column if that's possible. \n",
      "I've tried this code:\n",
      "df['c_u'] = df['c_u'].astype(str)\n",
      "\n",
      "to convert c_u to a string: but it failed and outputs:\n",
      "\n",
      "What I need the output to look like is:\n",
      "c_u                              tawgs.db_id\n",
      "hbhprecision.com         10813,449,6426,6427\n",
      "thomsonreuters.com            12519,510,6426\n",
      "\n",
      "etc.\n",
      "Please help and thank you very much in advance!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "for the first part, removing the brackets [ ] \n",
      "df['c_u'].apply(lambda x : x.strip(\"['\").strip(\"']\"))\n",
      "\n",
      "for the second part (assuming you removed your brackets as well), splitting the values across columns:\n",
      "df['tawgs.db_id'].str.split(',',  expand=True)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Secondary-y offsetted by 2 x-ticks\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084636/secondary-y-offsetted-by-2-x-ticks\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "My goal is to plot two different plots on the same figure.\n",
      "My code :\n",
      "df = pd.read_csv(\"https://raw.githubusercontent.com/theaupoulat/vc_fundraising/master/levee2019.csv\")\n",
      "\n",
      "# format amount raised\n",
      "df[\"amount_raised\"] = (df[\"amount_raised\"]\n",
      "                   .str.replace(\"M\", \"000000\")\n",
      "                   .str.replace(\" \", \"\")\n",
      "                   .str.replace(\".\", \"\")\n",
      "                   .str.replace(\"€\", \"\")\n",
      "                   .str.replace(\"NC\", \"0\")\n",
      "                   .str.replace('\\D', '')\n",
      "                   .astype(int)\n",
      "                   ) / 1000000\n",
      "\n",
      "# groupby object to extract aggregate values\n",
      "per_week = df.groupby(\"week_number\")\n",
      "\n",
      "# dataframe creation for plotting purposes\n",
      "d = {'weekly_amount': per_week.amount_raised.sum(), \"total_fundraisings\": per_week.size()}\n",
      "df_week_recap = pd.DataFrame(data=d)\n",
      "\n",
      "#plotting\n",
      "plt.figure()\n",
      "\n",
      "ax1 = df_week_recap['weekly_amount'].plot(secondary_y = True, color = 'violet')\n",
      "ax2 = df_week_recap[\"total_fundraisings\"].plot(kind='bar', color = 'blue')\n",
      "\n",
      "ax2.grid(True)\n",
      "ax1.grid(False)\n",
      "ax1.set_ylim([0,500])\n",
      "\n",
      "plt.show()\n",
      "\n",
      "generated the plot below:\n",
      "\n",
      "This is strange as the values displayed by the pink line are from the [same row + 2] than the bar plot (checked the data).\n",
      "I can't really understand where matplotlib decided to offset the DataFrame values.\n",
      "Any clues ?\n",
      "Thanks !\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Creating multiple dataframes (2) based off value in one column and stop when another value is observed\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084530/creating-multiple-dataframes-2-based-off-value-in-one-column-and-stop-when-ano\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "A dataframe structure looks like so:\n",
      "Date   Amount1     Amount2     Amount3\n",
      "\n",
      "NaN     Port        NaN        thing1\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3      \n",
      "NaN     Port        NaN        thing2\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3  \n",
      "Total     Nan        NaN        NaN\n",
      "1/1/17     2         3           3\n",
      "2/1/17     2         3           3  \n",
      "2/1/17     2         3           3  \n",
      "4/1/17     2         3           3  \n",
      "5/1/17     2         3           3  \n",
      "6/1/17     2         3           3  \n",
      "7/1/17     2         3           3  \n",
      "8/1/17     2         3           3 \n",
      "9/1/17     2         3           3\n",
      "10/1/17    2         3           3\n",
      "11/1/17    2         3           3 \n",
      "12/1/17    2         3           3  \n",
      "NaN        Nan       Nan         Nan\n",
      "\n",
      "I am interested in creating 2 dataframes.\n",
      "1.One dataframe collects the rows after thing2 is observed in the amount3 column and stops the row before total is observed in the date column.\n",
      "2.The 2nd dataframe will start on the row after total is observed and stop when a NaN (null) value is observed. \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can do with idxmax and .iloc \n",
      "df1=df.loc[df.Amount3.eq('thing2').idxmax():df.Date.eq('Total').idxmax()-1].copy()\n",
      "df2=df.loc[df.Date.eq('Total').idxmax():]\n",
      "df2=df2.loc[:df2.Date.isnull().idxmax()-1]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Return rows with max/min values at bottom of dataframe (python/pandas)\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084473/return-rows-with-max-min-values-at-bottom-of-dataframe-python-pandas\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to write a function that can look at a dataframe, find the max or min value in a specified column, then return the entire datafrane with the row(s) containing the max or min value at the bottom.\n",
      "I have made it so that the rows with the max or min value alone get returned.\n",
      "def findAggregate(df, transType, columnName=None):\n",
      "\n",
      "    if transType == 'max1Column':\n",
      "        return df[df[columnName] == df[columnName].max()]\n",
      "\n",
      "    elif transType == 'min1Column':\n",
      "        return df[df[columnName] == df[columnName].min()]\n",
      "\n",
      "Given the dataframe below, I want to check col2 for the MIN value\n",
      "Original Dataframe:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "orange   18       cat\n",
      "black    6        fish\n",
      "\n",
      "Expected output:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "orange   18       cat\n",
      "black    6        fish\n",
      "blue     2        dog\n",
      "\n",
      "Actual output:\n",
      "col1     col2     col3\n",
      "blue     2        dog\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Use idxmin or idxmax:\n",
      "df.append(df.iloc[df['col2'].idxmin()], ignore_index=True)\n",
      "\n",
      "     col1  col2  col3\n",
      "0    blue     2   dog\n",
      "1  orange    18   cat\n",
      "2   black     6  fish\n",
      "3    blue     2   dog\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to generate a new dataframe in python by considering conditions from other dataframes?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084371/how-to-generate-a-new-dataframe-in-python-by-considering-conditions-from-other-d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am performing data manipulation in python using pandas on a very large dataset, say 100 million rows. I have two dataframes and wish to generate third dataframe as per the conditions mentioned, the scenario is explained below:\n",
      "dataframe 1:\n",
      "Col_B and Col_D are of int64 type\n",
      "Col_A   Col_B   Col_C   Col_D\n",
      " A       11      B       20\n",
      " A       11      C       24\n",
      " B       14      R       19\n",
      "...      ...    ...      ...\n",
      "\n",
      "dataframe 2:\n",
      "Col_Z is of type float64 and remaining columns are of int64\n",
      "Col_X   Col_Y   Col_P   Col_Q   Col_Z\n",
      " 10      15      16      21      0.99\n",
      " 10      15      17      22      0.89\n",
      " 11      15      16      20      0.67\n",
      "...     ...     ...     ...      ...\n",
      "\n",
      "Condition to be applied:\n",
      "Consider only first row of both the dataframe, for the sake of understanding conditions:\n",
      "\n",
      "if the value of (Col_B is between the value of Col_X and Col_Y) and value of (Col_D is between the value of Col_P and Col_Q) then return the corresponding value of Col_A, Col_C and Col_Z, otherwise return NaN\n",
      "\n",
      "Expected Output (Dataframe 3):\n",
      "Col_A   Col_C   Col_Z\n",
      " A       B       0.99\n",
      "NaN     NaN      NaN\n",
      " B       R       0.67\n",
      "\n",
      "Note: This output is generated merely considering if there are only these three rows in dataframes but in actual each value of Dataframe 1 has to scan all of the values in Dataframe 2 until desired conditions is achieved. \n",
      "My Code:\n",
      "df3 = {}\n",
      "Col_A = []\n",
      "Col_C = []\n",
      "Col_Z = []\n",
      "for i in df1.iterrows():    \n",
      "    value = float(df2[(i[1][1] > df2['Col_X'].values) &\n",
      "      (i[1][1] < df2['Col_Y'].values) &\n",
      "      (i[1][3] > df2['Col_P'].values) &\n",
      "      (i[1][3] < df2['Col_Q'].values)]['Col_Z'])\n",
      "\n",
      "    if bool(value):\n",
      "        Col_Z.append(value)\n",
      "        Col_A.append(i[1][0])\n",
      "        Col_C.append(i[1][2])\n",
      "    else:\n",
      "        Col_Z.append(float('NaN'))\n",
      "        Col_A.append(float('NaN'))\n",
      "        Col_C.append(float('NaN'))\n",
      "\n",
      "This code is working fine uptill the condition is met, as soon as condition does'nt met, it throws a TypeError. Please can any rectify this.\n",
      "Also, I wanted to know if there is any alternate and efficient way to perform it. Please let me know.  \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can do this by considering the data set as a whole.\n",
      "\n",
      "Firstly, for more convenience, I suggest you to join your two dataset as one dataset. You can do it with the merge function or just concat. Here, I use concat since another solution uses merge. To be clear with what there performing, you can have a look at this.\n",
      "Then, you can define you condition on the whole columns. Take care of the and operator that becomes &.\n",
      "Finally, you can call the where function that returns Nan when the condition isn't satisfied.\n",
      "To fit the desired output, you can filter the columns using iloc or just calling the columns name.\n",
      "\n",
      "Here the code:\n",
      "# Import module\n",
      "import pandas as pd\n",
      "\n",
      "df1 = pd.DataFrame([[\"A\", 11,  \"B\", 20],\n",
      "                    [\"A\", 11,  \"C\", 24],\n",
      "                    [\"B\", 14,  \"R\", 19]],\n",
      "                   columns=[\"Col_A\", \"Col_B\", \"Col_C\", \"Col_D\"])\n",
      "df2 = pd.DataFrame([[10, 15,  16, 21, 0.99],\n",
      "                    [10, 15,  17, 22, 0.89],\n",
      "                    [11, 15,  16, 20, 0.67]],\n",
      "                   columns=[\"Col_X\", \"Col_Y\", \"Col_P\", \"Col_Q\", \"Col_Z\"])\n",
      "\n",
      "# Concat the dataframe\n",
      "df = pd.concat([df1, df2], axis=1)\n",
      "print(df)\n",
      "\n",
      "# Define the conditions\n",
      "condition_col_b = ((df.Col_X <= df.Col_B) & (df.Col_B < df.Col_Y))\n",
      "condition_col_d = ((df.Col_P <= df.Col_D) & (df.Col_D < df.Col_Q))\n",
      "\n",
      "print(condition_col_b & condition_col_d)\n",
      "# 0     True\n",
      "# 1    False\n",
      "# 2     True\n",
      "\n",
      "# Apply the condition\n",
      "output = df.where(condition_col_b & condition_col_d)\n",
      "print(output)\n",
      "#   Col_A  Col_B Col_C  Col_D  Col_X  Col_Y  Col_P  Col_Q  Col_Z\n",
      "# 0     A   11.0     B   20.0   10.0   15.0   16.0   21.0   0.99\n",
      "# 1   NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN    NaN\n",
      "# 2     B   14.0     R   19.0   11.0   15.0   16.0   20.0   0.67\n",
      "\n",
      "# Filter output\n",
      "print(output[['Col_A', 'Col_C', 'Col_Z']])\n",
      "#   Col_A Col_C  Col_Z\n",
      "# 0     A     B   0.99\n",
      "# 1   NaN   NaN    NaN\n",
      "# 2     B     R   0.67\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Seperating a dataframe into subsets based on a sorted columns values\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084296/seperating-a-dataframe-into-subsets-based-on-a-sorted-columns-values\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a dataframe and the columns are as follows\n",
      "['time_of_incident', 'vendor_tech', 'incident_closed']\n",
      "I have the data frame sorted by the vendor_tech alphabetically. I want to split this large dataframe. Size of about 18,000 entries into multiple dataframes based on the value in vendor_tech \n",
      "I have tried many solutions and can't seem to find anything like this on StackOverflow. I've tried many inefficient and convoluted solutions with no luck. \n",
      "My main problem is when I use .itertuple() and iterate over the objects I am not able to then add those objects to another DataFrame\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "# try this\n",
      "\n",
      "df\n",
      "     time_of_incident       vendor_tech Incident_closed\n",
      "0   1970-04-05 17:23:44.460 a           yes\n",
      "1   1994-11-25 17:23:44.460 a           no\n",
      "2   1980-02-12 17:23:44.460 a           no\n",
      "3   1978-06-22 17:23:44.460 b           yes\n",
      "4   1990-10-17 17:23:44.460 b           yes\n",
      "5   1960-05-27 17:23:44.460 b           yes\n",
      "6   1980-02-12 17:23:44.460 c           no\n",
      "\n",
      "Group your data by 'vendor tech'\n",
      "mini_df = [(name,group) for name,group in df.groupby('vendor_tech')]\n",
      "\n",
      "create a dictionary of dataframes\n",
      "mini_list = list(df['vendor_tech'].unique())\n",
      "mini_dict ={}\n",
      "for i in range(len(mini_df)):\n",
      "    label = mini_list.pop(0)\n",
      "    mini_dict['df_'+str(label)] = pd.DataFrame(mini_df[i][1], columns=df.columns)\n",
      "\n",
      "call each dataframe (which as unique based on 'vendor tech')\n",
      "df_a\n",
      "time_of_incident            vendor_tech Incident_closed\n",
      "0   1970-04-05 17:23:44.460 a           yes\n",
      "1   1994-11-25 17:23:44.460 a           no\n",
      "2   1980-02-12 17:23:44.460 a           no\n",
      "\n",
      "df_b\n",
      "    time_of_incident        vendor_tech Incident_closed\n",
      "3   1978-06-22 17:23:44.460 b           yes\n",
      "4   1990-10-17 17:23:44.460 b           yes\n",
      "5   1960-05-27 17:23:44.460 b           yes\n",
      "\n",
      "you can than save each df to a different file if you want\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Problem with importing pdf file using tabula in python and working on it\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084217/problem-with-importing-pdf-file-using-tabula-in-python-and-working-on-it\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "My pdf file is like this. I want to make table out of it with columns as Name,Father's Name , House Number,Serial NoI used tabula to get the pdf and try to work on the dataframe. But the data is all empty.\n",
      "df=tabula.read_pdf(\"1.pdf\",p\n",
      "df.head()\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Bokeh doesn't show labels?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084145/bokeh-doesnt-show-labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am trying to put labels in some circles. I have used the following code which worked fine for a similar example but not working here.\n",
      "from bokeh.plotting import figure, show, save\n",
      "from bokeh.tile_providers import CARTODBPOSITRON_RETINA\n",
      "from bokeh.io import curdoc, output_notebook, output_file, export_png\n",
      "from pyproj import Proj, transform\n",
      "from bokeh.transform import linear_cmap, log_cmap\n",
      "import pandas as pd\n",
      "from bokeh.models import (\n",
      "  GeoJSONDataSource, ColumnDataSource, Circle,CircleX,Hex, Square, Patches, LabelSet, HoverTool,Legend, LegendItem, Plot, LinearAxis, Grid, LogColorMapper, LinearColorMapper, ColorBar, BasicTicker,\n",
      "  TapTool,Range1d, PanTool, WheelZoomTool, BoxSelectTool,OpenURL, ZoomInTool, ZoomOutTool, Arrow, OpenHead, NormalHead, VeeHead, LabelSet, Label\n",
      ")\n",
      "from bokeh.palettes import *\n",
      "from haversine import haversine   \n",
      "\n",
      "def latlonrange(lat1,lon1,lat2,lon2):\n",
      "    p1 = Proj(init='epsg:4326')\n",
      "    p2 = Proj(init='epsg:3857')    \n",
      "    x1, y1 = transform(p1,p2,lon1,lat1)\n",
      "    x2, y2 = transform(p1,p2,lon2,lat2)\n",
      "    return {\"x_range\":(x1, x2), \"y_range\":(y1,y2)}   \n",
      "\n",
      "df = pd.DataFrame({'daily_cartons': [478,378,763,859], \n",
      "                   'daily_cartonsPercent': [19,15,31,35],\n",
      "                   'clong': [42.259387,42.235110,42.136309,41.874587],\n",
      "                   'clat': [-71.689145,-71.182045,-71.045608,-71.405451],          \n",
      "                   'cx_merc': [-7.980399e+06,-7.923949e+06,-7.908761e+06,-7.948818e+06],\n",
      "                   'cy_merc': [5.199914e+06,5.196263e+06,5.181420e+06,5.142212e+06]\n",
      "                  })\n",
      "\n",
      "curdoc().clear()\n",
      "output_notebook()\n",
      "\n",
      "latlonbox = latlonrange(43, -74, 41, -69)\n",
      "\n",
      "p = figure(title=\"XXXX\",\n",
      "           x_range=latlonbox[\"x_range\"], y_range=latlonbox[\"y_range\"],\n",
      "           x_axis_type=\"mercator\", y_axis_type=\"mercator\",\n",
      "           plot_width=1200, plot_height=750,\n",
      "           tools=\"pan,wheel_zoom,box_zoom,reset,zoom_in,zoom_out,save\")\n",
      "p.add_tile(CARTODBPOSITRON_RETINA)\n",
      "\n",
      "\n",
      "ksource = ColumnDataSource(dict(x=df['cx_merc'], y=df['cy_merc'], daily_cartons = df['daily_cartons'],\n",
      "                                daily_cartons_perc = df['daily_cartonsPercent'].astype(str) + '%', lcolor=viridis(4)))\n",
      "\n",
      "kcircle = Circle(x=\"x\", y=\"y\", size=30, line_color=\"#de2d26\", fill_color=\"white\", line_width=2, fill_alpha=1)\n",
      "kc1 = p.add_glyph(ksource, kcircle)\n",
      "\n",
      "klabels = LabelSet(x='x', y='y', text='daily_cartons_perc', level='glyph', source=ksource, render_mode='canvas',\n",
      "                   x_offset=-7, text_font_size = '8pt',text_font_style='bold', text_color = 'black', text_baseline ='middle' )\n",
      "p.add_layout(klabels)\n",
      "\n",
      "kc1_hover = HoverTool(renderers=[kc1], tooltips=[('Daily_cartons: ', '@daily_cartons{00,000}')])\n",
      "p.add_tools(kc1_hover)\n",
      "# #----------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "show(p)\n",
      "\n",
      "The above code is showing the circles but not the labels. Can't figure out why?\n",
      "\n",
      "Python:3.7.3, Bokeh:1.2.0\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 how to count the element with the same index?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084120/how-to-count-the-element-with-the-same-index\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have to get the numbers of counties of every state from a CSV file and then return which state has the most counties. But I don't know how to count the element that is under the de same index\n",
      "csv file:\n",
      "\n",
      "I tried to use .count() and numerate() but returns me an error key error: \"Alabama\" \n",
      "this is the code I used to create the actual data frame from the CSV file\n",
      "import pandas as PD\n",
      "\n",
      "census = pd.read_csv('census.csv')\n",
      "counties = pd.DataFrame({'State':census['STNAME'],'County':census['CTYNAME'],\n",
      "         'populations':census['CENSUS2010POP'],'population 2010': census['POPESTIMATE2010'], \n",
      "         'population 2015': census['POPESTIMATE2015']})\n",
      "counties = counties.set_index(['State', 'County'])\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "try:\n",
      "counties.groupby(['State','County']).agg('count')\n",
      "\n",
      "instead of:\n",
      "counties = counties.set_index(['State', 'County']).count()\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Divide date+hour column into two colums (format date, number) with Pandas - Python\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084055/divide-datehour-column-into-two-colums-format-date-number-with-pandas-pyth\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to separate these column into two, since Excel extract function just takes way too much I figured I could do it in Python with Jupyter Notebook, using Pandas. But I haven't really do this before, normally I would have data already treated so I am having difficulties.\n",
      "I tried using excel but it takes forever.\n",
      "+---------------------+----------------------+-----------------+\n",
      "|        date + hour  |        date only     |    hour only    |\n",
      "+---------------------+----------------------+-----------------+\n",
      "| 01/01/19 01         | 01/01/2019 (formated)|   1 (number)    |\n",
      "+---------------------+----------------------+-----------------+\n",
      "\n",
      "\n",
      "+---------------------+----------------------+-----------------+\n",
      "|        date + hour  |        date only     |    hour only    |\n",
      "+---------------------+----------------------+-----------------+\n",
      "| 01/01/19 01         | 01/01/2019 (formated)|   1 (number)    |\n",
      "+---------------------+----------------------+-----------------+\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "I think the discussion here will help a lot:\n",
      "How to split a column into two columns?\n",
      "Basically, you want to use pandas.Series.str.split (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html) to split the string in your Date & Hour column into a list and then assign the two halves of that list into new columns.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Grouping By A Column For A Total Count in Python\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57084039/grouping-by-a-column-for-a-total-count-in-python\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am working on a problem for a Intro to Data Science course on Coursera, and I am struggling with adding data to a column in a dataframe.\n",
      "This is the data set I'm working with:\n",
      "    SUMLEV  REGION  DIVISION    STATE   COUNTY  STNAME  CTYNAME     \n",
      "1   50      3       6           1       1       Alabama Autauga County  \n",
      "2   50      3       6           1       3       Alabama Baldwin County  \n",
      "3   50      3       6           1       5       Alabama Barbour County  \n",
      "4   50      3       6           1       7       Alabama Bibb County \n",
      "\n",
      "What I am trying to do is to insert a column called TotalCounties that has the total count of counties by state as a last column. I've done similar things in SQL, but it doesn't seem to work quite the same in Python.\n",
      "I have tried the code below, but the column ends up displaying as NaN instead of a number like I want it to.\n",
      "   counties_only_df = census_df[census_df['SUMLEV'] == 50]\n",
      "   x = counties_only_df.groupby('STNAME').count()['SUMLEV']\n",
      "   counties_only_df['Total Counties'] = x\n",
      "\n",
      "I would like a number to display in the newly created column instead of NaN.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "try:\n",
      "df['Total Counties'] = df.groupby('STNAME')['STNAME'].transform('count')\n",
      "\n",
      "change \"df\" for your DataFrame name\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Extrapolating between two dataframes (rounding issues)\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083982/extrapolating-between-two-dataframes-rounding-issues\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am having issues trying to extrapolate between two dataframes\n",
      " df1 = pd.DataFrame([(50,100),(150,250),(250,300)], columns=['a','b'])\n",
      " df2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b']) \n",
      "\n",
      "I tried\n",
      "cagr_7 = (df2/df1)**(1/5) - 1\n",
      "f = lambda c: c + c*cagr_7\n",
      "\n",
      "But often times the inbetween years don't make sense due to having report every year.  Is there another extrapolation option, maybe taking the difference and applying it across the years?\n",
      "I am trying to extrapolate between 5 different years.  Example 2016 to 2021.  Each dataframe is a year.\n",
      "I need help please.   \n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Using pandas to compute shares of columns that have particular values\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083914/using-pandas-to-compute-shares-of-columns-that-have-particular-values\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a DataFrame with two columns: keys and values. I want to construct a new column as follows. For each key, compute the frequency of each value, out of the total values for this key.\n",
      "I have code that achieves it, but I suspect that there must be a much simpler way to do this in pandas. Here is an example:\n",
      "def fun(sd):\n",
      "    uniqueValuesList = list(sd.drop_duplicates().dropna())\n",
      "    if len(uniqueValuesList)==0:\n",
      "        return pd.Series([0]*sd.shape[0], index=sd.index)\n",
      "    elif len(uniqueValuesList)==1:\n",
      "        return pd.Series([1]*sd.shape[0], index=sd.index)\n",
      "    else:\n",
      "        valuesList = list(sd)\n",
      "        valuesArr = np.array(valuesList)        \n",
      "        stackedValuesDf = pd.DataFrame([valuesArr]*len(valuesArr))\n",
      "        boolDf = stackedValuesDf==valuesList\n",
      "        frac = boolDf.sum() / boolDf.shape[0]\n",
      "        return frac\n",
      "\n",
      "keys =   ['1', '1', '1', '2', '3']\n",
      "values = ['a', 'b', 'b', 'c', np.nan]\n",
      "df = pd.DataFrame([keys, values]).T\n",
      "df.columns = ['keys', 'values']\n",
      "print(df.groupby('keys').values.apply(fun))\n",
      "\n",
      "This gives the desired output:\n",
      "0    0.333333\n",
      "1    0.666667\n",
      "2    0.666667\n",
      "3    1.000000\n",
      "4    0.000000\n",
      "\n",
      "That is, for the key '1', 'a' appears once  and 'b' appears twice, so they get 0.33 and 0.67, respectively. For '2' there is a singleton key so it get 1. For '3' there are no keys, so it gets 0.\n",
      "What is simpler pandas way to acheive this?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can't transform pd.Series.value_counts, so instead you can use two transforms with size:\n",
      "m = df['values'].notnull()\n",
      "\n",
      "df.loc[m, 'per'] = (df.loc[m].groupby(['keys', 'values'])['values'].transform('size')\n",
      "                     / df.groupby('keys')['values'].transform('size'))\n",
      "\n",
      "df['per'] = df['per'].fillna(0)\n",
      "\n",
      "#  keys values       per\n",
      "#0    1      a  0.333333\n",
      "#1    1      b  0.666667\n",
      "#2    1      b  0.666667\n",
      "#3    2      c  1.000000\n",
      "#4    3    NaN  0.000000\n",
      "\n",
      "\n",
      "Alteratively, with a merge:\n",
      "df1 = (df.groupby('keys')['values']\n",
      "         .apply(pd.Series.value_counts, normalize=True)\n",
      "         .to_frame('per'))\n",
      "df1.index.names=['keys', 'values']\n",
      "\n",
      "df = df.merge(df1.reset_index(), how='left')\n",
      "df['per'] = df['per'].fillna(0)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Fetch data from a DataFrame and insert into multiple rows in SQL table by generating queries\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083743/fetch-data-from-a-dataframe-and-insert-into-multiple-rows-in-sql-table-by-genera\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "\n",
      "Have a dataframe with edited data from table1.\n",
      "Use that dataframe to generate SQL queries (using python) that updates specific column and multiple rows in table1 in the SQL database. \n",
      "These queries should not have any dependency i,e must be able to run directly on the SQL database.\n",
      "\n",
      "I CANNOT have the following.\n",
      "1. When I generate my queries my python script cannot have connection strings to the SQL database.\n",
      "How can I achieve that if possible. I cannot think of a way.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to display a DataFrame without Jupyter Notebook crashing?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083727/how-to-display-a-dataframe-without-jupyter-notebook-crashing\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Everytime I try to display a complete DataFrame in my jupyter notebook, the note book crashes. The file wont start up so I had to make a new jupyter notebook file. When i do display(df) it only shows a couple of the rows, when I need to show 57623 rows. I need to show results for all of these rows and put them into an html file.\n",
      "I tried setting the max rows and max columns, but the entire dataframe would not print out without the notebook crashing\n",
      "''' python\n",
      "pd.set_option('display.max_columns', 24)\n",
      "    \"pd.set_option('display.max_rows', 57623)\n",
      "'''\n",
      "The expected results were for the entire DataFrame to print out, but instead the notebook would have an hourglass next to it and nothing would load.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to lookup two different ranges based on row's value Pandas dataframe\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083724/how-to-lookup-two-different-ranges-based-on-rows-value-pandas-dataframe\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm trying to do a conditional vlookup but with pandas. Here's the data i'm using\n",
      "n_age_scores\n",
      "type n     aging_n     mini_n   percent_n\n",
      "new        <30 days       0       0.5543\n",
      "new        31-50 days     31      0.6446\n",
      "new        51-100 days    51      0.3134\n",
      "\n",
      "e_age_scores\n",
      "type e           aging_e      mini_e   percent_e\n",
      "expansion        <30 days       0       0.33543\n",
      "expansion        31-50 days     31      0.4446\n",
      "expansion        51-100 days    51      0.6134\n",
      "\n",
      "Dataframe\n",
      "type        age    score\n",
      "new          33\n",
      "new          12\n",
      "expansion    3\n",
      "new          4\n",
      "expansion    100\n",
      "\n",
      "What I want to do is populate score with the percent column of either dataframe based on if the row type is new or expansion, an approximate match of value percent. \n",
      "How do I do this with Pandas?\n",
      "n_age_scores = aging_score_mapping.iloc[:,0:4] \n",
      "e_age_scores = aging_score_mapping.iloc[:,-4:9]\n",
      "\n",
      "\n",
      "    if df['deal_type'] == 'Expansion':\n",
      "       df = merge.e_age_scores(df, on='age_score')\n",
      "       if df['deal_type'] == 'new':\n",
      "          df = merge.n_age_scores(df, on='age_score')\n",
      "\n",
      "I'm not sure how to do this but I think i need to loop and merge with a approximate match and populate age_score with percent_n depending on the type.\n",
      "Is this possible with pandas?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Assign group number based on row criteria [Python] [duplicate]\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083683/assign-group-number-based-on-row-criteria-python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "\n",
      "This question already has an answer here:\n",
      "\n",
      "\n",
      "Pandas: counter increasing each time conditions are met\n",
      "\n",
      "                    2 answers\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "I have a dataframe with column 'type' and want to create a column 'group' that assigns a group number when type = 'a'. \n",
      "Starting data (ignore index, copied from jupyter notebook): \n",
      "df = pd.DataFrame ({'type':\n",
      "['a','b','a','b','b','a','b','b','b','b','b','b']})\n",
      "\n",
      "Index   type\n",
      "0   a\n",
      "1   b\n",
      "2   a\n",
      "3   b\n",
      "4   b\n",
      "5   a\n",
      "6   b\n",
      "7   b\n",
      "8   b\n",
      "9   b\n",
      "10  b\n",
      "11  b\n",
      "\n",
      "output should be:\n",
      "\n",
      "type    grouping\n",
      "a   1\n",
      "b   1\n",
      "a   2\n",
      "b   2\n",
      "b   2\n",
      "a   3\n",
      "b   3\n",
      "b   3\n",
      "b   3\n",
      "b   3\n",
      "b   3\n",
      "b   3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "It is not clear how you define the grouping but your desired output can be achieved by using cumsum and eq:\n",
      "df['grouping']=df['type'].eq('a').cumsum()\n",
      "\n",
      "Output:\n",
      "   type  grouping\n",
      "0     a         1\n",
      "1     b         1\n",
      "2     a         2\n",
      "3     b         2\n",
      "4     b         2\n",
      "5     a         3\n",
      "6     b         3\n",
      "7     b         3\n",
      "8     b         3\n",
      "9     b         3\n",
      "10    b         3\n",
      "11    b         3\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Averaging data while merging\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083551/averaging-data-while-merging\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have two dataframes as shown below:\n",
      "(i) result1\n",
      "     time browncarbon blackcarbon\n",
      " 180.0008    0.105270         NaN\n",
      " 181.3809    0.166545    0.001217\n",
      " 181.6197    0.071581         NaN\n",
      "\n",
      " 422 rows x 3 columns\n",
      "\n",
      "(ii) result2\n",
      "   start         end      toc \n",
      "179.9989    180.0002    155.0\n",
      "180.0002    180.0016    152.0\n",
      "180.0016    180.0030    151.0\n",
      "\n",
      "1364 rows x 3 columns\n",
      "\n",
      "The multiple start and end rows that get encapsulated into one of the time rows should also correspond to one toc row, as it does right now, however, it should be the average of the multiple toc rows, which is not the case presently. How do I do that? There is a related answer on Stack Overflow. The link is: Merging two pandas dataframes with complex conditions\n",
      "(iii) result3\n",
      "result1['rank'] = np.arange(length1)\n",
      "result3=pd.merge_asof(result1.sort_values('time'),result2,left_on='time',right_on='start')\n",
      "result3.sort_values('rank').drop(['rank','start','end'], axis=1)\n",
      "\n",
      "    time    browncarbon    blackcarbon    toc\n",
      "180.0008        0.10527            NaN  152.0\n",
      "\n",
      "422 rows X 4 columns\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 When adding a DataFrame column default value, how do I limit it to specific rows?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083474/when-adding-a-dataframe-column-default-value-how-do-i-limit-it-to-specific-rows\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am using a combination of beautifulsoup and pandas to try and get sports reference data by looping through boxscore pages, obtaining the dataframes for each team and concatenating them all together. I noticed that the way the table is formatted on each page, there are row dividers separating the starters from the reserves, and this row divider has the value \"Reserves\" in the 'Starter' column (which I later rename to 'Player_Name'), with the remaining column headers repeated for the rest of its values. When this data is input into the dataframe, the row dividers are brought in as a normal row. I would like to add a separate column that holds a Y/N value for whether or not that player started the game and remove all records where the 'Starters' column is equal to \"Reserves\".\n",
      "I have tried adding a column but I'm struggling with a method to get the default values to be \"Y\" for the first x number of rows and \"N\" for the remaining rows.\n",
      "Here is a brief example of the table followed by the code I am using. Let me know if you have any thoughts!\n",
      "EDIT: I may have oversimplified this, as there are actually two header columns and it appears this is causing an issue when trying the solutions presented. How can I remove the first header column that just states 'Basic Box Score Stats' and 'Advanced Box Score Stats'?\n",
      "Basic Box Score Stats            Advanced Box Score Stats\n",
      "Starters              MP    FG   +/-  xyz%\n",
      "Player1               20:00 17   5    12\n",
      "Player2               15:00 8    4    10\n",
      "Player3               10:00 9    3    8\n",
      "Player4               9:00  3    2    6\n",
      "Player5               8:00  1    1    4\n",
      "Reserves              MP    FG   +/-  xyz%\n",
      "Player4               7:00  1    1    2\n",
      "Player5               4:00  1    1    2\n",
      "Player6               3:30  1    1    2\n",
      "\n",
      "import pandas as pd\n",
      "from bs4 import BeautifulSoup\n",
      "#performed steps in bs4 to get the links to individual boxscores\n",
      "    for boxscore_link in boxscore_links:\n",
      "        basketball_ref_dfs=pd.read_html(MainURL + boxscore_link)\n",
      "        if len(basketball_ref_dfs) = 4:\n",
      "            away_team_stats = pd.concat([basketball_ref_dfs[0],basketball_ref_dfs[1]])\n",
      "            home_team_stats = pd.concat([basketball_ref_dfs[2],basketball_ref_dfs[3]])\n",
      "        else:\n",
      "            away_team_stats = basketball_ref_dfs[0]\n",
      "            home_team_stats = basketball_ref_dfs[1]\n",
      "#new code to be added here to fix 'reserve' row header for away/home_team_stats        \n",
      "full_game_stats = pd.concat([away_team_stats,home_team_stats])\n",
      "        full_season_stats = full_season_stats.append(full_game_stats,ignore_index=True)\n",
      "    full_season_stats\n",
      "\n",
      "#what I want:\n",
      "away_team_stats['Starter']='Y' # + some condition to only set this value for the first x occurrences or set to 'Y' until row value equals Reserve, then set remaining to 'N'\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can do this in three steps:\n",
      "\n",
      "Set the default value 'N' for the entire column using away_team_stats['Starter']='N'\n",
      "Set the value for the first x rows to be 'Y' using the iloc method with away_team_stats.iloc[:x, 2]='Y'\n",
      "(I believe the 'Starter' column will be in position 2 if appending to your example data but you may need to edit this)\n",
      "Remove the row with 'Player_Name' == 'Reserves' by using the loc method with away_team_stats = away_team_stats.loc[away_team_stats['Player_Name']!='Reserves', :]\n",
      "\n",
      "The iloc method will slice your dataframe by numerical index/column and the loc method will slice your dataframe by index/column label\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Plot bar chart of counts of where dummy variables == 1 AND a separate variable in the Series == 1\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57083452/plot-bar-chart-of-counts-of-where-dummy-variables-1-and-a-separate-variable-i\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a pandas DataFrame with dummy variables for age ranges, specifically '<35', '35-44', '45-54', '55-64', and '65+'. I have another dummy variable that represents if someone's hair has gone grey yet, 'grey?'.\n",
      "I would like to plot a bar chart of how many people have grey hair per 1,000 people by age group. So basically, for each age group dummy where dummy == 1, (count of grey dummy == 1 / people where age group dummy == 1) * 1000, and plot as bar chart.\n",
      "What is the best way to do this?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "I ended up figuring out a way to do it, but it might not be the best way to do it.\n",
      "counts_list = []\n",
      "\n",
      "for col in ['<35', '35-44', '45-54', '55-64', '65+']:\n",
      "    counts_df = df.groupby(col)['grey?'].value_counts()\n",
      "    try:\n",
      "        counts_list.append(counts_df[1][1] / (counts_df[1][1] + counts_df[1][0]) * 1000)\n",
      "    except:\n",
      "        counts_list.append(0)\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt; plt.rc(\"font\", size=14)\n",
      "y_pos = np.arange(len(['<35', '35-44', '45-54', '55-64', '65+']))\n",
      "\n",
      "plt.bar(y_pos, counts_list, align='center', alpha=0.5)\n",
      "plt.xticks(y_pos, ['<35', '35-44', '45-54', '55-64', '65+'])\n",
      "plt.ylabel('Grey/1k')\n",
      "plt.title('Grey by Age')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to make changes to time column based on current status and previous status in pandas?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082934/how-to-make-changes-to-time-column-based-on-current-status-and-previous-status-i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "My data frame has four columns: P_Id, Prev_State, Current_State, Timestamp. \n",
      "As the process goes through multiple states, the user might have the same timestamp for all the states. I want to add a timestamp value(1 s) for every data point as the process transitions from 1 state to another.\n",
      "By comparing and matching the Prev_State with the Current_State, corresponding timestamp of that data point is modified by adding 1 s. However, if the timestamp value is different from the previous data point, then I would want to keep the original timestamp.\n",
      "Note: My timestamp values are in the interval of 30 minutes.\n",
      "P_Id   Prev_State   Current_State   Timestamp\n",
      "001    None          Initial        2019-02-13 18:00:00   \n",
      "001    Ready         Loading        2019-02-13 18:00:00\n",
      "001    Initial       Ready          2019-02-13 18:00:00\n",
      "001    Loading       Executing      2019-02-13 18:30:00\n",
      "001    Executing     Evaluating     2019-02-13 18:30:00\n",
      "001    Evaluating    Terminating    2019-02-13 18:30:00\n",
      "\n",
      "Expected Output:\n",
      "P_Id   Prev_State   Current_State   Timestamp\n",
      "001    None          Initial        2019-02-13 18:00:00   \n",
      "001    Ready         Loading        2019-02-13 18:00:02\n",
      "001    Initial       Ready          2019-02-13 18:00:01\n",
      "001    Loading       Executing      2019-02-13 18:30:00\n",
      "001    Executing     Evaluating     2019-02-13 18:30:01\n",
      "001    Evaluating    Terminating    2019-02-13 18:30:02\n",
      "\n",
      "I am a newbie to python. Any help would be appreciated!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "This can be done with some fun networkx. We create a directed graph, then form the longest path. (Ideally your DataFrame is always connected by a single path like in the example). Then we order it, using an ordered Categorical so we can sort.  Finally, add 1s for each new entry and return the Series. \n",
      "import networkx as nx\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def add_second(gp):\n",
      "    # Get Path\n",
      "    G = nx.from_pandas_edgelist(gp, source='Prev_State', target='Current_State',\n",
      "                                create_using=nx.DiGraph())\n",
      "    order = nx.dag_longest_path(G)[:-1]\n",
      "\n",
      "    # Order\n",
      "    gp['Prev_State'] = pd.Categorical(gp.Prev_State, ordered=True, categories=order)\n",
      "    gp = gp.sort_values('Prev_State')\n",
      "\n",
      "    # Add 1s\n",
      "    s = gp.Timestamp + pd.to_timedelta(range(len(gp)), unit='s')\n",
      "    return s\n",
      "\n",
      "df['new_time'] = df.groupby(['P_Id', 'Timestamp'], group_keys=False).apply(add_second)\n",
      "\n",
      "Output:\n",
      "   P_Id  Prev_State Current_State           Timestamp            new_time\n",
      "0     1        None       Initial 2019-02-13 18:00:00 2019-02-13 18:00:00\n",
      "1     1       Ready       Loading 2019-02-13 18:00:00 2019-02-13 18:00:02\n",
      "2     1     Initial         Ready 2019-02-13 18:00:00 2019-02-13 18:00:01\n",
      "3     1     Loading     Executing 2019-02-13 18:30:00 2019-02-13 18:30:00\n",
      "4     1   Executing    Evaluating 2019-02-13 18:30:00 2019-02-13 18:30:01\n",
      "5     1  Evaluating   Terminating 2019-02-13 18:30:00 2019-02-13 18:30:02\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Using Percent (%) in Pandas and pyodbc SQL Server w/o Error\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082909/using-percent-in-pandas-and-pyodbc-sql-server-w-o-error\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I believe having % in my sql queries is causing issues in Python because of %s being used for variables. I have tried escaping character and have had no luck so far\n",
      "import pyodbc\n",
      "import pandas as pd\n",
      "conn = pyodbc.connect('...')\n",
      "cursor = conn.cursor()\n",
      "\n",
      "sql_statement = \"\"\"\n",
      "select ABS(CHECKSUM(NEWID()) % 2), %s\n",
      "\"\"\"\n",
      "s = sql_statement % (5)\n",
      "df = pd.read_sql_query(s, conn)\n",
      "\n",
      "ValueError: unsupported format character ')' (0x29) at index 33\n",
      "\n",
      "ABS(CHECKSUM(NEWID()) % 2) is supposed to just be a way to return a random value for each row\n",
      "This is just a simple example. Any time I try to use var like '%abc% I get the same issue as above, I believe the % characters are causing issues in the python libraries.\n",
      "Is there a way to escape these characters or to avoid this issue?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Typically just adding another '%' indicates that you're using '%' as a string and not a modulo. E.g.:\n",
      "print('5%%')\n",
      "\n",
      "Results in printing '5%'.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 TypeError: unhashable type: 'numpy.ndarray' in lag_plot() python\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082780/typeerror-unhashable-type-numpy-ndarray-in-lag-plot-python\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I want to check autocorrelation in this time series dataset. I use lag_plot() function for this purpose.\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import lag_plot\n",
      "series = pd.read_csv('daily-minimum-temperatures.csv', header=0)\n",
      "lag_plot(series)\n",
      "pyplot.show()\n",
      "\n",
      "Then I get this error: \n",
      "\n",
      "for val in OrderedDict.fromkeys(data): TypeError: unhashable type: 'numpy.ndarray'\n",
      "\n",
      "How can I fix that?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 ValueError when using pandas Groupby with aggregate function List\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082738/valueerror-when-using-pandas-groupby-with-aggregate-function-list\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a dataset that I am trying to organize using the pandas function groupby. I am trying to groupby the column zip code and aggregate by list so that the values of the dataframe that are grouped form a new list\n",
      "sheet1 = pd.read_excel(data, \"Data\")\n",
      "other_columns = sheet1.loc[:, sheet1.dtypes!=np.float64]    \n",
      "other_columns=other_columns.groupby(\"zip_code\").agg(list).reset_index()\n",
      "\n",
      "I want the code to be grouped by zip code, with each cell of grouped columns to be a list of the values. However, I am receiving this error message:\n",
      "ValueError: Shape of passed values is (75, 665), indices imply (74, 665)\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Count Number of cycles in graph/ plot using Python / Pandas / Numpy\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082596/count-number-of-cycles-in-graph-plot-using-python-pandas-numpy\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "How can I  find out how many times the Y value (Speed) ramps up and down from 700 -800 RPM to 1600 to 1800 RPM and vice versa from graph plotted in matplotlib by using python / pandas / numpy libraries. I have plotted this graph using matplotlib and using dataframe.\n",
      "Expected output should be--->\n",
      "Number of Ramps up in Speed  = 2 & Down = 2 looking at attached graph \n",
      "Attached image /graph below for more clarification \n",
      "enter image description here\n",
      "fig = plt.figure(figsize =(18,10))\n",
      "\n",
      "ax =plt.subplot(311)\n",
      "plt.plot(df.speed)\n",
      "\n",
      "ax.set_yticks([0, 500, 700, 1000, 1500, 1700, 2000])\n",
      "\n",
      "ax.set_xlabel(\"Time (Seconds)\")\n",
      "ax.set_ylabel(\"Speed (RPM)\")\n",
      "plt.grid()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Here is a vectorized solution:\n",
      "# Check if values are above or below a threshold\n",
      "threshold = 1200\n",
      "df['over_threshold'] = df['speed'] > threshold\n",
      "\n",
      "# Compare to the previous row\n",
      "# If over_threshold has changed, \n",
      "# then you either went above or fell below the threshold\n",
      "df['changed'] = df['over_threshold'] != df['over_threshold'].shift(1)\n",
      "\n",
      "# First one has, by definition, has no previous value, so we should omit it\n",
      "df = df.loc[1:,]\n",
      "\n",
      "# Count how many times a row is newly above or below threshold\n",
      "df.loc[df['changed']].groupby('over_threshold').agg({'changed': 'count'})\n",
      "\n",
      "You will then have counts of how many times you have ramped up or down.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to optimize pandas queries with interval index?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082582/how-to-optimize-pandas-queries-with-interval-index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I often need to query large pandas dataframes and I am looking to find the most optimized way to perform these queries. I deal with Linearly Referenced Systems (highways). Most of the roadway attributes are stored as linear events and they are indexed by route id and beginning and ending milepost. What I am looking for is querying the roadway attributes for a specific milepost. The following is a sample of a dataframe that stores the roadway attributes:\n",
      "import pandas as pd\n",
      "idx = pd.MultiIndex.from_arrays(\n",
      "    [pd.Index(['FC','FC','FC','FC','OWNER','OWNER','OWNER','OWNER']),\n",
      "     pd.Index(['RID1','RID1','RID2','RID2','RID1','RID1','RID2','RID2']),\n",
      "     pd.IntervalIndex.from_arrays([0,1,10,11,0,1,10,11],\n",
      "    [1,2,11,12,1,2,11,12])])\n",
      "idx.names = ['Item','RID','MP']\n",
      "df = pd.DataFrame({'Value':[1,2,3,4,5,6,7,8]})\n",
      "df.index = idx\n",
      "\n",
      "and the following is an example of a query dataframe:\n",
      "query_df = pd.DataFrame({\n",
      "        'Item':['FC'  ,'OWNER','FC'  ,'OWNER','OWNER'],\n",
      "        'RID' :['RID1','RID1' ,'RID1','RID2' ,'RID2' ],\n",
      "        'MP'  :[0.2   ,1.5    ,1.6   ,11.1   ,10.9   ]})\n",
      "query_df['Value'] = query_df.apply(lambda r:df.loc[r.Item].loc[r.RID].loc[r.MP],axis=1)\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Splitting a Dataframe into multiple emails\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082539/splitting-a-dataframe-into-multiple-emails\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm trying to set up a program where i can email an itemized statement to each credit card user at our company. I've pulled this in using pandas and grouped the raw data to how i need it. I would like to be able to send this out with the unique amounts to each recipient. I'm unsure on where to go next\n",
      "Here is what i have so far.\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"/users/user/documents/data.csv\")\n",
      "df['Total'] = df.groupby('Card Number')['Posted Amount'].transform('sum')\n",
      "body = df.groupby(['Card Number','Cardholder Name','Email','Total','Merchant \n",
      "     Name'])['Posted Amount'].agg('sum')\n",
      "print(body)\n",
      "\n",
      "I get a table afterwards that reads as follows:\n",
      "1234,billy,billy@test.com,500,paypal,50\n",
      "                              ebay**,25\n",
      "                              paypal,175\n",
      "                              amazon,250\n",
      "1235,Jim,Jim@test.com,500,paypal,25\n",
      "                          ebay**,50\n",
      "                          paypal,250\n",
      "                          amazon,175\n",
      "\n",
      "Etc... etc...\n",
      "I know i can set this to an HTML format but how do I go about sending each user a personalized email based on their spending? I would like to set the total to the subject of the amount per cardholder. Then in the body include the merchant names and posted amounts. What would be the best way to break this up?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can use Jinja.  Create a Jinja template, and pass your df as a dictionary(df.to_dict) with the logics you want:\n",
      "E.g. Templete(email_template.eml) and code from Terek Ziade’s Python Microservices Development\n",
      "Date: {{date}} \n",
      "From: {{from}} \n",
      "Subject: {{subject}} \n",
      "To: {{to}} \n",
      "Content-Type: text/plain \n",
      "\n",
      "Hello {{name}}, \n",
      "\n",
      "We have received your payment! \n",
      "\n",
      "Below is the list of items we will deliver for lunch: {% for item in items %}- {{item['name']}} ({{item['price']}} Euros) {% endfor %} \n",
      "\n",
      "Thank you for your business!\n",
      "\n",
      "-Tarek's Burger\n",
      "\n",
      "Code:\n",
      "from datetime import datetime     \n",
      "from jinja2 import Template     \n",
      "from email.utils import format_datetime     \n",
      "\n",
      "def render_email(**data):         \n",
      "    with open('email_template.eml') as f:\n",
      "        template = Template(f.read())         \n",
      "    return template.render(**data)\n",
      "\n",
      "data = {'date': format_datetime(datetime.now()),\n",
      "        'to': 'bob@example.com',\n",
      "        'from': 'tarek@ziade.org', \n",
      "        'subject': \"Your Tarek's Burger order\", \n",
      "        'name': 'Bob',\n",
      "        'items': [{'name': 'Cheeseburger', 'price': 4.5}, {'name': 'Fries', 'price': 2.}, {'name': 'Root Beer', 'price': 3.}]}     \n",
      "\n",
      "print(render_email(**data))\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Count each observation a row\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082491/count-each-observation-a-row\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a pandas df named df, with millions of observations (rows) and only 4 columns. \n",
      "I'm trying to convert the event_type column into several columns, and add a count to each row for that column.\n",
      "My df looks like this:\n",
      "    event_type            event_time       organization_id     user_id                 \n",
      "\n",
      "0   Applied Saved View  2018-11-22 10:59:57.360    3            0\n",
      "\n",
      "And I'm looking for this:\n",
      "    Applied_Saved_View event_time              organization_id user_id\n",
      "0       1              2018-11-22 10:59:57.360    3              0\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "I believe you are looking for something called pd.get_dummies. I assume you are trying to make this categorical data? I have no way of testing without sample data but see code below.\n",
      "df2 = pd.get_dummies(df['event_type'])\n",
      "new_df = pd.concat([df2,df],axis=1)\n",
      "\n",
      "I should mention, you should see how many unique values there are in this event type column because those each will become rows whether its 10 or 100000 unique values \n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 how to print/extract all rows and columns from the console?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082479/how-to-print-extract-all-rows-and-columns-from-the-console\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "i want to print or extract all columns and rows from my result data. The data is 61 rows and columns. The console displays like 6 or 7 columns and rows and fills the rest with dots but I need to obtain all the numbers. I uploaded an image of my data in the console. \n",
      "Image of the data with dots and missing columns and rows.\n",
      "Your help is deeply appreciated!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Check out the Pandas documentation. You should be able to set display for max_rows and max_columns like so:\n",
      "import pandas as pd\n",
      "\n",
      "pd.set_option(\"display.max_rows\", None)\n",
      "pd.set_option(\"display.max_columns\", None)\n",
      "\n",
      "As noted in the docs, None sets the limit to be unlimited. Warning: you're going to fill your console up very quickly with large frames.\n",
      "If you have particularly wide columns, you may need to tinker with display.max_colwidth.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Filter rows based on the count of unique values\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082451/filter-rows-based-on-the-count-of-unique-values\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I need to count the unique values of column A and filter out the column with values greater than say 2\n",
      " A C\n",
      "Apple 4\n",
      "Orange 5\n",
      "Apple 3\n",
      "Mango 5\n",
      "Orange 1\n",
      "\n",
      "I have calculated the unique values but not able to figure out how to filer them df.value_count()\n",
      "I want to filter column A that have greater than 2, expected Dataframe\n",
      "A B\n",
      "Apple 4\n",
      "Orange 5\n",
      "Apple 3\n",
      "Orange 1\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "value_counts should be called on a Series (single column) rather than a DataFrame:\n",
      "counts = df['A'].value_counts()\n",
      "\n",
      "Giving:\n",
      "A\n",
      "Apple     2\n",
      "Mango     1\n",
      "Orange    2\n",
      "dtype: int64\n",
      "\n",
      "You can then filter this to only keep those >= 2 and use isin to filter your DataFrame:\n",
      "filtered = counts[counts >= 2]\n",
      "df[df['A'].isin(filtered.index)]\n",
      "\n",
      "Giving:\n",
      "        A  C\n",
      "0   Apple  4\n",
      "1  Orange  5\n",
      "2   Apple  3\n",
      "4  Orange  1\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Pandas does not recognize global dataframe in a function\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082325/pandas-does-not-recognize-global-dataframe-in-a-function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have a method where I am trying to append one dataframe to anotehr, one of the dataframes need to be accessible outside the method.\n",
      "def method1():\n",
      "    global df_0;\n",
      "    df_1=pd.read_excel(filepath)\n",
      "    df_0=df_0.append(df_1)\n",
      "\n",
      "method1()\n",
      "\n",
      "I get:\n",
      "df_0 is not defined\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 get the key from a dict into a variable to use it across the script\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082145/get-the-key-from-a-dict-into-a-variable-to-use-it-across-the-script\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "Reading the data from a table into the dict and want to assign each value to a variable which I can use it in the script.\n",
      "code from utils\n",
      "def get_the_column_values():\n",
      "abcd = \"\"\"select abc from my_table \n",
      "            \"\"\"\n",
      "            df = pd.read_sql(abcd, conn)\n",
      "            abcs= df['abc'].to_list()\n",
      "            dict = {}\n",
      "            for data in abcs:\n",
      "                 dict[data] = ''\n",
      "\n",
      "code from the script \n",
      "get_column = abc.get_the_column_values()\n",
      "\n",
      "result is \n",
      "{'apple':'','Orange':''}\n",
      "\n",
      "Expected results is  \n",
      "fruit_var1=<somecode>\n",
      "fruit_var2=<somecode>\n",
      "print(fruit_var1) \n",
      "print(fruit_var2)\n",
      "apple\n",
      "Orange\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Get every nth weekday of month periodic in pandas [duplicate]\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57082058/get-every-nth-weekday-of-month-periodic-in-pandas\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "\n",
      "This question already has an answer here:\n",
      "\n",
      "\n",
      "Resample a pandas timeseries by “1st Monday of month” etc\n",
      "\n",
      "                    2 answers\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "I need to generate an array of dates at specific intervals.  for example every 3 sunday of the month:\n",
      "[2019-02-17, 2019-03-17, 2019-04-21]\n",
      "\n",
      "Is it possible to do this using standard pandas functions?  For example, specifying some particular freq field in the pd.date_range method or using pd.Dateoffset?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You could do something like this:\n",
      "s = pd.date_range('2019-01-01','2019-12-31', freq='D')\n",
      "\n",
      "s[(s.dayofweek == 6) & (s.day>=15) & (s.day<=21)]\n",
      "\n",
      "Output:\n",
      "DatetimeIndex(['2019-01-20', '2019-02-17', '2019-03-17', '2019-04-21',\n",
      "               '2019-05-19', '2019-06-16', '2019-07-21', '2019-08-18',\n",
      "               '2019-09-15', '2019-10-20', '2019-11-17', '2019-12-15'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Pandas dataframe outputting as text rather than standard format\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081937/pandas-dataframe-outputting-as-text-rather-than-standard-format\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am using pandas to work with an excel file and create a dataframe from that. It's able to read the file, but when i print the resulting dataframe it's showing up in a text format which is a lot different from the normal one we are used to.\n",
      "This is how i'm reading the excel file and printing it:\n",
      "locations = pd.read_excel('file.xlsx') \n",
      "print(locations)\n",
      "\n",
      "this outputs as \n",
      "which looks like a text output \n",
      "I want it to output normally like this: \n",
      "\n",
      "how can i fix my output?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "The 'pretty'format is only outputted when 'print' isn't called. Try:\n",
      "locations = pd.read_excel('file.xlsx')\n",
      "locations\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Dictionary Comprehension Syntax Understanding\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081862/dictionary-comprehension-syntax-understanding\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I was working with someone on here who helped me solve a problem by using list comprehnsion to populate a dictionary. I have an output called bar that contains open high low and close data and I just wanted to assign them to a dictionary. \n",
      "Example of the data:\n",
      "Timestamp: 2019-07-17 10:58:00+00:00\n",
      "Open: 1.8877\n",
      "High: 1.8878\n",
      "Low: 1.8871\n",
      "Close: 1.8878\n",
      "\n",
      "We did that via this code:\n",
      "def append(self, bar):\n",
      "    symbols = symbol = ['CLE', 'RBE']\n",
      "    bar_keys = key = ['Open', 'High', 'Low', 'Close']\n",
      "    self.newest_bar = {key: getattr(bar, key) for key in bar_keys}\n",
      "\n",
      "So it'll essentially get the bar.Open, bar.Close etc for every value in the list bar_keys. \n",
      "I'm essentially trying to add on top of this a 2nd symbol and trying to use the same exact syntax but I'm not sure why it's not working. \n",
      "I just want to now just take the bar.Close value for each symbol (CLE, RBE) and add them to my dictionary but I keep getting unhashable type. What is the difference between the working version above and my version below?\n",
      "symbols = symbol = ['CLE', 'RBE']\n",
      "self.closes = {key: getattr(bar, 'Close') for symbol in symbols}\n",
      "\n",
      "this is the whole code snippet with his working version and what ive added:\n",
      "def append(self, bar):\n",
      "    symbols = symbol = ['CLE', 'RBE']\n",
      "    bar_keys = key = ['Open', 'High', 'Low', 'Close']\n",
      "    self.newest_bar = {key: getattr(bar, key) for key in bar_keys}\n",
      "    self.bars[bar.Timestamp] = self.newest_bar\n",
      "    self.closes = {key: getattr(bar, 'Close') for symbol in symbols}\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "key is a list. You can't key a dict with a list. In the other code, the key from inside the dict comprehension is used instead.\n",
      "Stop using the same variable name for a bunch of different things.  \n",
      "def append(self, bar):\n",
      "    symbols = ['CLE', 'RBE']\n",
      "    bar_keys = ['Open', 'High', 'Low', 'Close']\n",
      "    self.newest_bar = {key: getattr(bar, key) for key in bar_keys}\n",
      "    self.bars[bar.Timestamp] = self.newest_bar\n",
      "    self.closes = {symbol: getattr(bar, 'Close') for symbol in symbols}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Prediction column is not being found in the axis\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081745/prediction-column-is-not-being-found-in-the-axis\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am predicting a specific value using a linear regression model. However, the actual column I want to predict is not \"found in the axis\"\n",
      "I have changed the prediction value to a random column and it works fine. But that's not the column I'm after.\n",
      "Here is the format of the Excel file.\n",
      "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n",
      "\n",
      "data = pd.read_excel(\"ENB2012_data.xlsx\")\n",
      "print(data.head())\n",
      "data = data[[\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"]]\n",
      "predict = \"Y1\"\n",
      "X = np.array(data.drop([predict], 1)) #the error lies on this line.\n",
      "Y = np.array(data[predict])\n",
      "\n",
      "I'm expecting the output to show predicted values for column Y1 \n",
      "Error message: KeyError: \"['Y1'] not found in axis\"\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "When you did this:\n",
      "data = data[[\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"]]\n",
      "\n",
      "you removed the column \"Y1\" from data. That way, you don't have \"Y1\" anymore in data.\n",
      "Try doing this:\n",
      "data = pd.read_excel(\"ENB2012_data.xlsx\")\n",
      "print(data.head())\n",
      "data = data[[\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"Y1\"]]\n",
      "predict = \"Y1\"\n",
      "Y = np.array(data[predict])\n",
      "X = np.array(data.drop([predict], 1))\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Choose higher value based off column value between two dataframes\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081743/choose-higher-value-based-off-column-value-between-two-dataframes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "question to choose value based on two df. \n",
      ">>> df[['age','name']]\n",
      "    age   name\n",
      "0    44   Anna\n",
      "1    22    Bob\n",
      "2    33  Cindy\n",
      "3    44  Danis\n",
      "4    55  Cindy\n",
      "5    66  Danis\n",
      "6    11   Anna\n",
      "7    43    Bob\n",
      "8    12  Cindy\n",
      "9    19  Danis\n",
      "10   11   Anna\n",
      "11   32   Anna\n",
      "12   55   Anna\n",
      "13   33   Anna\n",
      "14   32   Anna\n",
      "\n",
      ">>> df2[['age','name']]\n",
      "   age   name\n",
      "5   66  Danis\n",
      "4   55  Cindy\n",
      "0   44   Anna\n",
      "7   43    Bob\n",
      "\n",
      "expected result is all rows that value 'age' is higher than df['age'] based on column 'name.\n",
      "expected result\n",
      "    age   name\n",
      "12   55   Anna\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Per comments, use merge and filter dataframe:\n",
      "df.merge(df2, on='name', suffixes={'','_y'}).query('age > age_y')[['name','age']]\n",
      "\n",
      "Output:\n",
      "   name  age\n",
      "4  Anna   55\n",
      "\n",
      "\n",
      "IIUC, you can use this to find the max age of all names:\n",
      "pd.concat([df,df2]).groupby('name')['age'].max()\n",
      "\n",
      "Output:\n",
      "name\n",
      "Anna     55\n",
      "Bob      43\n",
      "Cindy    55\n",
      "Danis    66\n",
      "Name: age, dtype: int64\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Is there a way of group by month in Pandas starting at specific day number?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081662/is-there-a-way-of-group-by-month-in-pandas-starting-at-specific-day-number\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I'm trying to group by month some data in python, but i need the month to start at the 25 of each month, is there a way to do that in Pandas?\n",
      "For weeks there is a way of starting on Monday, Tuesday, ... But for months it's always full month.\n",
      "pd.Grouper(key='date', freq='M')\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You could offset the dates by 24 days and groupby:\n",
      "np.random.seed(1)\n",
      "dates = pd.date_range('2019-01-01', '2019-04-30', freq='D')\n",
      "df = pd.DataFrame({'date':dates,\n",
      "                   'val': np.random.uniform(0,1,len(dates))})\n",
      "\n",
      "# for groupby\n",
      "s = df['date'].sub(pd.DateOffset(24))\n",
      "\n",
      "(df.groupby([s.dt.year, s.dt.month], as_index=False)\n",
      "   .agg({'date':'min', 'val':'sum'})\n",
      ")\n",
      "\n",
      "gives\n",
      "        date        val\n",
      "0 2019-01-01  10.120368\n",
      "1 2019-01-25  14.895363\n",
      "2 2019-02-25  14.544506\n",
      "3 2019-03-25  17.228734\n",
      "4 2019-04-25   3.334160\n",
      "\n",
      "Another example:\n",
      "np.random.seed(1)\n",
      "dates = pd.date_range('2019-01-20', '2019-01-30', freq='D')\n",
      "df = pd.DataFrame({'date':dates,\n",
      "                   'val': np.random.uniform(0,1,len(dates))})\n",
      "\n",
      "s = df['date'].sub(pd.DateOffset(24))\n",
      "df['groups'] = df.groupby([s.dt.year, s.dt.month]).cumcount()\n",
      "\n",
      "gives\n",
      "         date       val  groups\n",
      "0  2019-01-20  0.417022       0\n",
      "1  2019-01-21  0.720324       1\n",
      "2  2019-01-22  0.000114       2\n",
      "3  2019-01-23  0.302333       3\n",
      "4  2019-01-24  0.146756       4\n",
      "5  2019-01-25  0.092339       0\n",
      "6  2019-01-26  0.186260       1\n",
      "7  2019-01-27  0.345561       2\n",
      "8  2019-01-28  0.396767       3\n",
      "9  2019-01-29  0.538817       4\n",
      "10 2019-01-30  0.419195       5\n",
      "\n",
      "And you can see the how the cumcount restarts at day 25.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Copy a column from one dataframe to another when they both share an index in Pandas\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081653/copy-a-column-from-one-dataframe-to-another-when-they-both-share-an-index-in-pan\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am trying to copy a column of data from one dataframe to another, using the index as a reference. When copying the column, I want to fill any entry that does not appear in both dataframes with a NaN. \n",
      "For example, I have these two dummy dfs:\n",
      "df1 = \n",
      "       col_1  col_2  col_3  col_4\n",
      "index\n",
      "A          1      4      7     10\n",
      "B          2      5      8     11\n",
      "C          3      6      9     12\n",
      "\n",
      "df2 = \n",
      "       col_5  col_6\n",
      "index\n",
      "A         13     15\n",
      "C         14     16\n",
      "\n",
      "And I would like to copy col_5 to df1 based on the shared index so df1 looks like:\n",
      "df1 = \n",
      "       col_1  col_2  col_3  col_4  col_5\n",
      "index\n",
      "A          1      4      7     10   15\n",
      "B          2      5      8     11   NaN\n",
      "C          3      6      9     12   16\n",
      "\n",
      "Since they're different lengths I can't simply do df1['col_5'] = df2['col_5'], and I didn't have any success with a  df1.merge(df2, how='left'), and then I'd have to drop any unwanted columns anyway.\n",
      "Any help appreciated. Thanks!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "You can do df1.insert(df1.shape[1], \"col_5\", df2[\"col_5\"], which will put col_5 at the end (df1.shape[1] returns the number of columns in df1) of df1 with the indices properly matched.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How to convert scientic notation of my code?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081426/how-to-convert-scientic-notation-of-my-code\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I've created a function to check the range of values in a pandas dataframe. But the output is producing all values with scientific notation. \n",
      "When I select_dtypes to include only int, I don't get this problem. It only happens when I include float. How can I get non-scientific values?\n",
      "# Function to check range of value in a col.\n",
      "def value_range(col):\n",
      "    max = data[col].max()\n",
      "    min = data[col].min()\n",
      "    return max-min\n",
      "\n",
      "# value_range('total_revenue')\n",
      "\n",
      "numerical_data = data.select_dtypes(include=[float, int]).columns\n",
      "print(value_range(numerical_data))\n",
      "\n",
      "Out:\n",
      "Unnamed: 0                               3.081290e+05\n",
      "number_of_sessions                       1.340000e+02\n",
      "total_bounce                             1.080000e+02\n",
      "total_hits                               3.706000e+03\n",
      "days_difference_from_last_first_visit    1.800000e+02\n",
      "transactions                             2.500000e+01\n",
      "total_revenue                            2.312950e+10\n",
      "organic_search                           1.110000e+02\n",
      "direct                                   8.300000e+01\n",
      "referral                                 1.070000e+02\n",
      "social                                   1.120000e+02\n",
      "paid_search                              4.400000e+01\n",
      "affiliates                               3.700000e+01\n",
      "display                                  8.500000e+01\n",
      "target_if_purchase                       1.000000e+00\n",
      "target_total_revenue                     2.039400e+09\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "value_range(numerical_data).apply(lambda x: format(x, 'f')) solves the problem. Thanks Sparrow1029.\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Error when trying to label x and y axis in Pandas horizontal bar graph\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081423/error-when-trying-to-label-x-and-y-axis-in-pandas-horizontal-bar-graph\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am trying to set x and y labels on my horizontal bar graph, but I get this error. Please help me through it. \n",
      "This is my code for the graph: \n",
      "alt_method[['Jan_avg','Jan_count']].sort_values(by=['Jan_avg','Jan_count'],ascending=True).plot.barh(xlim = (0,60000),ylim = (6300.0,64289.0))\n",
      "\n",
      "alt_method.set_xlabel('Shipment')\n",
      "\n",
      "and this is my error message:\n",
      "\n",
      "AttributeError: 'DataFrame' object has no attribute 'set_xlabel'\n",
      "\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "alt_method is a DataFrame. You can use .plot() to plot the DataFrame, but to set an axis label, you should use the plotting commands available via matplotlib.pyplot (commonly abbreviated to plt):\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df.plot() # per your code\n",
      "plt.ylabel('ylabel goes here')\n",
      "plt.show()\n",
      "\n",
      "A reproducible example: \n",
      "df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/fpp2/goog200.csv\", index_col=0)\n",
      "df['value'].plot()\n",
      "plt.ylabel('price')\n",
      "plt.xlabel('time')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Find and return max value for a column and the row of the max value as an array\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081259/find-and-return-max-value-for-a-column-and-the-row-of-the-max-value-as-an-array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I am working with a panda dataframe to try to find maximum values. I need to return the max value for a column as well as the row that max value was found in. For example, my dataframe looks like this:\n",
      "       0    1    2\n",
      "0     43   36   33\n",
      "1     43   36   33\n",
      "2     43   36   33\n",
      "3     43   36   33\n",
      "4     43   36   33\n",
      "5     43   36   33\n",
      "6    174  168  165\n",
      "7    161  153  140\n",
      "8    141  132  129\n",
      "9    124  107  111\n",
      "10   130  106  109\n",
      "11   120   98  101\n",
      "\n",
      "I want to return the maximum value for each column and the row that the max value was found in like the following as an array:\n",
      "0   174   6\n",
      "1   168   6\n",
      "2   165   6\n",
      "\n",
      "I've used:\n",
      "for column in df:\n",
      "##    finds the max in each column\n",
      "    df_max=np.amax(df,axis=0)\n",
      "\n",
      "to get:\n",
      "0   174\n",
      "1   168\n",
      "2   165\n",
      "\n",
      "However, I haven't found a way to get the row value.\n",
      "I've used:\n",
      "for column in df:\n",
      "    row_df-df[df==np.amax(df,0)]\n",
      "\n",
      "row_df=row_df.dropna(0,how='all')\n",
      "print(\"row_df:\")\n",
      "print(row_df)\n",
      "\n",
      "row_df_index=row_df.index.values\n",
      "print(\"row_df_index\")\n",
      "print(row_df_index)\n",
      "\n",
      "to get:\n",
      "row_df:\n",
      "       0    1    2\n",
      "6     174   168   165\n",
      "\n",
      "row_df_index:\n",
      "[6]\n",
      "\n",
      "But these won't help me if I have max values in other rows since I need to be able to match them up.\n",
      "Any suggestions?\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "idxmax is the method which returns the index of first occurrence of maximum over requested axis.\n",
      "So you could use agg to aggregate results:\n",
      "df.agg(['max', 'idxmax'])\n",
      "\n",
      "which returns:\n",
      "          0    1    2\n",
      "max     174  168  165\n",
      "idxmax    6    6    6\n",
      "\n",
      "If you need to have max and idxmax as columns, use transpose:\n",
      "df.agg(['max', 'idxmax']).transpose()\n",
      "\n",
      "which gives:\n",
      "   max  idxmax\n",
      "0  174       6\n",
      "1  168       6\n",
      "2  165       6\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Is there a way to turn a pandas dataframe in matlab into a matlab table?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081181/is-there-a-way-to-turn-a-pandas-dataframe-in-matlab-into-a-matlab-table\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I have matlab 2019 running a python script using the matlab py. wrapper. It returns a pandas dataframe in Matlab. This dataframe is a table of strings. I am curious if there is a way to convert the pandas dataframe in matlab into a matlab table. \n",
      "Currently I am writing the pandas dataframe to a csv and importing it into matlab as a work around.\n",
      "clear classes\n",
      "mod = py.importlib.import_module('test')\n",
      "py.importlib.reload(mod)\n",
      "cP = py.test.FlowTracker().todays_portfolio(1,10)\n",
      "\n",
      "cP returns my pandas dataframe, which I am trying to get correctly into a matlab table.\n",
      "cant seem to convert it no matter what I try \n",
      "thanks!\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 How do I calculate number of words and number of unique words contained within a list of a column across all rows of my dataframe?\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081116/how-do-i-calculate-number-of-words-and-number-of-unique-words-contained-within-a\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I generated a column df['adjectives'] in my pandas dataframe that has a list of all the adjectives from another column, df['reviews'].\n",
      "The values of df['adjectives'] are in this format, for example: \n",
      "\n",
      "['excellent', 'better', 'big', 'unexpected', 'excellent', 'big']\n",
      "\n",
      "I would like to create a new column that counts the total number of words in df['adjectives'] as well as the number of 'unique' words in df['adjectives'].\n",
      "The function should iterate across the entire dataframe and apply the counts for each row.\n",
      "For the above row example, I would want df['totaladj'] to be 6 and df['uniqueadj'] to be 4 (since 'excellent' and 'big' are repeated)\n",
      "import pandas as pd\n",
      "\n",
      "df=pd.read_csv('./data.csv')\n",
      "\n",
      "df['totaladj'] = df['adjectives'].str.count(' ') + 1\n",
      "\n",
      "df.to_csv('./data.csv', index=False)\n",
      "\n",
      "The above code works when counting the total number of adjectives, but not the unique number of adjectives.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Is this the type of behavior that you are looking for?\n",
      "Based off of your description I assumed that the values in the adjectives column are a string formatted like a list e.g. \"['big','excellent','small']\"\n",
      "The code below converts the strings to a list using split(), and then gets the length using len().Finding the number of unique adjectives is done by converting the list to a set before using len().\n",
      "df['adjcount'] = df['adjectives'].apply(lambda x:  len(x[1:-1].split(',')))\n",
      "\n",
      "df['uniqueadjcount'] =  df['adjectives'].apply(lambda x:  len(set(x[1:-1].split(','))))\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Summing by a column which is not the index variable (Python)\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081115/summing-by-a-column-which-is-not-the-index-variable-python\n",
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "New to Python here (background is in SAS primarily).\n",
      "I am trying to sum by a column which is not the index variable (in the below example, the index variable is 'department' and I am trying to sum by 'employee_fixed'). I can't make it the index variable because the index variable is being used as part of a for loop. Code below should make it clear.\n",
      "#Creating dataset of departments you want to keep in your dataset\n",
      "   #Setting df to only include departments specified\n",
      "    cc = ['Furniture','Food','Clothing']\n",
      "    for index in range(len(cc)): \n",
      "    df3_cc = df[df['department'].isin([cc[index]])]\n",
      "    #set the department as the index variable so you can aggregate \n",
      "    df3_cc = df3_cc.set_index('department')\n",
      "    df3_cc\n",
      "    #Creating dataset of people who are NOT approved department\n",
      "     #Setting df to only include the condition specified in \"notapprov\"\n",
      "    notapprov = ['NO']\n",
      "    df3_cc = df3_cc[df3_cc['appr_list_chc'].isin(notapprov)]\n",
      "    df3_cc\n",
      "    #drop unnecessary columns from dataframe\n",
      "    df3_cc = df3_cc.drop(['fisc_yr_per'], axis=1)\n",
      "    # sum up the hours based on the indexed departments\n",
      "    # for those NOT approved to work that department and charging anyway\n",
      "    # >40hrs in the latest period\n",
      "    df3_cc = df3_cc[df3_cc['hrs_per'] >= 40].sum(level='employee_fixed') \n",
      "    #output to CSV\n",
      "    df3_cc.to_csv(r\"C:\\Users\\etc\\table3_\"+cc[index]+\".csv\")\n",
      "\n",
      "The end result should be a separate CSV for each item in 'cc', with the summed number of hours of each employee (in 'employee_fixed') working in each department that is not authorized to work in that department (including only those who works >=40 hours in the current period).\n",
      "Sample Input:\n",
      "Department employee_fixed appr_list_chc hrs_per\n",
      "Furniture John NO 45\n",
      "Furniture Jacob NO 50\n",
      "Food Jackie YES 100\n",
      "Food Jeremy NO 75\n",
      "FOOD Jim NO 10\n",
      "Clothing Jonas NO 200\n",
      "Clothing Jerry YES 10\n",
      "Output:\n",
      "table3_furniture.csv\n",
      "Department employee_fixed appr_list_chc hrs_per\n",
      "Furniture John NO 45\n",
      "Furniture Jacob NO 50\n",
      "table3_food.csv\n",
      "Department employee_fixed appr_list_chc hrs_per\n",
      "Food Jeremy NO 75\n",
      "table3_food.csv\n",
      "Department employee_fixed appr_list_chc hrs_per\n",
      "Clothing Jonas NO 200\n",
      "Thanks!\n",
      "EDIT: found the answer! \n",
      " df3_cc = df3_cc[df3_cc['hrs_per'] >= 40].sum(level='employee_fixed') \n",
      "became\n",
      " df3_cc = df3_cc[df3_cc['hrs_per'] >= 40]\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "\n",
      "Turns out all that was needed was to change one line:\n",
      "df3_cc = df3_cc[df3_cc['hrs_per'] >= 40].sum(level='employee_fixed')\n",
      "\n",
      "to:\n",
      "df3_cc = df3_cc[df3_cc['hrs_per'] >= 40]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 제목 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "질문 Getting error from sqlite : Error binding parameter 12 - probably unsupported type\n",
      "====================================================================================================\n",
      "링크: https://stackoverflow.com/questions/57081101/getting-error-from-sqlite-error-binding-parameter-12-probably-unsupported-ty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** 질문 내용 **************************************************\n",
      "\n",
      "I created a database and table using \"BLOB\" as a datatype for all columns. I am trying to insert data and I keep getting \"Error binding parameter 12 - probably unsupported type.\". I have tried everything I know: from checking the \n",
      "datatype in my columns and matching the datatype when I create the table but non seems to be working. \n",
      "My columns have a lot null values, and there is no way of around getting rid of the nulls. I have also tried pyodbc and that didn't work(I got Driver Error). I am also open to changing to a different library just to get this to work. I just want to create a simple database that stores data from a pandas dataframe \n",
      "# connecting to database\n",
      "oldData = pd.read_excel('onTimeMetrics-2019-7-16.xlsx')\n",
      "\n",
      "#SQL lite Code\n",
      "database = \"On_Time_metrics_Database.db\"\n",
      "\n",
      "table1 =  \"\"\"CREATE TABLE IF NOT EXISTS 'Metrics' (\n",
      "     \"Group\"                           BLOB  ,\n",
      "     \"Sales Order\"                     BLOB  ,\n",
      "     \"Sales Order Type\"                BLOB  ,\n",
      "     \"Product hierarchy Primary Order\" BLOB  , \n",
      "     \"PHC Description\"                 BLOB  ,\n",
      "     \"Ret Expect Ship Date\"            BLOB  , \n",
      "     \"Ret Act Ship Date\"               BLOB  ,\n",
      "     \"Total Orders\"                    BLOB  ,\n",
      "     Late                              BLOB  , \n",
      "     \"On Time\"                         BLOB  ,\n",
      "     \"Order Number\"                    BLOB  ,\n",
      "     \"Cause Codes\"                     BLOB  , \n",
      "     \"Order Created Date\"              BLOB  ,\n",
      "     \"Secondary Cause Codes\"           BLOB  ,\n",
      "     \"Primary Cause Codes\"             BLOB  ,\n",
      "     \"Fiscal Period\"                   BLOB  ,\n",
      "     \"Days Late\"                       BLOB  \n",
      "    );\"\"\"\n",
      "\n",
      "def asfloat(df):\n",
      "\n",
      "    col1 = ['Total Orders','Late', 'On Time', 'Days Late']\n",
      "\n",
      "    df[col1].astype(float, inplace=True)\n",
      "\n",
      "    col2 = ['Sales Order','Product hierarchy Primary Order', 'Order Number',\n",
      "           \"Order Created Date\", \"Fiscal Period\",\"Ret Expect Ship Date\",\"Ret Act Ship Date\"]\n",
      "    df[col2].astype(str, inplace=True)\n",
      "\n",
      "\n",
      "\n",
      "def create_table(conn, create_table_sql):\n",
      "    \"\"\" create a table from the create_table_sql statement\n",
      "    :param conn: Connection object\n",
      "    :param create_table_sql: a CREATE TABLE statement\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    try:\n",
      "        c = conn.cursor()\n",
      "        c.execute(create_table_sql)\n",
      "    except Error as e:\n",
      "        print(e)\n",
      "\n",
      "def insertValue(conn, sql, params):\n",
      "    try:\n",
      "        cursor = conn.cursor()\n",
      "        cursor.executemany(sql, params)\n",
      "\n",
      "        cursor.commit()\n",
      "\n",
      "    except Error as e:\n",
      "        conn.rollback()\n",
      "\n",
      "        print(\"Didn't write a file to Access Database, experienced error\")\n",
      "        print(e)\n",
      "\n",
      "\n",
      "conn = create_connection(database) # I have a create_connection func.\n",
      "\n",
      "\n",
      "sql ='''INSERT INTO Metrics VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''' \n",
      "\n",
      "\n",
      "asfloat(oldData)\n",
      "params = oldData.itertuples(index=False)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if conn is not None:\n",
      "    # create projects table\n",
      "    create_table(conn, table1) #create table\n",
      "    insertValue(conn, sql, params)\n",
      "else:\n",
      "    print(\"Error! cannot create the database connection.\")\n",
      "\n",
      "\n",
      "Error binding parameter 12 - probably unsupported type.\n",
      "\n",
      "************************************************** 답변 내용 **************************************************\n",
      "################################################## 답변이 없습니다 ##################################################\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# In[1]:\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "from urllib.request import urlopen\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# 검색어 설정\n",
    "tag = input(\"검색어를 입력하십시오-->\")\n",
    "\n",
    "# url 설정 및 url 읽기\n",
    "url = \"https://stackoverflow.com/questions/tagged/\" + tag\n",
    "page = urlopen(url) \n",
    "document = page.read()\n",
    "\n",
    "# 객체 생성 및 파일 읽기\n",
    "soup = BeautifulSoup(document, \"html.parser\")\n",
    "questions = soup.find(id=\"questions\")\n",
    "questions_list=questions.find_all(\"a\", class_=\"question-hyperlink\")\n",
    "\n",
    "# 메인 알고리즘\n",
    "for questions in questions_list:\n",
    "    \n",
    "    print(\"@\"*50, \"제목\", \"@\"*50)\n",
    "    \n",
    "    # Q&A 질문 요약\n",
    "    print(\"질문\",questions.get_text()) # 조금 더 가벼움!\n",
    "    \n",
    "    # Q&A 링크 생성\n",
    "    print(\"=\"*100)\n",
    "    print(\"링크:\",\"https://stackoverflow.com\"+questions.get(\"href\"))\n",
    "          \n",
    "    \n",
    "    # Q&A 링크 읽기\n",
    "    qna_url=\"https://stackoverflow.com\"+questions.get(\"href\")\n",
    "    qna_page = urlopen(qna_url) \n",
    "    qna_document = qna_page.read()\n",
    "    \n",
    "    # 객체 생성 및 파일 읽기\n",
    "    soup_qna = BeautifulSoup(qna_document, \"html.parser\")\n",
    "    qna_questions = soup_qna.find(class_=\"question\")\n",
    "    \n",
    "    # 질문 내용 출력\n",
    "    print(\"*\"*50 ,\"질문 내용\", \"*\"*50)\n",
    "    print(qna_questions.select(\"div.post-text\")[0].text)\n",
    "    \n",
    "    # 답변 내용 출력\n",
    "    print(\"*\"*50 ,\"답변 내용\", \"*\"*50)\n",
    "    qna_answers = soup_qna.find(id=\"answers\")\n",
    "    if qna_answers.select_one(\"div.answer > div.post-layout > div.answercell > div.post-text\") == None:\n",
    "        print(\"#\"*50,\"답변이 없습니다\",\"#\"*50)\n",
    "    else:\n",
    "        print(qna_answers.select_one(\"div.answer > div.post-layout > div.answercell > div.post-text\").text)\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n",
    "# In[223]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력해주십시오-->겨울왕국과 같은 애니메이션 영화 하나 추천해주세요\n",
      "검색할 페이지 번호를 입력하십시오-->1\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=3&dirId=303&docId=306484686&qb=6rKo7Jq47JmV6rWt6rO8IOqwmeydgCDslaDri4jrqZTsnbTshZgg7JiB7ZmUIO2VmOuCmCDstpTsspztlbTso7zshLjsmpQ=&enc=utf8§ion=kin&rank=1&search_sort=0&spq=1\n",
      "**********질문 제목\n",
      "영화추천해주세요보스베이비나 겨울왕국 같은 애니메이션영화좋아하구요메이즈러너 해리포터처럼 시리즈도 좋아하고히든피겨스나 악마프라다같은것도 좋아해요\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "유명해서 다 봤을지도 모르지만 혹시나해서 추천 드려봅니당( 추천 영화 모두 애니메이션임 )1,너의 이름은2,주먹왕 랄프3,인크레더블4,인사이드 아웃5,하울의 움직이는 성6,도리를 찾아서7,모아나8,보스베이비2 9,토이스토리10,씽(sing)11,주토피아12,센과 치히로의 행방불명13,빅히어로14,업(up)이것도 너무나 유명하지만 제가 제일 좋아하는 애니메이션 영화 ‘몬스터 주식회사’!! 이상 추천 끝!! 다봤더라도 하나정도는 다시봐도 재미 있을껄요!!ㅎㅎㅎㅎ\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "인크레더블2랑 앤트맨과 와스프 추천드릴게여\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "지식대장 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "유명해서 다 봤을지도 모르지만 혹시나해서 추천 드려봅니당( 추천 영화 모두 애니메이션임 )1,너의 이름은2,주먹왕 랄프3,인크레더블4,인사이드 아웃5,하울의 움직이는 성6,도리를 찾아서7,모아나8,보스베이비2 9,토이스토리10,씽(sing)11,주토피아12,센과 치히로의 행방불명13,빅히어로14,업(up)이것도 너무나 유명하지만 제가 제일 좋아하는 애니메이션 영화 ‘몬스터 주식회사’!! 이상 추천 끝!! 다봤더라도 하나정도는 다시봐도 재미 있을껄요!!ㅎㅎㅎㅎ\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "지식대장 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "인크레더블2랑 앤트맨과 와스프 추천드릴게여\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=8&dirId=80101&docId=331067571&qb=6rKo7Jq47JmV6rWt6rO8IOqwmeydgCDslaDri4jrqZTsnbTshZgg7JiB7ZmUIO2VmOuCmCDstpTsspztlbTso7zshLjsmpQ=&enc=utf8§ion=kin&rank=2&search_sort=0&spq=1\n",
      "**********질문 제목\n",
      "영화 추천해주세요!!(공포영화는 사절)1. 한국영화 (너의 결혼식,나의 특별한 형제,극한직업 등등)2. 미국영화(영어권 영화) (알라딘 같은 종류)3. 애니메이션 영화(겨울왕국,모아나,토이스토리,센과 치히로의 행방불명 등등)\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "tnt1**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 롱 리브더 킹,기생충,말모이2.당갈 3.카\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "tnt1**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 해적: 바다로 간 산적2. 스파이더맨:파 프롬 홈3. 주먹왕 랄프\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "tnt1**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "최근  알라딘 그리고 스파이더맨 파 프롬 홈 봤는데....정말 재밌게... 의미있게...감동적이였습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "설탕인형 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 롱 리브더 킹,기생충,말모이2.당갈 3.카\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "설탕인형 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 해적: 바다로 간 산적2. 스파이더맨:파 프롬 홈3. 주먹왕 랄프\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "설탕인형 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "최근  알라딘 그리고 스파이더맨 파 프롬 홈 봤는데....정말 재밌게... 의미있게...감동적이였습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비빔밥 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 롱 리브더 킹,기생충,말모이2.당갈 3.카\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비빔밥 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 해적: 바다로 간 산적2. 스파이더맨:파 프롬 홈3. 주먹왕 랄프\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비빔밥 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "최근  알라딘 그리고 스파이더맨 파 프롬 홈 봤는데....정말 재밌게... 의미있게...감동적이였습니다.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "작성자가 직접 삭제한 답변입니다.\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 롱 리브더 킹,기생충,말모이2.당갈 3.카\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "작성자가 직접 삭제한 답변입니다.\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "1. 해적: 바다로 간 산적2. 스파이더맨:파 프롬 홈3. 주먹왕 랄프\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "작성자가 직접 삭제한 답변입니다.\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "최근  알라딘 그리고 스파이더맨 파 프롬 홈 봤는데....정말 재밌게... 의미있게...감동적이였습니다.\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=3&dirId=30302&docId=331140474&qb=6rKo7Jq47JmV6rWt6rO8IOqwmeydgCDslaDri4jrqZTsnbTshZgg7JiB7ZmUIO2VmOuCmCDstpTsspztlbTso7zshLjsmpQ=&enc=utf8§ion=kin&rank=3&search_sort=0&spq=1\n",
      "**********질문 제목\n",
      "영화 추천해주세요\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "저는 주변에서도 로봇같다, 감정변화가 거의 없는 것 같다는 소리를 많이 들어요.남들이면 아무리 긴장할 상황이어도 긴장같은 건 안되고 슬프다는 영화를 봐도 아무렇지 않고 감동받지도 않아요. 웃기다는 걸 봐도 웃기지가 않고요. 그냥 친구들이랑 놀 때 재밌으면 웃긴 하는데 다른 감정변화는 거의 없어요..그리고 남자친구가 보고 10번울었다는 영화를 추천해줘서 봤었는데도 별 감흥 없고요ㅜ 근데 저도 영화보고 여러 감정을 느껴보고 싶어서사랑을 느낄 수 있는 거나 슬프거나 감동적인 영화 좀 추천해주세요.!\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "선물  증인  언터처블  추천요^^\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "제가 감명깊게본 영화중엔쇼생크탈출노킹온헤븐스도어죽은시인의사회쉰들러리스트라이언일병구하기그린마일굿윌헌팅이정도가 있네요\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "꼭 웃고울고의 감정이 아니라저한테 특별한 감정을 줬던 영화들을 추천드려볼께요.​일단국내영화는주홍글씨 정말추천드립니다네이버에 써잇는 내용,평점 보시지말고 그냥보시길 .. 명작이지만 유작이기때문에.. 감독에대한 분노가 섞여있어서 영화를 객관적으로판단하는 글이 별로없어요.그냥 이영화는 아무선입견없이 봐야 그충격과 오묘한감정을 그대로 느낄수잇어요.​재미/스릴 은씬시티, 슛뎀업 추천드려요너무나 매력적인 두영화이기때문에 질문자님이감정을 느낄수있을거라 생각합니다​둘다 교훈은없고 그냥킬링타임용 영화입니다액션,피,총,베드신이 난무하고 볼거리 화려하고심장이 가만히 있을겨를이 없이봤어요​​로맨스는 너무억지스러운게 많아서 잘안보는데엔젤아이즈 추천드립니다단순한로맨스물은 아니고 여러가지요소들이 담겨있는데로맨스특유에 오글거림이나 억지가 없고사실 더 중요한 내용들을 연결시키는데 로맨스가 껴있는 영화에요 생각많이하게되고 심오한영화​크래쉬 - 이영화도 추천드리고싶은 영화중 하나에요얽히고섥히는게 사람인생이고, 자기 마음대로만 살아지는 것이 아닌인생이죠그런내용을 제일 잘녹인 영화.​스타이즈본 이걸 추천드리고싶긴하나..이영화는 정말 사람마다 평이다른데요그냥 대부분의 평은 슬프고 좋은노래나오는영화다 이런평이나,누구누구가 나쁘다.이런평들이 압도적이라단면만보면 그냥단순한 영화에요..​이 영화를 다른방향으로 알아본다면 진짜 소름돋는 잘만든 영화라고 저는 감히 말할수있어요.영화의 진짜주인공을 레이디가가라고 생각할수가...없어요..좀 자신의삶과 연관해서 평이갈리는 영화에요감독이 정말 대단하다고 느낍니다.모든걸 담아냈다고 생각해요​​​​그리고​마지막으론 그냥 애니메이션 별의미는없으나킬링타임으로 보기에좋은 무황인담 추천드려요굳이비유하면 일본판 영화아저씨랑 정도..전혀 다르지만요 어떤분은 이걸보고 게이가됬다는 평가를 남겼습니다재밌게 보실수있다고 생각하고 이만 적겠습니다.​\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "teng**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "선물  증인  언터처블  추천요^^\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "teng**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "제가 감명깊게본 영화중엔쇼생크탈출노킹온헤븐스도어죽은시인의사회쉰들러리스트라이언일병구하기그린마일굿윌헌팅이정도가 있네요\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "teng**** 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "꼭 웃고울고의 감정이 아니라저한테 특별한 감정을 줬던 영화들을 추천드려볼께요.​일단국내영화는주홍글씨 정말추천드립니다네이버에 써잇는 내용,평점 보시지말고 그냥보시길 .. 명작이지만 유작이기때문에.. 감독에대한 분노가 섞여있어서 영화를 객관적으로판단하는 글이 별로없어요.그냥 이영화는 아무선입견없이 봐야 그충격과 오묘한감정을 그대로 느낄수잇어요.​재미/스릴 은씬시티, 슛뎀업 추천드려요너무나 매력적인 두영화이기때문에 질문자님이감정을 느낄수있을거라 생각합니다​둘다 교훈은없고 그냥킬링타임용 영화입니다액션,피,총,베드신이 난무하고 볼거리 화려하고심장이 가만히 있을겨를이 없이봤어요​​로맨스는 너무억지스러운게 많아서 잘안보는데엔젤아이즈 추천드립니다단순한로맨스물은 아니고 여러가지요소들이 담겨있는데로맨스특유에 오글거림이나 억지가 없고사실 더 중요한 내용들을 연결시키는데 로맨스가 껴있는 영화에요 생각많이하게되고 심오한영화​크래쉬 - 이영화도 추천드리고싶은 영화중 하나에요얽히고섥히는게 사람인생이고, 자기 마음대로만 살아지는 것이 아닌인생이죠그런내용을 제일 잘녹인 영화.​스타이즈본 이걸 추천드리고싶긴하나..이영화는 정말 사람마다 평이다른데요그냥 대부분의 평은 슬프고 좋은노래나오는영화다 이런평이나,누구누구가 나쁘다.이런평들이 압도적이라단면만보면 그냥단순한 영화에요..​이 영화를 다른방향으로 알아본다면 진짜 소름돋는 잘만든 영화라고 저는 감히 말할수있어요.영화의 진짜주인공을 레이디가가라고 생각할수가...없어요..좀 자신의삶과 연관해서 평이갈리는 영화에요감독이 정말 대단하다고 느낍니다.모든걸 담아냈다고 생각해요​​​​그리고​마지막으론 그냥 애니메이션 별의미는없으나킬링타임으로 보기에좋은 무황인담 추천드려요굳이비유하면 일본판 영화아저씨랑 정도..전혀 다르지만요 어떤분은 이걸보고 게이가됬다는 평가를 남겼습니다재밌게 보실수있다고 생각하고 이만 적겠습니다.​\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "선물  증인  언터처블  추천요^^\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "제가 감명깊게본 영화중엔쇼생크탈출노킹온헤븐스도어죽은시인의사회쉰들러리스트라이언일병구하기그린마일굿윌헌팅이정도가 있네요\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "비공개 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "꼭 웃고울고의 감정이 아니라저한테 특별한 감정을 줬던 영화들을 추천드려볼께요.​일단국내영화는주홍글씨 정말추천드립니다네이버에 써잇는 내용,평점 보시지말고 그냥보시길 .. 명작이지만 유작이기때문에.. 감독에대한 분노가 섞여있어서 영화를 객관적으로판단하는 글이 별로없어요.그냥 이영화는 아무선입견없이 봐야 그충격과 오묘한감정을 그대로 느낄수잇어요.​재미/스릴 은씬시티, 슛뎀업 추천드려요너무나 매력적인 두영화이기때문에 질문자님이감정을 느낄수있을거라 생각합니다​둘다 교훈은없고 그냥킬링타임용 영화입니다액션,피,총,베드신이 난무하고 볼거리 화려하고심장이 가만히 있을겨를이 없이봤어요​​로맨스는 너무억지스러운게 많아서 잘안보는데엔젤아이즈 추천드립니다단순한로맨스물은 아니고 여러가지요소들이 담겨있는데로맨스특유에 오글거림이나 억지가 없고사실 더 중요한 내용들을 연결시키는데 로맨스가 껴있는 영화에요 생각많이하게되고 심오한영화​크래쉬 - 이영화도 추천드리고싶은 영화중 하나에요얽히고섥히는게 사람인생이고, 자기 마음대로만 살아지는 것이 아닌인생이죠그런내용을 제일 잘녹인 영화.​스타이즈본 이걸 추천드리고싶긴하나..이영화는 정말 사람마다 평이다른데요그냥 대부분의 평은 슬프고 좋은노래나오는영화다 이런평이나,누구누구가 나쁘다.이런평들이 압도적이라단면만보면 그냥단순한 영화에요..​이 영화를 다른방향으로 알아본다면 진짜 소름돋는 잘만든 영화라고 저는 감히 말할수있어요.영화의 진짜주인공을 레이디가가라고 생각할수가...없어요..좀 자신의삶과 연관해서 평이갈리는 영화에요감독이 정말 대단하다고 느낍니다.모든걸 담아냈다고 생각해요​​​​그리고​마지막으론 그냥 애니메이션 별의미는없으나킬링타임으로 보기에좋은 무황인담 추천드려요굳이비유하면 일본판 영화아저씨랑 정도..전혀 다르지만요 어떤분은 이걸보고 게이가됬다는 평가를 남겼습니다재밌게 보실수있다고 생각하고 이만 적겠습니다.​\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=3&dirId=30302&docId=331326506&qb=6rKo7Jq47JmV6rWt6rO8IOqwmeydgCDslaDri4jrqZTsnbTshZgg7JiB7ZmUIO2VmOuCmCDstpTsspztlbTso7zshLjsmpQ=&enc=utf8§ion=kin&rank=4&search_sort=0&spq=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********질문 제목\n",
      "로맨스 애니영화 추천!\n",
      "\n",
      "\n",
      "**********질문 내용\n",
      "너의 췌장을 먹고싶어, 너의 이름은 같은 로맨스물 애니영화 추천부탁드립니다. 되도록 답변자님이 높게 평가하시는 영화부터 순서대로 알려주세요!!\n",
      "\n",
      "\n",
      "++++++++++답변자++++++++++\n",
      "영화학생 님 답변\n",
      "++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "----------답변 내용----------\n",
      "시간을 달리는 소녀   감독 호소다 마모루 출연 나카 리이사, 이시다 타쿠야 개봉                  초속5센티미터   감독 신카이 마코토 출연 미즈하시 켄지, 하나무라 사토미, 오노우에 아야카 개봉                  언어의 정원   감독 신카이 마코토 출연 이리노 미유, 하나자와 카나 개봉 2013. 08. 14. / 2016. 07. 07. 재개봉                 목소리의 형태   감독 야마다 나오코 출연 이리노 미유, 하야미 사오리 개봉 2017. 05. 09.                 마음이 외치고 싶어해   감독 나가이 타츠유키 출연 미나세 이노리, 우치야마 코우키, 아마미야 소라, 호소야 요시마사, 후지와라 케이지, 요시다 요 개봉 2016. 03. 30.                 늑대아이   감독 호소다 마모루 출연 미야자키 아오이, 오오사와 타카오, 쿠로키 하루, 니시이 유키토, 오노 모모카 개봉 2012. 07. 21. / 2012. 09. 13. 재개봉                 귀를 기울이면   감독 콘도 요시후미 출연 혼나 요코, 타카하시 잇세이 개봉 2007. 11. 22.                 바다가 들린다   감독 모치즈키 토모미 출연 토비타 노부오, 세키 토시히코, 사카모토 요코 개봉 미개봉                 후세: 말하지 못한 내 사랑   감독 미야지 마사유키 출연 코토부키 미나코, 미야노 마모루, 코니시 카츠유키 개봉 2013. 03. 28.                 타마코 러브 스토리   감독 야마다 나오코 출연 스자키 아야, 타마루 아츠시, 카네코 유우키 개봉 2014. 09. 25.           일본 애니메이션 추천 드립니다!​<너의 이름은.>과 동급에 놓을 만한 영화는 <시간을 달리는 소녀><초속 5센티미터><언어의 정원>이 있겠습니다. <너의 이름은.> 보고 많이 앓았다가 이 영화 보면 나을까했는데 더 앓았어요;;​그만큼 여파가 있고, 여운도 많다는 걸 의미하겠죠.​다른 영화들도 로맨스 애니로 보기에 충분히 괜찮습니다. 꼭 위에서부터 1~10등 순은 아니지만, 그래도 하나씩 챙겨보시면 좋을 만한 작품들 골라봤습니다.^^​​아울러, 왓챠플레이에서 일본 애니메이션 바로 감상할 수 있으니 참고해주세요~​무료(2주) 체험 기간 이용 후, 매월 저렴하게 영화 감상하는 좋은 혜택 챙기시면 좋겠습니다.​<너의 이름은.><너의 췌장을 먹고 싶어> 검색해서 왓챠 추천 엔진으로 비슷한 작품도 추천 받고,6만 편의 영화를 무제한 감상하셔도 좋을 것 같네요.^^​http://wcha.it/2iZ2S2y                         ​\n",
      "-----------------------------\n",
      "==============================링크==============================\n",
      "https://kin.naver.com/qna/detail.nhn?d1id=3&dirId=3021101&docId=330619496&qb=6rKo7Jq47JmV6rWt6rO8IOqwmeydgCDslaDri4jrqZTsnbTshZgg7JiB7ZmUIO2VmOuCmCDstpTsspztlbTso7zshLjsmpQ=&enc=utf8§ion=kin&rank=5&search_sort=0&spq=1\n",
      "**********질문 제목\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select_one'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-eab3fbc06324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mcontent_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqna_questions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div.c-heading > div.c-heading__content\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontent_q\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select_one'"
     ]
    }
   ],
   "source": [
    "# 지식인 검색\n",
    "site = \"https://kin.naver.com/search/list.nhn?\"\n",
    "tag = input(\"검색어를 입력해주십시오-->\")\n",
    "num = input(\"검색할 페이지 번호를 입력하십시오-->\")\n",
    "temp_url = site + \"sort=none&query=\" + tag \n",
    "temp_url_encode=parse.urlparse(temp_url)\n",
    "query = parse.parse_qs(temp_url_encode.query)\n",
    "query_encode = parse.urlencode(query, doseq=True)\n",
    "\n",
    "url = site+query_encode+\"&section=kin&page=\"+ num\n",
    "\n",
    "#\n",
    "\n",
    "page = urlopen(url) \n",
    "document = page.read()\n",
    "soup = BeautifulSoup(document.decode(\"utf-8\"), \"html.parser\")\n",
    "\n",
    "temp_qna = soup.find(class_=\"basic1\")\n",
    "qna_list = temp_qna.select(\"li > dl > dt > a\")\n",
    "\n",
    "for qna in qna_list:\n",
    "    \n",
    "    # 링크 생성\n",
    "    qna_url = qna.attrs[\"href\"]\n",
    "    print(\"=\"*30+\"링크\"+\"=\"*30)\n",
    "    print(qna_url)\n",
    "    \n",
    "    # 링크 읽기\n",
    "    qna_page = urlopen(qna_url.encode(\"ascii\",\"ignore\").decode(\"ascii\",\"ignore\")) \n",
    "    qna_document = qna_page.read()\n",
    "    \n",
    "    # 객체 생성\n",
    "    soup_qna = BeautifulSoup(qna_document.decode(\"utf-8\"), \"html.parser\")\n",
    "    \n",
    "    # 질문\n",
    "    print(\"*\"*10 + \"질문 제목\")\n",
    "    qna_questions = soup_qna.find(class_=\"question-content__inner\")\n",
    "    \n",
    "    if qna_questions == None:\n",
    "        pass\n",
    "    else:\n",
    "        title_q = qna_questions.select_one(\"div.c-heading > div.c-heading__title > div.c-heading__title-inner > div.title\").text\n",
    "        print(title_q.strip())\n",
    "    \n",
    "    content_q = qna_questions.select_one(\"div.c-heading > div.c-heading__content\")\n",
    "    if content_q == None:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"\\n\")\n",
    "        print(\"*\"*10 + \"질문 내용\")\n",
    "        print(content_q.text.strip())\n",
    "        \n",
    "    # 답변\n",
    "    qna_answers = soup_qna.find(class_=\"answer-content__inner\")\n",
    "    \n",
    "    title_q_list = qna_answers.select(\"div.c-heading-answer__title > p\")\n",
    "    content_q_list = qna_answers.select(\"div._endContentsText\")\n",
    "    \n",
    "    if title_q == None:\n",
    "        print(\"답변이 없습니다\")\n",
    "    else:\n",
    "        for title_q in title_q_list:\n",
    "            for content_q in content_q_list:\n",
    "                print(\"\\n\")\n",
    "                print(\"+\"*10 + \"답변자\" + \"+\"*10)\n",
    "                print(title_q.text.strip())\n",
    "                print(\"+\"*26)\n",
    "                print(\"\\n\")\n",
    "                print(\"-\"*10 + \"답변 내용\" + \"-\"*10)\n",
    "                print(content_q.text.strip())\n",
    "                print(\"-\"*29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 -> 수치화(DTM, Word2Vec 등)\n",
    "#문서간 단어들의 차이를 계산? 유클리드, 코사인 유사도 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "0.6666666666666667\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# 문서1     1         1         0         1\n",
    "# 문서2     1         0         1         1\n",
    "# 문서3     2         0         2         2\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(x,y):\n",
    "     return np.dot(x,y)/(norm(x)*norm(y))\n",
    "    \n",
    "doc1=np.array([1,1,0,1])\n",
    "doc2=np.array([1,0,1,1])\n",
    "doc3=np.array([2,0,2,2])\n",
    "\n",
    "print(cos_sim(doc1, doc2))\n",
    "print(cos_sim(doc1, doc3))\n",
    "print(cos_sim(doc3, doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:/datasets/the-movies-dataset/movies_metadata.csv\")\n",
    "data = data.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 28, 'name...</td>\n",
       "      <td>http://www.lordreetlamorale-lefilm.com/#/nav/t...</td>\n",
       "      <td>76609</td>\n",
       "      <td>tt1242521</td>\n",
       "      <td>fr</td>\n",
       "      <td>L'Ordre et la Morale</td>\n",
       "      <td>Dissidents in a French colony attack a police ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-11-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>[{'iso_639_1': 'fr', 'name': 'Français'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rebellion</td>\n",
       "      <td>False</td>\n",
       "      <td>6.3</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8941</td>\n",
       "      <td>tt1132607</td>\n",
       "      <td>fr</td>\n",
       "      <td>Versailles</td>\n",
       "      <td>A young mother Nina and her son Enzo find them...</td>\n",
       "      <td>...</td>\n",
       "      <td>2008-05-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>[{'iso_639_1': 'fr', 'name': 'Français'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Versailles</td>\n",
       "      <td>False</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 99, 'name': 'Documentary'}, {'id': 107...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64074</td>\n",
       "      <td>tt1528224</td>\n",
       "      <td>en</td>\n",
       "      <td>Two in the Wave</td>\n",
       "      <td>An in-depth analysis of the relationship betwe...</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-05-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>[{'iso_639_1': 'fr', 'name': 'Français'}, {'is...</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two in the Wave</td>\n",
       "      <td>False</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197723</td>\n",
       "      <td>tt0471239</td>\n",
       "      <td>en</td>\n",
       "      <td>Lotte Reiniger: Homage to the Inventor of the ...</td>\n",
       "      <td>Follows the life and work of animator Lotte Re...</td>\n",
       "      <td>...</td>\n",
       "      <td>2001-11-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lotte Reiniger: Homage to the Inventor of the ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269797</td>\n",
       "      <td>tt0492337</td>\n",
       "      <td>en</td>\n",
       "      <td>RKO Production 601: The Making of 'Kong, the E...</td>\n",
       "      <td>An in-depth look at the genesis, production, a...</td>\n",
       "      <td>...</td>\n",
       "      <td>2005-10-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RKO Production 601: The Making of 'Kong, the E...</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adult belongs_to_collection budget  \\\n",
       "19995  False                   NaN      0   \n",
       "19996  False                   NaN      0   \n",
       "19997  False                   NaN      0   \n",
       "19998  False                   NaN      0   \n",
       "19999  False                   NaN      0   \n",
       "\n",
       "                                                  genres  \\\n",
       "19995  [{'id': 18, 'name': 'Drama'}, {'id': 28, 'name...   \n",
       "19996                      [{'id': 18, 'name': 'Drama'}]   \n",
       "19997  [{'id': 99, 'name': 'Documentary'}, {'id': 107...   \n",
       "19998                                                 []   \n",
       "19999                                                 []   \n",
       "\n",
       "                                                homepage      id    imdb_id  \\\n",
       "19995  http://www.lordreetlamorale-lefilm.com/#/nav/t...   76609  tt1242521   \n",
       "19996                                                NaN    8941  tt1132607   \n",
       "19997                                                NaN   64074  tt1528224   \n",
       "19998                                                NaN  197723  tt0471239   \n",
       "19999                                                NaN  269797  tt0492337   \n",
       "\n",
       "      original_language                                     original_title  \\\n",
       "19995                fr                               L'Ordre et la Morale   \n",
       "19996                fr                                         Versailles   \n",
       "19997                en                                    Two in the Wave   \n",
       "19998                en  Lotte Reiniger: Homage to the Inventor of the ...   \n",
       "19999                en  RKO Production 601: The Making of 'Kong, the E...   \n",
       "\n",
       "                                                overview  ... release_date  \\\n",
       "19995  Dissidents in a French colony attack a police ...  ...   2011-11-16   \n",
       "19996  A young mother Nina and her son Enzo find them...  ...   2008-05-19   \n",
       "19997  An in-depth analysis of the relationship betwe...  ...   2010-05-19   \n",
       "19998  Follows the life and work of animator Lotte Re...  ...   2001-11-19   \n",
       "19999  An in-depth look at the genesis, production, a...  ...   2005-10-23   \n",
       "\n",
       "      revenue runtime                                   spoken_languages  \\\n",
       "19995     0.0   136.0          [{'iso_639_1': 'fr', 'name': 'Français'}]   \n",
       "19996     0.0   113.0          [{'iso_639_1': 'fr', 'name': 'Français'}]   \n",
       "19997     0.0    91.0  [{'iso_639_1': 'fr', 'name': 'Français'}, {'is...   \n",
       "19998     0.0     0.0                                                 []   \n",
       "19999     0.0     0.0                                                 []   \n",
       "\n",
       "         status  tagline                                              title  \\\n",
       "19995  Released      NaN                                          Rebellion   \n",
       "19996  Released      NaN                                         Versailles   \n",
       "19997  Released      NaN                                    Two in the Wave   \n",
       "19998  Released      NaN  Lotte Reiniger: Homage to the Inventor of the ...   \n",
       "19999  Released      NaN  RKO Production 601: The Making of 'Kong, the E...   \n",
       "\n",
       "       video vote_average vote_count  \n",
       "19995  False          6.3       24.0  \n",
       "19996  False          6.0        4.0  \n",
       "19997  False          5.5        5.0  \n",
       "19998  False          0.0        0.0  \n",
       "19999  False          8.0        1.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()['overview']\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    20000\n",
       "Name: overview, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#단순 코사인 유사도 기반 계산(연습)\n",
    "tfidf=TfidfVectorizer(stop_words='english')\n",
    "tfidf\n",
    "#data['overview'].isnull().value_counts()\n",
    "data['overview']=data['overview'].fillna('') \n",
    "#NaN의 경우는 ''로 (tfidf 작업시 NaN있으면 에러가 발생)\n",
    "data['overview'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 47487)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat=tfidf.fit_transform(data['overview'])\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cos_sim = linear_kernel(tfidf_mat, tfidf_mat) # 여기서 너무 많이하면 다운됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01575748, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01575748, 1.        , 0.04907345, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.04907345, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.08375766],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.08375766, 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=20000, step=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "Toy Story                      0\n",
       "Jumanji                        1\n",
       "Grumpier Old Men               2\n",
       "Waiting to Exhale              3\n",
       "Father of the Bride Part II    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12356"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx['Rambo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "def get_recommendations(title,cos_sim=cos_sim,idx=idx):\n",
    "    idx_title = idx[title]\n",
    "    sim_score = list(enumerate(cos_sim[idx_title]))\n",
    "    sim_score = sorted(sim_score, key=lambda x:x[1], reverse=True)\n",
    "#     print(sim_score)\n",
    "    top10 = sim_score[1:11]\n",
    "    resList = []\n",
    "    for titleidx, sim in top10:\n",
    "        resList.append(idx.index[titleidx])\n",
    "    # 이렇게 하면 몇번째 요소를 기준으로 sort할 것인지 알 수 있다!\n",
    "    # 답을 list에 담아놓고 그걸 아예 Series나 df에 인수로 던져버릴 수도 있다!\n",
    "#     sim_score = sorted(sim_score,reverse=True)\n",
    "#     top10 = sim_score[1:11]\n",
    "#     print(top10)\n",
    "#     [for i in sim_score]\n",
    "    return idx[resList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "First Blood                         2290\n",
       "Rambo III                           2291\n",
       "Rambo: First Blood Part II          2289\n",
       "Universal Soldier                   2693\n",
       "Billy Jack                          3832\n",
       "Strawberries in the Supermarket     9632\n",
       "Apocalypse Now                      1165\n",
       "Journey from the Fall              12252\n",
       "Little Soldier                     16638\n",
       "The Search                         10030\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Rambo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                                                               'Toy Story',\n",
       "                                                                        'Jumanji',\n",
       "                                                               'Grumpier Old Men',\n",
       "                                                              'Waiting to Exhale',\n",
       "                                                    'Father of the Bride Part II',\n",
       "                                                                           'Heat',\n",
       "                                                                        'Sabrina',\n",
       "                                                                   'Tom and Huck',\n",
       "                                                                   'Sudden Death',\n",
       "                                                                      'GoldenEye',\n",
       "       ...\n",
       "                                                            'Never Say... Never!',\n",
       "                                                                         'Calmos',\n",
       "                                                    'How to Make Love to a Woman',\n",
       "                                                             'After Fall, Winter',\n",
       "                                                         'Violeta Went to Heaven',\n",
       "                                                                      'Rebellion',\n",
       "                                                                     'Versailles',\n",
       "                                                                'Two in the Wave',\n",
       "                  'Lotte Reiniger: Homage to the Inventor of the Silhouette Film',\n",
       "       'RKO Production 601: The Making of 'Kong, the Eighth Wonder of the World''],\n",
       "      dtype='object', name='title', length=20000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.index\n",
    "# Series의 index 참조법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "def get_recommendations(title,cos_sim=cos_sim):\n",
    "    idx_title = idx[title]\n",
    "    sim_score = list(enumerate(cos_sim[idx_title]))\n",
    "    sim_score = sorted(sim_score, key=lambda x:x[1], reverse=True)\n",
    "#     print(sim_score)\n",
    "    mi = sim_score[1:11]\n",
    "    print(mi) # mi는 enum이니까!\n",
    "    res = [i[0] for i in mi]\n",
    "    print(data['title'].iloc[res]) # 여기에서 다시 그냥 dataframe에서 참조하는게 나을 수도 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2290, 0.270096538982602), (2291, 0.25905150988472103), (2289, 0.22903386751019214), (2693, 0.12866032401646968), (3832, 0.1188555373851827), (9632, 0.11721259693472731), (1165, 0.11268056475960117), (12252, 0.10666761645648196), (16638, 0.105670102882229), (10030, 0.105591993807548)]\n",
      "2290                         First Blood\n",
      "2291                           Rambo III\n",
      "2289          Rambo: First Blood Part II\n",
      "2693                   Universal Soldier\n",
      "3832                          Billy Jack\n",
      "9632     Strawberries in the Supermarket\n",
      "1165                      Apocalypse Now\n",
      "12252              Journey from the Fall\n",
      "16638                     Little Soldier\n",
      "10030                         The Search\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "get_recommendations('Rambo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span>2,000</span>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# 로그인 후의 정보에 대해서 가져와보자!\n",
    "login_info = {'m_id':'gtpgg1013',\n",
    "              'm_passwd': 'password1234'}\n",
    "# 서버 안에서만 돌아가는 일종의 프로그램\n",
    "url_login = \"http://www.hanbit.co.kr/member/login_proc.php\"\n",
    "# login_proc이라는 php파일에 login 정보를 던져줘야 내가 누군지 사이트에서 알 수 있다\n",
    "# session 객체 생성\n",
    "session = requests.session()\n",
    "res = session.post(url_login, data=login_info)\n",
    "# res\n",
    "url_mypage = \"http://www.hanbit.co.kr/myhanbit/myhanbit.html\"\n",
    "res = session.get(url_mypage)\n",
    "res\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'html.parser') # 추출 해 보자~\n",
    "mi = soup.select_one(\"#container > div > div.sm_mymileage > dl.mileage_section1 > dd > span\")\n",
    "print(mi)\n",
    "# 마일리지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#archive.ics.uci.edu/ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
