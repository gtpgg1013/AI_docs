mytext.stem<-mystemmer.func(text)
mytext.stem
table(str_split(text," "))
table(str_split(mytext.stem," "))
mystemmer.func<-function(mytextobj){
mytext<-str_replace_all(mytextobj,"(\\bam ) | (\\bare ) | (\\bis ) | (\\bwas ) | (\\bwere ) | (\\bbe )","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
mytext.stem<-mystemmer.func(text)
mytext.stem
function(mytextobj){
mytext_<-str_replace_all(mytextobj,"(\\bam ) | (\\bare ) | (\\bis ) | (\\bwas ) | (\\bwere ) | (\\bbe )","be ")
return(mytext_)
}
mytext.stem<-function(mytextobj){
mytext_<-str_replace_all(mytextobj,"(\\bam ) | (\\bare ) | (\\bis ) | (\\bwas ) | (\\bwere ) | (\\bbe )","be ")
return(mytext_)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
mytext.stem<-mystemmer.func(text)
mytext.stem
mytext.stem<-mytext.stem(text)
mytext.stem<-function(mytextobj){
mytext_<-str_replace_all(mytextobj,"(\\bam ) | (\\bare ) | (\\bis ) | (\\bwas ) | (\\bwere ) | (\\bbe )","be ")
return(mytext_)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
mytext.stem<-mytext.stem(text)
mytext.stem
table(str_split(text," "))
table(str_split(mytext.stem," "))
library(stringr)
library(tm)
mytext.stem<-function(mytextobj){
mytext_<-str_replace_all(mytextobj,"(\\bam ) | (\\bare ) | (\\bis ) | (\\bwas ) | (\\bwere ) | (\\bbe )","be ")
return(mytext_)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
mytext.stem<-mytext.stem(text)
mytext.stem
table(str_split(text," "))
table(str_split(mytext.stem," "))
mytext.stem<-function(mytext){
mytext<-str_replace_all(mytext,"(\\bam) | (\\bare ) | (\\bis ) | (\\bwas ) | (\\bwere ) | (\\bbe )","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
mytext.stem<-mytext.stem(text)
mytext.stem
table(str_split(text," "))
table(str_split(mytext.stem," "))
mytextfunc<-function(mytext){
mytext<-str_replace_all(mytext,"(\\bam) | (\\bare ) | (\\bis ) | (\\bwas ) | (\\bwere ) | (\\bbe )","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
mytext.stem<-mytextfunc(text)
mytext.stem
mytext<-str_replace_all(mytext,"(\\bam) | (\\bare) | (\\bis) | (\\bwas) | (\\bwere) | (\\bbe)","be ")
mytextfunc<-function(mytext){
mytext<-str_replace_all(mytext,"(\\bam) | (\\bare) | (\\bis) | (\\bwas) | (\\bwere) | (\\bbe)","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
mytext.stem<-mytextfunc(text)
mytext.stem
table(str_split(text," "))
text<-c("I am a boy. You are a boy. He might be a boy.")
text
mytextfunc<-function(mytext){
mytext<-str_replace_all(mytext,"(am) | (are) | (is) | (\\bwas) | (\\bwere) | (\\bbe)","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
text
mytext.stem<-mytextfunc(text)
mytext.stem
mytextfunc<-function(mytext){
mytext<-str_replace_all(mytext,"(am) | ( are) | (is) | (\\bwas) | (\\bwere) | (\\bbe)","be ")
return(mytext)
}
mytext.stem<-mytextfunc(text)
mytext.stem
text<-c("I am a boy. You are a boy. He might be a boy.")
text
mytext.stem<-mytextfunc(text)
mytext.stem
mytextfunc<-function(mytext){
mytext<-str_replace_all(mytext,"(am) | (are) | (is) | (\\bwas) | (\\bwere) | (\\bbe)","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
text
mytext.stem<-mytextfunc(text)
mytext.stem
str_extract_all(mytext,boundary("sentence"))
mytext<-"The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
str_extract_all(mytext,boundary("sentence"))
mytextfunc<-function(text){
mytext<-str_replace_all(text,"(\\bam)|(\\bare)|(\\bis)|(\\bwas)|(\\bwere)|(\\bbe)","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
text
mytext.stem<-mytextfunc(text)
mytext.stem
mytextfunc<-function(text){
mytext<-str_replace_all(text,"(\\bam )|(\\bare )|(\\bis )|(\\bwas )|(\\bwere )|(\\bbe )","be ")
return(mytext)
}
text<-c("I am a boy. You are a boy. He might be a boy.")
text
mytext.stem<-mytextfunc(text)
mytext.stem
table(str_split(text," "))
table(str_split(mytext.stem," "))
mytext<-"The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
str_extract_all(mytext,boundary("sentence"))
str_extract_all(mytext,boundary("word"))
myword<-unlist(str_extract_all(mytext,boundary("word")))
myword
table(myword)
length(table(myword)) #서로 다른 단어들의 개수
sum(table(myword)) #수치들의 합합
sum(table(myword)) #수치들의 합
str_replace_all(mytext,"\\bUnited States","United_States")
str_replace_all(mytext,"United States","United_States")
str_replace_all(mytext,"^United States","United_States")
mytext.2gram<-str_replace_all(mytext,"\\bUnited States","United_States")
#str_replace_all(mytext,"\\bUnited States","United_States")
str_extract_all(mytext.2gram, boundary("word"))
#str_replace_all(mytext,"\\bUnited States","United_States")
myword2<-unlist(str_extract_all(mytext.2gram, boundary("word")))
table(myword2)
length(table(myword2))
#연습
#1. 2단어씩 연결하여 출력
mywords<-unlist(str_extract_all(mytext,boundary("word")))
mywords
table(mywords)
length(table(mywords))
length(mywords)
for(i in 1:25){
print(mywords[i]+mywords[i+1])
}
for(i in 1:25){
print(mywords[i])
}
for(i in 1:25){
print(mywords[i],mywords[i+1])
}
for(i in 1:25){
print(mywords[i])
print(mywords[i+1])
}
for(i in 1:25){
word<-mywords[i] + mywords[i+1]
print(word)
}
#2.
library(tm)
my.text.location<-"Data/papers/"
#txt 파일 전체 읽은 다음 말뭉치 만들어야 함
VCorpus(DirSource(my.text.location)) #휘발성 말뭉치 만듬 : 소스는 Dirsource 함수로 주면 됨!
#txt 파일 전체 읽은 다음 말뭉치 만들어야 함
txt<-VCorpus(DirSource(my.text.location)) #휘발성 말뭉치 만듬 : 소스는 Dirsource 함수로 주면 됨!
txt
#txt 파일 전체 읽은 다음 말뭉치 만들어야 함
mypaper<-VCorpus(DirSource(my.text.location)) #휘발성 말뭉치 만듬 : 소스는 Dirsource 함수로 주면 됨!
mypaper
summary(mypaper)
class(mypaper)
mypaper[[1]]
mypaper[[2]]
mypaper[[2]]$content
mypaper[[2]]$meta
mypaper[[2]]$content
mypaper[[1]]$content #뜻하는게 무엇인가?
mypaper[[2]]$content
mypaper[[2]]$meta
meta(mypaper[[2]], tag='author')<-"김저자"
mypaper[[2]]$meta
mypuncts<-lapply(mypaper, myfunc)
#알파벳+숫자 두개 다 쓸수 있는 alnum / 특수문자 punct
myfunc<-function() {
str_extract_all(x,"[[:alnum:]]{1,}[[:punct:]]{1,}[[:alnum:]]{1,}")
}
mypuncts<-lapply(mypaper, myfunc)
#알파벳+숫자 두개 다 쓸수 있는 alnum / 특수문자 punct
myfunc<-function(x) {
str_extract_all(x,"[[:alnum:]]{1,}[[:punct:]]{1,}[[:alnum:]]{1,}")
}
mypuncts<-lapply(mypaper, myfunc)
mypuncts
unlist(mypuncts)
table(unlist(mypuncts))
as.data.frame(table(unlist(mypuncts)))
library(dplyr)
as.data.frame(table(unlist(mypuncts))) %>% arrange(desc(Freq))
mydigits<-lapply(mypaper,mydigitfunc)
#수치로 된 자료를 추출
mydigitfunc<-function(x){
str_extract_all(x,"[[:digit:]]{1,}")
}
mydigits<-lapply(mypaper,mydigitfunc)
mydigits
unlist(mydigits)
as.data.frame(table(unlist(mydigits))) %>% arrange(desc(Freq))
#대문자로 시작하는 단어 추출
myupperfunc<-function(x){
str_extract_all(x,"[[:upper:]]{1}[^ ]")
}
myuppers<-lapply(mypaper,myupperfunc)
myuppers
#대문자로 시작하는 단어 추출
myupperfunc<-function(x){
str_extract_all(x,"[[:upper:]]{1}.*")
}
myuppers<-lapply(mypaper,myupperfunc)
myuppers
str_extract_all(x,"[[:upper:]]{1}[[:alnum:]]{1,}")
#대문자로 시작하는 단어 추출
myupperfunc<-function(x){
str_extract_all(x,"[[:upper:]]{1}[[:alnum:]]{1,}")
}
myuppers<-lapply(mypaper,myupperfunc)
myuppers
#이런 모든걸 해주는 함수 : tm_map()
tm_map(mypaper, removeNumbers)
#이런 모든걸 해주는 함수 : tm_map()
mycorpus<-tm_map(mypaper, removeNumbers) #패키지가 tm인거 쓰면 됨
mycorpus
mycorpus[[1]]$content
removePunctuation("hlelsfldnl22---,,,,sfdlsnhd@$!$")
install.packages("SnowballC")
library(SnowballC)
#어근을 얻을수 있는 패키지
wordStem(c("learn","learns","learning","learned"))
wordStem(c("bought"))
wordStem(c("bought"))
wordStem(c("brought"))
wordStem(c("runed"))
#stemDocument 보면 어근추출 알고리즘 내용 볼 수 있음
#어근추출
cleaned<-tm_map(mypaper,stemDocument)
cleaned
cleaned[[1]]
cleaned[[1]]$content
str_extract_all(cleaned,"[[:alnum:]]{1,}")
myfunc<-function(x){
str_extract_all(x,"[[:upper:]]{1}[[:alnum:]]{1,}")
}
mywords<-lapply(mypaper,myfunc)
unlist(mywords)
mywords
myfunc<-function(x){
str_extract_all(x,"[[:alnum:]]{1,}")
}
mywords<-lapply(mypaper,myfunc)
unlist(mywords)
mywords
sapply(mypaper,myfunc)
class(sapply(mypaper,myfunc))
#
mytempfunc<-function(myobj,oldexp,newexp){
newobject<-tm_map(myobject,content_transformer(function(x,pattern)
gsub(pattern,newexp,x)),oldexp)
print(newobject)
}
#필요없는 공란을 없애줌
mycorpust<-tm_map(mycorpus,stripWhitespace)
mycorpust
mycorpust[[1]]
mycorpust[[1]]$content
#만약 tolower(일반 벡터에 쓰이는 함수)를 corpus객체에 쓰고싶으면 content_transformer함수로 변환필요
mycorpust<-tm_map(mycorpus,content_transformer(tolower))
mycorpust[[1]]$content
#불용어 사전 적용 -> 삭제
tm_map(mycorpus,removeWords,words=stopwords("SMART"))
#불용어 사전 적용 -> 삭제
mycorpust<-tm_map(mycorpus,removeWords,words=stopwords("SMART"))
mycorpust[[1]]$content
#어근 동일화
tm_map(mycorpus, stemDocument,language="en")
#어근 동일화
mycorpust<-tm_map(mycorpus, stemDocument,language="en")
mycorpust[[1]]$content
strsplit(mytext," ")
str_split(mytext, ' ')
strsplit(mytext," ")
class(sapply(str_split(mytext, ' '), length)) #결과는 vector로 나온다
#문서*단어 행렬
#DTM
DocumentTermMatrix(mycorpus)
#TDM
TermDocumentMatrix(mycorpus)
#문서*단어 행렬
#DTM : row:D / col:T
dtm.e<-DocumentTermMatrix(mycorpus)
dtm.e
#불용어 사전 적용 -> 삭제
mycorpust<-tm_map(mycorpus,removeWords,words=stopwords("SMART"))
mycorpust
inspect(dtm.e[1:3,50:60])
#Titanic에서 호칭 추출해서 factor로 분류
View("/Data/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
View("/Data/titanitc/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
View("/Data/titanic/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
View("Data/titanic/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
view("Data/titanic/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
View("Data/titanic/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
View("titanic/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
train<-read.csv("/Data/titanic/train.csv")
#Titanic에서 호칭 추출해서 factor로 분류
train<-read.csv("Data/titanic/train.csv")
View(train)
train
str(train)
train$Name
myaliasfunc<-function(x){
str_extract_all(x,"//.[[:alpha:]]")
}
myalias<-lapply(train$Name,myaliasfunc)
myalias
myaliasfunc(train$Name)
str_extract_all(x,"[[:alpha:]]//.")
myaliasfunc<-function(x){
str_extract_all(x,"[[:alpha:]]//.")
}
myaliasfunc(train$Name)
myaliasfunc<-function(x){
str_extract_all(x,"[[:alpha:]]")
}
myaliasfunc(train$Name)
myaliasfunc<-function(x){
str_extract_all(x,"[[:alpha:]]{1,}//.")
}
myaliasfunc(train$Name)
myaliasfunc<-function(x){
str_extract_all(x,"[[:alpha:]]{1,}")
}
myaliasfunc(train$Name)
myaliasfunc<-function(x){
str_extract_all(x,"[[:alpha:]]{1,}//.")
}
myaliasfunc(train$Name)
str_extract_all(x,"[[:alpha:]]{1,}\\.")
myaliasfunc<-function(x){
str_extract_all(x,"[[:alpha:]]{1,}\\.")
}
myaliasfunc(train$Name)
as.data.frame(myaliasfunc(train$Name))
as.factor(myaliasfunc(train$Name))
unlist(myaliasfunc(train$Name))
as.factor(unlist(myaliasfunc(train$Name)))
a<-as.factor(unlist(myaliasfunc(train$Name)))
train1<-data.frame(train,alias=a)
a
str(train)
train1
train1<-data.frame(train,alias=a)
a
train$Name
a
myfunc<-function(x){
str_extract_all(x,".\\..\\..")
}
as.factor(unlist(myfunc(train$Name)))
myfunc<-function(x){
str_extract_all(x,".")
}
as.factor(unlist(myfunc(train$Name)))
myfunc<-function(x){
str_extract_all(x,"\\.{2}")
}
as.factor(unlist(myfunc(train$Name)))
str_extract_all(x,"\\.")
myfunc<-function(x){
str_extract_all(x,"\\.")
}
as.factor(unlist(myfunc(train$Name)))
myfunc<-function(x){
str_extract_all(x,"\\.")
}
as.factor(unlist(myfunc(train$Name)))
str(as.factor(unlist(myfunc(train$Name))))
myfunc<-function(x){
str_extract_all(x,"[[:alpha:]]\\.")
}
as.factor(unlist(myfunc(train$Name)))
myfunc<-function(x){
str_extract_all(x,"[[:alpha:]]{1,}\\.")
}
as.factor(unlist(myfunc(train$Name)))
table(a)
df<-data.frame(table(a))
df
df %>% arrange(desc(Freq))
df<-df %>% arrange(desc(Freq)) %>% head(5)
df
library(ggplot2)
ggplot(df)
ggplot(data=df,aes(x=a,y=Freq)) + geom_col()
ggplot(data=df,aes(x=a,y=Freq)) + geom_col() +coord_flip()
df<-rename(df,alias=a,freq=Freq)
ggplot(data=df,aes(x=alias,y=freq)) + geom_col() +coord_flip()
mynyt<-VCorpus(DirSource(my.text.location2))
my.text.location2<-"/Data/nyt"
mynyt<-VCorpus(DirSource(my.text.location2))
mynyt<-VCorpus(DirSource(my.text.location2))
mynyt<-VCorpus(DirSource(my.text.location2))
my.text.location2<-"/Data/nyt/"
mynyt<-VCorpus(DirSource(my.text.location2))
my.text.location2<-"/Data/nyt/"
mynyt<-VCorpus(DirSource(my.text.location2))
my.text.location2<-"Data/nyt/"
mynyt<-VCorpus(DirSource(my.text.location2))
mynyt
my.text.location2<-"Data/nyt/"
mynyt<-VCorpus(DirSource(my.text.location2))
mynyt
mynyt[[1]]$content
tm_map(mynyt, removeNumbers)
tm_map(mynyt, removePunctuation)
tm_map(mynyt, removeSparseTerms)
tm_map(mynyt, removeWords, word=stopwords("SMART"))
tm_map(mynyt, stemDocument, language="english")
tm_map(mynyt, stripWhitespace())
tm_map(mynyt, content_transformer(tolower))
tm_map(mynyt, stripWhitespace
#n-gram 도출
install.packages("RWeka")
source('C:/Users/user/Desktop/lecture_dir/rwork/20190527.R', echo=TRUE)
library(RWeka)
mytext<-c("The United States comprises fifty states.", "In the United States, each state has its own laws.","However, federal law overrides state law in the United States.")
mytext
mytemp<-VCorpus(VectorSource(mytext))
libraray(tm)
librray(tm)
library(tm)
mytemp<-VCorpus(VectorSource(mytext))
#단어문서행렬
ngram.tdm<-TermDocumentMatrix(mytemp)
ngram.tdm
inspect(ngram.tdm)
ngram.tdm
bigramTokenizer<-function(x){
NGramTokenizer(x, Weka_control(min=2,max=3))
}
TermDocumentMatrix(mytemp, control=list(tokenize=bigramTokenizer))
mybi<-TermDocumentMatrix(mytemp, control=list(tokenize=bigramTokenizer))
inspect(mybi)
class(inspect(mybi))
#control이라는
ngram.tdm<-TermDocumentMatrix(mytemp, control=list(tokenize=bigramTokenizer))
bigramTokenizer<-function(x){
NGramTokenizer(x, Weka_control(min=2,max=2))
}
#control이라는
ngram.tdm<-TermDocumentMatrix(mytemp, control=list(tokenize=bigramTokenizer))
inspect(ngram.tdm)
str(ngram.tdm)
ngram.tdm$dimnames$Terms
ngram.tdm$dimnames$Docs
bigramTokenizer<-function(x){
NGramTokenizer(x, Weka_control(min=2,max=3))
}
#control이라는
ngram.tdm<-TermDocumentMatrix(mytemp, control=list(tokenize=bigramTokenizer))
inspect(ngram.tdm)
str(ngram.tdm)
ngram.tdm$dimnames$Terms
apply(ngram.tdm)
ngram.tdm[,]
ngram.tdm[1,]
apply(ngram.tdm[,], 1, sum)
class(apply(ngram.tdm[,], 1, sum))
#2:열단위 연산(문서)
apply(ngram.tdm[,], 2, sum)
#matix도 활용가능
#1:행단위 연산(단어) : 전체 문서에서 해당 단어가 몇번 등장?
df<-apply(ngram.tdm[,], 1, sum)
df
table(df)
str(df)
library(dplyr)
df %>% arrange(desc(names))
#matix도 활용가능
#1:행단위 연산(단어) : 전체 문서에서 해당 단어가 몇번 등장?
bigramlist<-apply(ngram.tdm[,], 1, sum)
sort(bigramlist, decreasing =TRUE )
str(bigramlist)
#한국어 처리
install.packages("KoNLP")
library(KoNLP)
library(KoNLP)
library(stringr)
library(rJava)
library(KoNLP)
#한국어 처리
install.packages("KoNLP")
library(KoNLP)
library(rJava)
library(KoNLP)
