09/13

## Week 1

![curriculum](.\curriculum.jpg)

- 원하면 competition host도 할 수 있다
- real : 훨씬 복잡, 다양
- competition : 데이터 전처리 / 모델링에 국한되어 있음 : 추가로 더 공부해야 하는 경우도 꽤 있긴 하지만!
- Pipeline을 확실히 구축해 기본 Baseline을 만들되, creativity를 놓치지 말라!



- Basic ML model review
  - Linear : separate by 2
  - Tree : divide and conquer : box
  - knn : implementation : closeness
  - Neural Network



- No Free Lunch Theorem

- GB
  - To get a bit closer to the destination, we train a tree to reconstruct the difference between the target function and the current predictions of an ensemble, which is called the **residual**:
  - 새로운 Tree가 앙상블의 error를 최대한 줄이는 쪽으로 학습
  - 잔차 = ideal - 이전까지의 error
  - *R*(**x**)=*f*(**x**)−*D*(**x**)



- Additional Materials and Links
  - https://www.coursera.org/learn/competitive-data-science/supplement/AgAOD/additional-materials-and-links



- Hardware / Software setup



- Numeric features
  - tree based
    - almost similar
  - non tree based
    - scaling impact high : minmax scaling : distribution not changed
    - scaling impact comparabliy lower, but if doing standard scaling, classifier could find difference easier



- Outlier
  - lower / upper bound : removing



- Rank
  - If dont have enough time to preprocess data for knn/linear/neural network model, just change to rank could be the answer : [-12312, 2, 33] => [0,1,2]
  - scipy.stats.rankdata
  - store map for test data / concat train & test => dataset



- Log transform & Rasing to power < 1 : Quite good for NN
  - make clf more collect



- Feature generation : by Prior knowledge / EDA!
  - new feature : from experience / digging data : INSIGHT
  - Adding / Substracting / Multipling / Dividing
  - feature interaction
  - for recognizing patterns : discard first part of price : ex) 2.49 => 0.49, 1.00 => 0.00 : pattern recog!



- Summary



- Categorical & ordinal features

  - label encoding : for tree based model
  - frequency encoding : encoding num is frequency values! : groupby().size() / size()
    -  you can get discriminate which feature is correlated to the target higher or not
  - one hot encoding : for non-tree : knn / NN
    - if ohe to tree-based model : make models power decrease
    - **make sparse matrix is important**
    - => mix two columns : pclass, sex (3,2) => pclass_sex (6) : is better for expressing coefficiecy!
      - Interaction between categorical features!

  - summary:
    - ![summary](.\summary.jpg)

- scaling시 train과 test를 concat해서 사용하면 data leak이 날 수 있다! => 분포가 보여지기 때문에





- Datetime & coordinates (lat/long)
  - datetime
    -  just separate by year, month.... sec : can see the repetititve patterns
    - row-dependent / row-independent
    - if time serise data could be related to the event that occurs special timing, make columns using that special day : ex) holiday : ![holiday](.\holiday.jpg)
    - make features using diference between dates! : churn(이탈) customer : last purchase day - last call day
  - Coordinate
    - 한글 데이터 ㄱㄱ
    - add distance to the important structures (hospital, etc) , extract interesting points on the map
    - find special area in grid (most expensive) / area with special old building
    - center of clusters
    - Aggregated statistics
    - rotate longitude / latitude => make good features for tree-based model : make decision tree's classificaiton line more linear



- Missing Nan
  - Tree 기반은 Nan이나 이상치에 강함! => 없는 값을 존재하지 않는 값 : -999 등의 값으로 replace ok
  - but DNN 모델은 아주 이상하게 작동한다..... => 우리 했을때 이래서 Tree 기반 모델로 했어야 했을까?
  - 또는 row 당 Nan의 갯수를 new feature로 만들 수도 있다.
  - 만약 test에 없는 데이터가 train에서 나온다? => frequency 기준으로 비슷한 놈으로 replace한다



- 관련 서류
  - http://sebastianraschka.com/Articles/2014_about_feature_scaling.html : feature scaling
  -  https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/  : feature enginnering