# 20191021 SK C&C interview Preparing

## 기본적 준비할 것

### 1. MNIST Fashion dataset

ResNet

GoogleNet

ResNet-GoogleNet



### 2. 기존의 틀 깨는 과감한 실행



### 스크립트화는 이정도면 OK, 이제 내일 오가면서 어떻게 바로 에센스화 시킬지 고민하자 : 이니시에이팅 문장 골라내기

### 두 괄 식 !

### 여러가지 주제에 대한 내 생각 정리 !



### 지원 동기

검증 장치에 그냥 걸려들어가는 경우?

꿈이 뭔가요? 왜 하는가?

- 이 질문을 왜 언제? 하나? : 맥락이 긍정인가 부정인가
  - 공통질문 : 도입식의 오픈 질문
  - 지원동기 !
    - 앞에 어떤 질문이 있었나 : 이어지는 내용으로 같이 말해줘야함
    - 내가 따를만한 모델이 있다?
  - 나의 지원동기?
    - 데이터를 분석하고, 그 안에서 인사이트를 찾아내는데서 짜릿함을 느끼기 때문입니다. 지금까지는 경험적으로 얻어진 통찰로 의사 결정을 해왔다면, 세상이 진짜 복잡해진 지금, 이제는 데이터 기반 의사 결정의 시대가 왔다고 생각합니다.
    - 물론 단박에 찾아지지 않는 경우도 참 많고, 무수한 반복작업에 시달려야 할 때도 있습니다. 하지만, 그 과정들이 소중하고 즐겁습니다. 또한 다양한 도메인들의 데이터를 공부할 때는 더욱 그렇습니다. 제가 아는 범위가 넓어지는 것이기 때문입니다. 그리고 그것이 제가 SK C&C에 지원한 이유기도 합니다. 다양한 위치에서 다양한 데이터를 분석하며, 회사와 사회에 가치 있는 일을 하고, 저 자신도 더욱 성장하고 싶습니다.



### 추가 중요 내용

도전적인 패기

- 빅 콘테스트 참가
  - 첫 큰 공모전에 참여한 것이었고, 이제 막 데이터 분석에 입문한지 얼마 안 되었을 때였으므로 그 자체가 쉽지 않은 도전이었다.
  - 입상을 목표로 치열하게 도전했고, 심지어 입상하지 못하더라도 분명 그 도전을 통해서 얻을 것이 많을 것이라 생각했고, 실제로 그러했다.
    - 데이터를 어떻게든 시각화하고, 분석하여 필요한 정보를 얻는 능력이 향상되었다
    - 데이터 분석, 머신러닝 딥러닝 모델 라이브러리의 숙련도가 향상되었다
    - 파이프라인의 중요성을 깨달았다.
    - 이 것을 기회로 삼아 다른 데이터 챌린지에 입상하는 계기가 되었다
- 이쪽 공부를 시작하고 얼마 안되어서, 패기로 '모두의 연구소'에 있는 딥러닝 논문을 매주 발표하는 연구실에 들어갔다 : Deeplab 논문반
  - 솔직한 마음으로는 한 두달만 버티면 간극이 좀 좁혀질 줄 알았다. 하지만, 딥러닝의 분야는 너무나 넓었다. 배경 지식 또한 어마어마하게 많이 필요했다.
  - 이곳은 어디인가 싶고, 내 자신이 작아질 때도 많았다. 하지만 악물고 버티고 최대한 따라잡으려고 노력했다.
  - 여전히 2시간에 2편의 최신 논문을 전부 이해하는 것은 버겁다. 그러나 이제는 적어도 개략적인 요소는 파악할 수 있게 되었고, 다양한 최신 논문을 세미나 형식으로 참관하면서 트렌드를 조금이나마 따라가게 되었다.
  - '슬로우페이퍼' 라는 더 시간을 깊게 두고 논문을 읽는 스터디를 하며 '혼자서 논문 읽는 법'을 공부해나갔다.
- 슬로우페이퍼
  - 처음에는 영어와 수식으로 뒤범벅된 논문을 읽는다는 사실 자체가 큰 두려움이었다
  - 그리고 실제로 읽었을 때 잘 이해하지 못하고, 자괴감이 생기기도 했다, 그러나 포기하기는 싫었다
  - 배경 지식을 이해하기 위해 미리 선정된 논문의 배경 지식을 알려주는 논문을 읽고 갔으며, 필요하다면 논문을 보다 쉽게 풀이해놓은 블로그 글을 참고한 후 참가했다
  - 질문에 주저하지 않았다 : 모르는 것 그냥 넘어가면 그대로 끝까지 모른다!
  - 현재의 작음에 실망하지 말고, 오늘 'keep-going'하자는 마인드로 도전했다
  - 가장 최근에 읽었던 논문은 뭔가? : Weight Agnostic Neural Network
- 나는 여기 임원 달려고 왔다

창의적 경험

- 미국 수경재배 경험
- 자연어 처리 모델이 번역 / 챗봇 등에만 국한되어 있어서 말투를 따라하는 모델 만들어봄
  - 한번 뒤집어서, 시각을 바꿔 보니 재미있는 결과물을 얻을 수 있었다.

팀워크 관련 질문

- 어떤 프로젝트를 하게 되면, 'giver'와 'taker' 두 종류로 나누어 진다고 한다. 실제로 빅 콘테스트 공모전을 참가하며, 또한 PPT 팀 프로젝트를 수행하며 그렇게 자연스럽게 되었다. 하지만 나의 마인드는 언제나, 적어도 내가 참여한 프로젝트에서는 'giver'가 되자는 주의였다. 'giver'로써 프로젝트에 기여할 때, 그 실제 기술들이 다시 나의 기술로 축적될 것이라 생각했다. 또한 그렇게 'giver'로써 참여해야 내가 내 자신을 그 프로젝트에 '기여하고 참여했다'라고 당당히 말할 수 있다는 생각이 들었다. 실제로, 두 프로젝트를 같이 수행했던 팀원 중 나와 같은 성향을 띈 형과 두 프로젝트의 대부분을 수행했는데 결국 우리 둘이 가장 실력 향상이 컸고, 실제로 현재 가장 좋은 기회를 많이 얻고 있는 상황이다.
- 그럼에도 불구하고 내가 'taker'들을 미워하거나 아니꼽게 생각하지 않았던 점이 오히려 'giver'로써 공헌하는 데 좋은 마음가짐을 갖게 했다. 분명히 각자의 사정이 존재했었고, 그 점을 수용했고 억지로 닦달하지 않았다. 그 대신 그들이 더 잘 해낼 수 있는 일을 맡겼다. 특히, 아쉽게도 아무리 해도 코딩 능력은 잘 향상되지 않던 친구가 있었는데 정보 수집력은 좋았다. 그래서 두 프로젝트에서 도메인 지식을 잘 검색하고 추려내어서 공유하는 역할을 시켰더니 잘 해내었다. 각자의 위치에서 능력이 적절히 발휘되어 더 좋은 결과를 낳았다.

### 필살기

빅 콘테스트

앨리스 데이터 챌린지

PPT

성경말투 따라잡기

파이썬 영상처리 프로그램

(파이썬 GUI를 이용한 작업관리자 프로그램)

다양한 스터디



### 확실해진 점 : 적어도 내 자소서는 씹어먹을 정도는 되어야 한다!

1. conv 신경망들의 개념

   - 컨볼루션 연산 : 여러개의 필터를 이용하여 이미지의 특성을 뽑아내는 feature map으로 압축
     - 각 filter 모양 에 따라서 강조된 각 feature map이 나오게 됨
   - 1x1 conv를 수행하면 : 여러개의 feature map으로부터 비슷한 성질 갖는 것들을 묶어낼 수 있고, 결정적으로 feature map 숫자를 줄일 수 있다 => 연산량이 줄고 => 망이 더 깊어짐!

   - ResNet
     - 이전까지 : VGGNet
     - 망이 깊어지면 더 성능이 좋아지는데 그렇게 하기가 너무 힘들었어! : vanishing/ exploding gradient가 일어나기 쉽다! : 역전파되는 그래디언트가 자꾸 소실되어서 제대로 학습이 되지 않음! : 다시 앞쪽으로 갈수록 갱신되는 weight가 엄청 작음 : 결국엔 0되버리기!
     - 파라미터 수도 엄청 많았다 : V/G나 과적합 일어나기 쉽다 : 채널들이 죽어버려서 노이즈처럼 작용할 수도 있고, 계산량만 증가!
     - 파라미터 수가 엄청 큰데 그에 비해 데이터셋이 따라주지 못하면, 모델은 그 데이터에 과적합될 것!
     - 이걸 해결하기 위해 act func.를 relu로 바꾸고, BN을 도입하고, 파라미터의 초기값 설정 방법을 개선하는 등 했지만 근본적 해결은 안됨
     - Residual Learning! : 깊이에 따른 효과를 얻을 구조!
       - skip connection!
       - 입력에서 출력으로 바로 elementwise하게 합을 해주는 shortcut 연결을 해주게 됨으로써, 입력의 작은 움직임도 잘 파악해서 학습을 더 잘 되게함
       - 중간의 3x3 conv를 1x1 3x3 1x1로 연산하여 연산량을 줄임
       - 그리고 activation func와 BN의 위치에 따라서 residual block의 성능이 크게 변화하는 것을 알게 되고, 수정하게 됨
   - Inception module : 하드웨어 자원을 효율적으로 쓰면서 깊게 하고싶다!
     - GoogleNet
     - 결국 더 다양한 feature를 추출하기 위해 여러개의 convolution을 병렬적으로 활용하려고 함! : 다양한 scale의 feature를 추출할 수 있다
       - 근데 너무 연산량이 큼 : bottleneck : 1x1 conv 사용
     - 나중에는 5x5도 3x3 2개로 쪼개고 (전자는 1px 추출하기 위해 25이지만, 후자는 18(9+9)임!) 3x3 도 3x1 1x3 이따위로 쪼개서 절감
   - openpose network 구조
     - VGGnet의 첫 10개 레이어를 활용하여 input image의 feature map 생성
     - 그 다음 두개의 CNN 브랜치로 나뉜다 :  Part Confidence Maps, Part Affinity Fields
       - 총 6stage를 반복하면서 학습하게 됨 : ground truth 정답 레이블이 있어 loss 구하고 optimize하는 방식으로 network 학습
       - confidence는 신체 각 랜드마크 부위에 예상되는 위치를 히트맵으로 나타낸 것
         - 총 랜드마크 갯수의 heatmap 생성
       - affinity는 각 신체 부위가 사람들마다 잘 연결되게 학습시키는 것
   - Batch Normalization? : covarience shift를 해결하자!
     - 텐서들이 학습되는 레이어들을 지나면서 각 layer들의 input 분포가 일정하지 않게 됨.
     - 심지어 test랑 train의 분포도 다름! : 이런게 학습을 잘 되지 못하게 함!
     - 그러면! 신경망의 각 레이어의 분포를 같게 해주자!
     - 미니배치 사이즈의 mean과 varience로 표준화해주고, 학습 가능한 웨이트를 만들어서 최적의 분포를 만들게 학습시키자!

2. 챗봇 프로젝트 => 성경말투 따라잡기 프로젝트로 변경되어 실행

   - seq2seq + Attention

### '행복' : 일과 행복을 연결시키는 나만의 생각?

- 일하면서 행복하게 일해야 결국 자발적으로 / 의욕적으로 할 수 있다!
- 일하면서 나의 행복을 어떻게 찾을 수 있을까?
  - 최신 기술이 끊임없이, 빠르게 범람하는 분야로 지원했다고 생각합니다. 누군가는 이러한 새로운 배움이 부담스럽고, 각자의 행복을 방해하는 것으로 생각할수도 있습니다. 그러나 저는 이러한 새로운 배움이 즐겁고, 그로 인해 더 나은것을 만들어내는 것이 크게 즐겁습니다. 그 결과 다양한 스터디에 참여하였고, 이러한 일은 제게 부담이 아닌 즐거움으로 다가왔습니다.
  - 그리고 행복은 결국 나 스스로 만족하고, 주어진 환경에 감사하며 살아가는데서 오는 것이라고 생각합니다. 비록 삶이란게 완벽한 것 하나 없고 부족한 것이 많겠지만, 그 자체에 투정하는 것이 아니라 어떻게 하면 더 편리하게 바꿀까, 그렇게 고민하며 해결해나가는 것이 삶의 또 하나의 재미라고 생각합니다.
  - 그냥 주어진 일을 기계처럼 받아서 하는 게 아니라, '이러한 업무를 이렇게 처리하는 것'이 내가 더 행복해지는 일이라는 것을 인지하는 것이 중요하다고 생각합니다.

### 'Social Value' : 사회적 가치에 기여하라!

- 새로운 광고 캠페인, '업', '짝'만 보아도 숨은 의도를 잘 알수 있습니다.
  - 문제를 문제 그 자체, 스트레스로 받아들이는 것보다, 사회와 기업이 함께 해결해나가는 '업'
  - 사회와 기업이 함께 단짝으로 짝궁처럼 해결하는 '짝'
- 이러한 가치가 SK 주식회사에서는 '인공지능 문자통역' : 쉐어톡 으로 서비스!
- 강사 말이 문자로 (STT) / 일 대 다수의 문자 서비스 지원 : 접근성 크게 향상
- 저는 이러한 부분이 진정으로 기술이 나와 사회를 행복하게 하는 중요한 요소라고 생각하였고, 이러한 일을 저도 해 낼 기대를 하며 왔습니다. 
- 기술로 기업과 사회가 더 큰 행복을 만든다는 철학, 그 철학이 저를 SK에 지원하게 된 큰 계기 중 하나입니다. 

##### 말을 하다보면 항상 '산공인데 방향을 돌렸다' 라는 식의 표현이 자연스럽게 나오던데, 이 부분은 바꾸는게 확실히 좋겠다 : 소개를 하고 대답을 할때 AtoZ로 다 말한다기보다는 담백하게 

##### **'진짜 내가 왜 이 직무를 하고 싶은지, 어떻게 준비해왔는지 대답'**



- 인공지능 에이브릴
  - 얼굴 인식 : 얼굴 detection
  - 대화 : 의도 파악 기반의 챗봇
  - WKS : watson knowledge studio : 머신러닝 알고리즘을 통해 자동 데이터 분석 / 추출
  - 음성 합성서비스 : Watson TTS
  - 음성 인식 : Watson STT



- 빅데이터 AccuInsight+
  - 빅데이터 수집 / 처리 / 분석 / 시각화 서비스 지원
  - ML modeler
  - DL modeler
  - Cloud search
  - data insight



#### 어떻게 준비하는가?

##### 면접 통과 신입사원이 말하는 중요한 면 1

자기소개서 완벽 숙지 : 각 문항 잘 준비 + 부족한 자소서는 돌려막을 수 있는 대답을 준비하자!

거울 보며 예상 질문 뽑아 : 입모양 / 표정 신경쓰며 대답

대학원이나 공부 말고 대학에 돌아가면 뭘 하고 싶어요? : 나는 뮤지컬 활동을 하는게 참 즐거웠다

회사 언제 그만둘거 같냐 : 나는 사실 늙어서까지 계속 일하는게 목표이다! 그냥 회사를 다니는 일은 10년 혹은 20년 있을 수도 있지만, 그 이후에도 계속 커리어를 개발해나가며 프리랜서로도 일하고싶다. 그리고 이 쪽 분야 일은 새로운 지식이 끊임없이 범람하고, 기술들도 너무나 빠르게 발전한다. 계속 그러한 것들을 공부하고 정복해나가며 도전적이고 진취적인 삶을 살아나가는 게 인생에서 참 중요한 요소중 하나라고 생각한다.

**프로젝트 중간에 어려웠던 점은 무엇이었고** , **내가 맡았던 일은 무엇이었는지 정확히 설명 가능히 준비!** : 하나하나 다 정리를 하고, 무엇을 물어봐도 바로 답을 할 수 있도록 완벽히 숙지

전공책이 필요한 부분이 있으면 다시 봄!

실제로 일을 잘 할 수 있는 사람 : 인재상이 사실 정해져 있다!

면접관에 따라 1분소개를 하는 => 구지 튀는 모습보다는:

​	**진솔하게 본인이 어떤 경험을 했고 / 어떤 개인 역량 향상 노력 / 어떤 노력으로 회사에 가치를 줄것인지 => 담백하게 정리해서 말하는 소개가 더 좋겠다!**



##### 면접 통과 신입사원이 말하는 중요한 면 2

제일 중요 : 어떤 일을 했는지 => 스스로 알고 정리

​	대학 생활 어떤 프로젝트 : 무슨 일 / 결과 / 무엇을 얻었는지

​	예상 질문 별로 이 질문에는 어떤 프로젝트를 이야기 해야겠다 정리!

**실패했던 경험이 무엇인가? : 공모전에서 떨어진 이유 / 실제 그 공모전에서 수상을 한 팀은 어떤 식으로 준비를 하여서 성공했는지 : '우리는 이런게 부족해서 떨어졌구나' : 실패를 분석하는 모습! 	=> 빅 콘테스트의 실패 사례도 잘 분석해서 가보자! + 수상자와 비교**

1분 자기소개 기반으로 질문이 들어왔다 : 글로벌 회사 성장 기여? (1분 자기소개도 생각을 해야되는구만!)

시간 부족 : 단순 지식보다는 프로젝트 경험에서 경험에 비중을 높여!



##### 면접 통과 신입사원이 말하는 중요한 면 3

예상 질문들 쫘-악 뽑아서 30초 분량 대답할 수 있게 정리

​	내가 어떤 사람인지 찾아나가는 과정 : 예상 답변 달달 외우기보다는 '내용 위주의 대답' 숙지

기술을 물어보는 질문 : 학교 프로젝트 / 과제 : 꼬리 + 꼬리 => 너, 기술에 관심 있는 사람이니?

해당 기술에 대해 숙지하고 있니? 아는 것은 최대한 아는 선에서 설명

내가 데이터직군인데 일단 : 객체지향 또는 데이터 기본 직무 질문들이 들어올텐데 뭘 물어볼까?

**모르는 지식을 싹 다 커버할 수는 없다 : 여태까지 했던 기술들을 내가 잘 아는 기술들을 싹 다 잘 정리해서 : 꼬리질문들을 예상하면서 담백하게, 사실 위주로 준비!**

**SK C&C 안에서 어떤 구체적인 목표를 세우고 있는지, 일에 대한 진정성?**

주변 어른들에게 말하듯이 편안한 분위기에서 얘기!

'답'이 없는 질문을 할 때가 많다 : 가끔은 본인이 아는 경우도 있지만, 어떤 태도들을 가지고 있는지!

**프로젝트를 수행하며 내가 얻은 것, 심지어 실패하였더라도!**





## 면접을 보는 이유!

- 전문 면접관이 아니다 : 대부분은 현업에서 일 하시는 팀장급
- '이런 애'가 내 밑에서 일했으면 좋겠다!
- 객관적으로 기준이 정해져 있지 않다
- '굉장히 보수적인 마인드' : 굉장히 성격 / 외모 / 무난히 : 생각보다 고지식! : 깔끔히
- 살짝 미소지음
- 말하는 분을 한명씩 쳐다보기 : 스윽-미소짓기 (계속해서~!)
- 말하는 사람의 눈을 턱 / 목 쳐다보기
- 누가 말하면 자신감있게 2초간 보기
- 자신감있게 못들었으면 : 잘 못들었는데 한번 더 말씀해주시겠어요?
- => 정중히, 그러나 당당하게
- 말하는 태도 / 인성을 사실 보려고 함 : **열정 / 패기 / 겸손함 / 자신감**
- 만약 진짜진짜 모르겠다면 : 제가 나중에 엘리베이터에서 마주쳤을 때, 다시 한번 제대로 말씀드릴 수 있도록 찾아보고 공부하도록 하겠습니다!
- 걍 까칠하게 들어오더라도 결국은 넉살있게, 웃으면서 넘길 수 있는 그런 인성, 하하, 죄송합니다! => **'표정관리'**
- '상사들의 입장에서 생각하라' : **부드럽게, 위트있게, 상사에게 도움을 줄 수 있는**
- '니 잘 모르네?' : 제가 아직 아는것도 별로 없고, 현업에서 오랫동안 일하신 분들의 내공을 따라가기엔 훨씬 못 미칠거라 생각합니다, 하지만 기회가 주어진다면 적극적으로, 열심히 배울 자신이 있습니다.
- 마지막말 : 주인정신 : 회사에서 그냥 자리지키다가 월급받아가는 사람이 아니라, 상사입장에서 '든든한' 후임이 되고싶다!
- 이 회사 너무 좋다 / 나는 존나 커서 이 회사에서 임원되는게 목표다! / 그리고 위의 두 이슈 잘 종합해서 얘기하는 것도 좋을듯!
- 무슨 일 있어도 이 회사에 남아있을 거 같다!는 이미지 남겨주기
- **쫄지말고 / 당당하고 / 그러나 남을 까진 말자**



### 자소서 기반 질문

**프로젝트 중간에 어려웠던 점은 무엇이었고** , **내가 맡았던 일은 무엇이었는지 정확히 설명 가능히 준비!** : 하나하나 다 정리를 하고, 무엇을 물어봐도 바로 답을 할 수 있도록 완벽히 숙지

**실패했던 경험이 무엇인가? : 공모전에서 떨어진 이유 / 실제 그 공모전에서 수상을 한 팀은 어떤 식으로 준비를 하여서 성공했는지 : '우리는 이런게 부족해서 떨어졌구나' : 실패를 분석하는 모습! 	=> 빅 콘테스트의 실패 사례도 잘 분석해서 가보자! + 수상자와 비교**

**모르는 지식을 싹 다 커버할 수는 없다 : 여태까지 했던 기술들을 내가 잘 아는 기술들을 싹 다 잘 정리해서 : 꼬리질문들을 예상하면서 담백하게, 사실 위주로 준비!**

**SK C&C 안에서 어떤 구체적인 목표를 세우고 있는지, 일에 대한 진정성?**

- 높은 목표 설정
  - 이제 실제로 바닥부터 구현하였으므로 내용을 정확히 이해했다.
  - 더 쉽게, 빠르게 구현하기 위해서 keras의 만들어진 모델 및 가중치까지 로드해서 학습시킬 수 있다
    - transfer learning : 훨씬 빠르고, 높은 acc를 얻으며 학습할 수 있었다.
  - 최근에 더 발전된 이미지 관련 딥러닝 알고리즘 (object detection)을 더 알아야겠다는 생각이 들어서 YoloV3에 대해서 공부함
  - Yolo v3 : yolo랑 기본아키텍쳐는 비숫 : CNN 레이어를 어떤 것을 사용했는지 정도가 다름
    - 정확하다기보다는 빠르다!
    - '알고리즘적으로' 빠르다!
    - 알고리즘 과정
      - 448 * 448 * 3 Input
      - googleNet layer 20 layer => 14 * 14 *1024
      - conv 4회
      - FC layer 2회
      - 7 * 7 * 30 => last output
    - bounding box : from 7 * 7 * 30 <= 요게 타겟데이터!
      - 맨 마지막에 7 * 7 * 30 그리드인데, 30개의 각 값이 다른 의미를 가짐
      - 앞에 5개는 상 - 하 좌표들 값
      - 5 -10개는 좌 - 우 좌표들 값
      - 나머지 20개는 number of classes
    - inference시 인접한 부분의 것 중 확률이 너무 낮으면 제거
    - 결국 grid 하나당 (x, y, w, h, c) 의 쌍이 2개이다 => 최대 하나의 사진에서 96개 검출이 한계!
    - 각 class마다 98개의 bounding box의 확률을 계산!
      - IoU 계산으로 조건에 부합하지 못하는 bounding box는 드롭!
  - loss function이 중요!
    - x,y 끼리 mse / w,h 끼리 rmse / grid 안에 obj가 있을 확률 mse / grid 안에 obj가 없을 확률 mse / class 값끼리도 mse
    - regression 으로 나는 그냥 풀었다 (cross entropy 안쓰고)
- 기존의 틀을 깨는 과감한 실행 
  - 일단 그냥 진짜 돈을 보내주거나, 식량을 보내주는 것이 아닌, '식량을 만드는 시설'을 우리가 직접 만들어서 보내줄 수 있었다는 점이 상당히 기존의 틀을 깨는, 새로운 도전이었다.
  - 양배추, 양상추, 토마토의 수확을 올렸다
  - 핸드펌프 사용
  - 액체 비료를 사용함
  - 솜을 사용
  - 가장 중요했던 부분은 외국인 친구들과 영어로 커뮤니케이션하면서 타협점을 만들면서 좋은 결과를 냈다는 것이라고 생각합니다.
  - 미국에서 기본적인 수경재배 방법은 전기를 이용하는 모터를 사용하는 것으로 고정관념이 잡혀 있었는데, 이를 간단히 포기하고 차선책을 빠르게 선택한 것이 참 좋았던 부분이라 생각합니다.
- 빅 콘테스트 : 챔피언 리그
  - 게임 데이터를 이용해서 이탈 유저와 평균 결제액을 예측하는 프로젝트
  - EDA 및 Feature Engineering
    - 전투 / 지출 / 혈맹 / 거래 / 활동 등으로 나누어져 있는 테이블을 하나로 합침
    - Null data는 implementation 함 id별 average / group별 average 등 기준
    - EDA를 하던 도중 : 40000명의 data 중 오직 0.065%의 사람이 매출의 11%를 차지하고 있음!
      - 모델을 구축할 때 타겟 레이블의 편향이 매우 큼을 인지함
    - knn, kmeans 머신러닝 혹은 평균, 중간값 등으로 NaN을 보간함
    - 데이터의 feature를 하나하나 시각화하여 특성을 분석하려고 노력함
    - 모델링 (LSTM)에 너무 집중하여 실제 더 중요한 데이터를 더 등한시했다
    - 정형 데이터 컴페티션에는 사실 부스팅 계열의 머신러닝 알고리즘이 더 강력한 경우가 많다

### 자소서 / 경험에서 부족한 내용 추가





### 예상 질문 LIST





### Kaggle 수업에서 배운 ML 기본개념 정리

- 앙상블 : 배깅 / 부스팅 / 스태킹
  - 배깅 : 살짝 다른 버전의 같은 모델들을 평균하는 것!
  - 부스팅 : **이전 모델의 성능을 고려하여 각 모델이 순차적으로 만들어지는 모델의 가중 평균 형식**
  - 스태킹 : 서로 다른 타입의 모델 결합 : 처음에는 base learner로 prediction 구한 다음, 나머지 **meta model**들을 base learner들의 output으로 학습
- TP / TN / FP / FN : T / F는 실제로 맞췄느냐, 못맞췄느냐 : P / N 은 예측값이 어떻게 나왔느냐
- Accuracy : 그냥 정확도 : TP + TN / TP + TN + FP + FN
- 모델링 성과를 평가하는 지표 중 하나.
- Precision : 모델이 True라고 분류한 것 중 실제 True의 비율 : TP / TP + FP : **내가 예측한 것중 True**
- Recall : 실제 True 중 모델이 True라고 예측한 것의 비율 : TP / TP + FN : **실제 True 중 맞춘 비율**
- Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우에 관심이 있으나, 바라보고자 하는 관점만 다름 => Precision (모델 입장) / Recall (실제 정답(data)입장)
  - 확실한 날만 맞다고 하면 precision은 올라가겠지만 그게 의미가 있나?
    - ex: 20일 비옴, 그 중 이틀은 특정한 날 확신. 그 두날만 하면 100% => meaningless
  - 그래서 Recall도 같이 고려해야 함
  - Acc가 좋긴 하나, data의 domain이 불균형하면 성능지표로 부적합!
    - 비 오는거 예측하는데 그 지역이 비가 엄청 안오는 곳이다. 의미가 있는가? => 노의미
  - **그래서 Precision과 Recall 조화평균 => F1 score**
- ROC curve
  - TPR : True Positive Rate : 민감도 : 1인 케이스에 대해 1로 잘 예측한 반응
  - FPR : False Positivie Rate : 1-특이도 : 0인 케이스에 대해 1로 잘못 예측한 반응
  - ROC curve가 위로 볼록할수록 잘못 예측하는 확률이 낮으면서 잘 예측하는 확률은 높은 것이기 때문에 좋은 모델! => 그래서 AUC 구해서 모델의 Metric으로 사용 가능
- 언제 l1, l2 정규화 각각 사용하는가?
  - l1-> 변수 각각에 걸리는 가중치 중 일부가 아예 0이 되어버리길 원할 때 (그래서 feature selection으로도 사용됨)
  - l2-> 변수 각각에 걸리는 가중치 전부가 엇비슷하게 되길 바랄때 (튀는 값이 없도록; shrinkage method)
  - https://images.app.goo.gl/5pEwP9EhHRcwz73d9
- Hyperparameter Optimization의 여러 방법
  - For : 학습률 / 미니배치 크기 / L2 정규화 계수 / # of layers / # of conv. filters
  - Manual Search
    - 보통 논문의 계수들을 그대로 가져오나, 데이터가 다를 것이므로 확실히 잘 작동한다는 보장이 없다 : 대중적으로 알려진 노하우 등에 의존하여 hyperparam 값들을 설정하고, val set으로 성능 결과를 보고 가장 좋은 결과의 값을 선정, 최종 제출하는 방식
    - 일단 내가 선택한 것이 최적이라는 보장이 없고, 단일 param 설정시 각각이 트레이드 오프 관계에 있는 경우가 많은데, 이를 직관적으로 풀이하기 힘들 가능성이 높음
  - Grid Search
    - 구간내의 후보 값들을 일정한 간격으로 뽑아 성능 측정후 최고 선정
    - param의 짝들이 많아지면 검사 갯수가 기하급수적으로 늘어남
  - Random Search
    - 구간 내에서 Random으로 뽑아 성능 측정 후 최고 선정
    - 확률적으로 탐색하므로 최적을 더 빨리 찾을 가능성이 있다
  - 어쨌든 위의 애들은 Manual 빼고는 이전 h.p 결과값들에 대한 사전 지식이 다음번 후보 선정에 반영이 안되어있음 <= 불필요한 탐색!
  - h.p 탐색시 사전 지식을 충분히 반영하는 체계적인 방법론이 Baysian Optimization
    - Surrogate model : 현재까지 조사된 입력값 - 함수값 셋들을 바탕으로 목적함수의 대략적인 형태를 확률적인 추정을 수행하는 모델 : Gaussian Process : 불확실성을 줄여나감 : 요 추정 형태를 바탕으로 새로운 input 구간을 설정
    - Acquistition Function : input 구간에서 EI라는 특정 알고리즘을 사용해서 새로운 input 후보를 선정 후, 모델 성능 결과를 계산하고 이를 새로운 함수값으로 간주함. 지금까지 한 것 반복
    - 반복 후 생성된 평균 함수를 최적화 하는 하이퍼파라미터를  찾으면 됨
      - 평균 함수는 함수값들을 다 지나도록 설계되는 함수
- 내가 발표했던 부분 : Validation

### 10/31 추가 정리해야 할 부분

지금까지 내가 한 것

- 빅 콘테스트

- 앨리스 데이터 챌린지

- PPT

- 성경말투 따라잡기

- 파이썬 영상처리 프로그램

- (파이썬 GUI를 이용한 작업관리자 프로그램)

- 다양한 스터디

  - Kaggle

    - Coursera 강의를 듣고, 팀을 구성해 Kaggle Competition에 도전하는 스터디입니다.

  - spark

    - 기본 구조 : 클러스터 매니저에 익스큐터와 드라이버 프로세스가 연결되어 있고, 이 드라이버 프로세스에 명령을 내려 익스큐터들을 컨트롤

      보통 하둡 프레임워크와 결합될 때 더 성능이 좋다

      hadoop과의 가장 큰 차이점 : in memory 연산이 가능하기 때문에 훨씬 빠른 속도로 데이터를 읽어올 수 있다.

      기본적으로 연습은 pyspark를 사용했음 : 파이썬을 주 언어로 사용했기 때문에 편해서

      기본 구조 : 나는 고수준 API로 연습했다 :  Data source(Hive, hadoop)으로 데이터 호출 => 여러 트랜스포메이션을 거치면서 플랜을 짬 => 이를 action으로 한번에 처리 => spark job을 내부 익스큐터들의 상태에서 가장 이상적이게 배분해서 실행

  - ai 논문 읽기 : slowpaper
  
    - 매주 토요일마다 모여 한 편의 딥러닝 논문을 같이 읽는 스터디입니다. 남이 해석해 놓은 논문 내용이 아니라 직접 3시간 동안 논문을 낑낑대며 읽어보며, 제 힘으로 논문을 읽는 법을 깨달았습니다.
    - 가장 최근에 읽은 논문 : Weight Agnostic Neural Networks
      - weight에 별로 상관 없는 인공신경망
      - 현재까지의 concept는 모델의 가중치를 학습시켜서 좋은 결과를 낳은 반면, 이제는 모델의 structure를 학습시켜서 만들면, 가중치에 큰 영향을 받지 않는 네트워크를 만들 수 있을 것이라는 내용
      - 작은 네트워크의 집합을 만들고 => 그것들을 특정한 알고리즘으로 랭크를 매긴 후, 좋은 애(로스가 낮은)들은 노드를 만들거나, 커넥션을 만들거나, activation function을 바꾸는 등의 처리를 통해 변형하여 이 작업을 반복하는 일종의 유전 알고리즘
      - 이렇게 했더니 이 작업을 그렇게 많이 안해도 Mnist 92% 찍더라
      - 모델 가중치를 학습 안해도 이정도 잘하는건 꽤나 좋지 않은가?
      - 그러나 이게 이런 모델을 풀로 학습시켰을 때 기존보다 좋다거나 훨씬 생산적이라는 보장은 아직 없으나, GAN과 같은 새로운 패러다임을 제시한 아주 도전적인 논문이기 때문에 이어지는 연구가 기대됨
  
  - ai 딥랩
  
    - 매 주 월요일 두개의 논문을 세미나 형식으로 발표합니다. 최신 트렌드의 논문에 대한 정보를 얻을 수 있는 시간입니다.
  
  - **ai college : Explainable AI**
  
    - 이제는 '이게 맞아?'라고 누군가 물어본다면 '왜?'라는 답변이 꼭 필요한 시대가 되었습니다.
      - 저 또한 딥러닝 모델들을 설계하며 괜찮은 결과를 얻을 수 있는 방법들은 알아내었지만, 그 이유에 대한 증명이 '경험적으로 실험해 보니 그렇더라' 하는 경우가 상당부분 존재했습니다.
      - 이렇게 되면 블랙박스를 그냥 가져다 쓰는 것이다! 라는 한계에서 벗어나기 참 힘들것이라는 생각이 들었습니다.
      - 그래서 딥 뉴럴 넷의 내재된 정보를 사람이 해석 가능하게 하자 : Interpretability
        - 모델이 내가 원하는 대로 학습하는구나! 라고 Verify할 수 있다
        - 취약한 부분을 더 잘 구분해서 => Improve / Debug 에 크게 유용하다!
          - 이전까지는 취약한 부분을 'Data의 양'으로 커버하려 했는데 데이터를 구하고 정제하고 어쩌고 하는데 시간과 노력이 엄청나게 든다! => 만약 모델을 어디를 고쳐야할 지만 알면 엄청나게 절약될텐데!
        - 새로운 발견을 해낼 수 있다
        - 이제는 시대가 바뀌어서 AI 설명해야할 의무와 권리가 생겼다.
    
    - 기본적인 DNN의 해석(Interpretability)
    
      - 모델을 해석하거나 / 결과를 해석하는 것으로 나뉨
      - 결국 현재 가장 핫한 부분은 이미지를 해석할 때 '어떤 부분을 보고 결정했느냐' 알아보는것
        - 일종의 'Heatmap' 찍어보기! : Saliency map
        - 보통 이제 나온 결과를 다시 backprop하면서 맨 처음 픽셀로 되살려보는 과정을 진행함
        - 여기에서 최신 논문은 오히려 noise를 통해 gradient의 꺼끌거림을 smoothing 해준다고 함
          - 함수가 많은 노드들을 지나면서 굉장히 구불해지고 뾰족해지는 것을 smoothing 해줌으로써 넓은 범위에 퍼져있던 sliency map을 실제 원하는 부분에 제대로 나타낼 수 있음 
        - 다양한 종류가 있는데, 현재 공부한건 CAM / GRAD-CAM
    
    - 평가 method
    
      - 시각적으로 보는 Quality
    
      - 실제 점수 매기는 Quantity
    
        - ROAR & KAR
    
          특정 att 지우고 train 해서 원래꺼랑 비교
    
          영향 큰거 놔두고 적은 것들은 지우고 train 해서 원래꺼랑 비교
    
    - 결론 : 결국 '왜 잘 돌아가는지 제대로 설명해 주자!'



### 빅 콘테스트 & 캐글

1. 빅 콘테스트
   1. 내가 기여한 부분
   
      1. EDA 및 Feature Engineering
         - 전투 / 지출 / 혈맹 / 거래 / 활동 등으로 나누어져 있는 테이블을 하나로 합침
         - Null data는 implementation 함 id별 average / group별 average 등 기준
         - EDA를 하던 도중 : 40000명의 data 중 오직 0.065%의 사람이 매출의 11%를 차지하고 있음!
           - 모델을 구축할 때 타겟 레이블의 편향이 매우 큼을 인지함
         - knn, kmeans 머신러닝 혹은 평균, 중간값 등으로 NaN을 보간함
         - 데이터의 feature를 하나하나 시각화하여 특성을 분석하려고 노력함
         - 모델링 (LSTM)에 너무 집중하여 실제 더 중요한 데이터를 더 등한시했다
         - 정형 데이터 컴페티션에는 사실 부스팅 계열의 머신러닝 알고리즘이 더 강력한 경우가 많다
   
   2. 내가 얻은 부분
   
      1. 내 작업에만 너무 집중하다보니 남이 이미 해놓은 것을 제대로 보지 못함
      2. 지난해 대회도 비슷한 내용이었는데, 트리 기반 솔루션으로 해답을 냄.
      3. 정형 데이터에는 트리 기반 솔루션이 좀더 잘 적용된다.
      4. 게임 데이터라서 일단 꽤나 재미있게 분석했다. 특히나, 데이터가 전체 나름대로 정규화 되어있었는데, 이를 역정규화하는 과정에서 실제 데이터 상(train 데이터 기준) 약 0.065%의 사람이 전체 매출 11% 이상을 담당하고 있는 것을 보며 이 게임이 참 위험하구나 하고 깨닫기도 했음. 그리고 이 피쳐를 고려하여 모델을 이중으로 나눠서 나름 메타 학습을 시도해보려 했으나 그렇게 잘 되지는 않았다.
      5. 이 대회를 참가하면서 데이터를 다루는 것 : 로드하고, 헤드를 찍어보고, 머지하고, 아웃라이어를 걸러내고, 피봇을 만들어보고, 크로스 테이블 만들어보고 등등 데이터를 다루고 정제하는 데 익숙해진 아주 소중한 시간이었다.
         - 또한, 데이터를 제대로 이해하기 위해 각 피쳐들의 분포를 시각화하여 눈으로 확인하였고, 각 피쳐간의 관계를 corr Mat를 만들어 heatmap을 만들어 분석했다.
      6. 나름대로 최선을 다하고 굉장한 노력을 들였지만, 결과가 잘 나오지 않아 당시에는 마음이 아팠다. 그러나 이러한 어떻게든 분석해보려고 다각도에서 접근하는 노력이 데이터 분석을 하는 데 있어 나의 시야를 넓혀주었고, 또한 기술적으로는 Python이라는 언어와 더욱더 친근해지고 데이터 분석 관련, 머신러닝, 그리고 딥 러닝 라이브러리의 사용에 친숙해지는 좋은 계기가 된 시간이었다.
   
   3. 실패한 이유, 남들은 어떻게 했더라 + 앞으로 어떻게 할 지 정리
   
      1. **공모전에서 떨어진 이유 / 실제 그 공모전에서 수상을 한 팀은 어떤 식으로 준비를 하여서 성공했는지 : '우리는 이런게 부족해서 떨어졌구나' : 실패를 분석하는 모습! 	=> 빅 콘테스트의 실패 사례도 잘 분석해서 가보자! + 수상자와 비교**
   
      2. 일 평균 결제 금액에 대한 고찰
   
         - 유의미한 변수를 찾는 것이 힘들었다. 군집화, 네트워크 분석, 피봇, 크로스탭 등을 하며 나름대로 피쳐를 만들고 분석하려 노력했지만 '절대 유의미한 변수'를 찾는 것에는 실패했다.
   
         - 평균 결제 금액을 예측 할 때, many-to-many의 모델을 만들어서 맨 마지막에 softmax로 꺼내어 regression을 취하는 모델이 잘 작동할 것이라 생각했지만, 이는 그냥 오산이었다. (딥러닝을 마술처럼 믿었다)
         - 또한 test data와 train data의 결제 금액의 maximum이 같을 것이라는 보장이 없었기 때문에, test data를 일단 0~1로 예측한 후, 상수 N을 곱해줘서 추론하는 게 더 논리적이었을 것이라는 생각이 들었다.
         - 위에도 설명했지만 딥 러닝이 마술처럼 모든 일을 다 해결해 줄 것이라는 생각을 버리게 된 좋은 계기였다. 다양한 알고리즘이 다양한 문제들을 위해 존재하고, 이를 잘 조합하는 게 현실적인 문제를 해결하는 데 실질적인 도움이 됨을 깨달았다
           - 일단 딥러닝 (특히 sequential data를 위한) 모델은 너무 무겁다. parameter 수도 엄청나게 많아서 해석도 힘들다. 이 계기를 통해 data challenge platform인 kaggle 도전하여 더 배우고 싶은 의지가 생겼고, 스터디에 참가하게 되었다.
           - 또한, 여기서 얻은 인사이트로 elice data challenge라는 대회에 참여하여 100여명 가운데 10위를 차지했다.
   
2. 캐글
   1. 지금까지 배운 지식들 총 정리 : 개념 툭 하면 나올 수 있게?
      - 위에 정리
   2. competition
      1. 기본 competition (titanic + image classification)
      2. 실전 competition이 부족? 이라는 말에서 elice data challenge 어필
      3. quick, draw
   
3. Elice Data Challenge

   1. 그러나 빅 콘테스트에서의 경험을 바탕으로 elice 데이터 챌린지에서 10위를 달성

   2. 10위의 순위를 차지했다 : 

      1. 시계 데이터를 피쳐로 나누어 분석
      2. 각 데이터의 이상치 제거 후 투입
      3. 문서같은 경우에는 tf-idf 행렬을 만들어 점수화하여 모델에 넣어줌
      4. 시간문제상 보통 rf같은 앙상블 머신러닝 모델 사용
      5. 나의 입상 비결 
         1. 집요함 :  column들과 feature들을 분석했다. 히트맵을 찍어보면서 trial-error 방식의 테스트도 해보고, 피쳐를 새롭게 만들어보며 최선을 다했다.
         2. 포기하지 않음 : 맨 마지막에 15위까지만 입상이었는데, 시간 맨 마지막에 많은 사람들이 최선을 다해서 제출을 하는바람에 경쟁이 치열했다. 나 역시도 그시간에 포기하지 않고 계속 모델을 발전시킨 결과로 좋은 결과가 있었던 것 같다.
         3. 혼자가 아닌 함께 : 같이 수업을 듣는 형과 분석에 대한 인사이트를 나누며 함께 공부했다. 물론 룰을 어기긴 싫어서 코드 자체를 공유하지는 않았고, 어떤식으로 분석을 진행했는지, 어떤 시각으로의 분석이 더 효과적인지에 대해 피드백을 나눈것이 큰 도움이 되었다.

      

      1. 시계열 데이터를 분석하여 가시거리를 예측하는 문제
      2. 고객 정보를 기반으로 다음 분기에 고객이 금융 상품에 가입할지 예측하는 문제
      3. 구매자의 결제 내역을 기반으로 직업을 예측하는 문제
      4. 전문가의 와인 평가내역(문서) 및 와인의 특성을 고려하여 와인의 평가점수를 예측

      

      시계열 데이터를 그냥 피쳐로 투입했던 게 약간 부족했던 점이었다. 이걸 한번 더 생각해서 LSTM같은 시계열 데이터에 적합한 인공신경망에 투입시켰다면 더 높은 점수를 받을 수도 있었을 것이다.

4. 여태까지 내가 했던 일, 잘 하는 일 정리 => FACT 위주!
   1. 여러 프로젝트
      1. 교육에서 내가 얻은 것은?
      2. 내가 했던 프로젝트로 얻을 수 있던 결과?
      3. 네가 했던 프로젝트가 실제로 어떻게 적용할 수 있느냐?
      4. 어떤 경험이 있으면 현존하는 프로그램에 비해서 어떤 게 좋은지 어필?
      5. 툴이나 가져다 쓴 opensource를 설명해보고, 직접 구현하면 어디까지 구현할 수 있는지?
      6. API는 어디까지 써볼 수 있느냐?
   2. 스터디
      1. **XAI도 기본 개념은 쭉 정리해서 가는게 좋겠다**



### 실제 프로젝트 내용

#### **공동의 목표 달성을 위한 협업 : PPT**

**내가 실제로 한 것**

- 아이디어 제안 : 발표가 중요해지는 현대 사회, 그러나 현대인의 삶은 더 바빠지기 때문에 혼자서도 나의 발표를 간단하게 평가하고 피드백 받을 수 있으면 좋겠다!
- 기본 로직 제안
  - 동영상을 일정 텀을 두고 스냅샷을 찍은 다음, 각 스냅샷에 대해 Pose Estimation과 Face Emotion Recognition을 한다.
  - 각 snapshot에 대해 feedback을 제공한다
  - **전체 snapshot의 데이터들을 통계적으로 분석하여 피드백을 제공**
    - 각 랜드마크의 이동 분포가 너무 작다 : 각 부위의 움직임이 너무 적다 : 적극적인 제스처 제안
    - eye detection 횟수 / snapshot 수 : 얼마나 정면을 발표자가 바라보며 하였는가 등
- GCP에 개발 환경을 마련하여(GPU가 부착된 VM을 할당), 딥 러닝 알고리즘을 수행할 수 있는 환경을 구축하였음 (CUDA, cuDNN)
- Django 웹 서버와 연동하기 위해 venv를 활용한 가상 환경 구성 : keras / tensorflow inferrencing / Django Framework 개발 환경과의 conflict 방지
- OpenCV, OpenPose를 활용한 Human Pose Estimation landmark 좌표값 얻음
- Pose 데이터(좌표)를 자체 알고리즘을 활용하여 자세가 올바른 presentation과 적합하지 않다면 각 지표에 대해 조언을 주게 설계
  - 너무 눈을 스크린 쪽만 바라보고 있다 : 청중을 덜 응시하고 있다
  - 허리가 너무 굽어있다
  - 손을 주머니에 꽂고 발표하고 있다
- Kaggle Face Emotion Data를 활용하여 얼굴 표정을 분류하는 모델 학습
  - ResNet51 아키텍쳐로 학습, val acc 기준 55% 달성
  - positive / negative / neutral 3가지 분류로 나누어서 통계적으로 분석, 피드백 제공

**위 프로젝트를 통해 내가 얻은 것**

- Openpose / OpenCV 등의 오픈소스 사용에 익숙해짐
- 딥 러닝 학습을 위한 클라우드 환경 구축을 직접 다 해봄, 익숙해짐
- 이미지 분류 모델 구축 : 얼굴 감성 분석

**실제로 얻을 수 있었던 결과 & opensource 사용**

- Openpose를 이용해 얻은 face / body landmark 좌표를 활용해 발표시 자세 피드백
- 점수 및 세부 알고리즘은 원래 목표는 End - to - End 딥 러닝으로 구현해보려 했으나(Sequential), 데이터가 부족하고 오버피팅이 날 우려가 너무 커서(완전 이유를 잘 알 수 없는 블랙박스 모델) handcraft 알고리즘으로 작성함
- 그리고 이를 python Django framework를 이용해 웹에 게시

**실제로 적용할 수 있는 부분**

- 프레젠테이션을 준비함에 있어 본인은 나름대로 열심히 준비했다고 하지만 막상 발표를 보면 준비가 제대로 되어있지 않은 경우가 많다.
- 이 프로그램을 통해 중요한 발표가 있을 경우 시선처리, 제스쳐, 자세 등 다양한 비언어적 요소를 피드백 받고, '반복적으로' 연습할 수 있는 부분이 상당히 매력적인 부분이다.

**현존 프로그램**

- 비슷한 현존 프로그램으로는 'AI 면접' 소프트웨어를 들 수 있는데 그것은 채점, 즉 '시험'에만 한정적으로 쓰이는 제품임
- 그에 비해 PPT는 특정 시기에만 사용되는 것이 아니라 범용적으로 어디에서나, 다양한 목적을 위해 사용될 수 있으므로 타겟 고객층과 활용 범위가 더 넓다.



#### 성경말투 따라잡기

**내가 실제로 한 것**

sequential data를 다루는 모델에 관심이 많아졌고, 이를 활용하여 실제 프로그램을 한번 만들고 싶었다.

- 그런데 챗봇같은 경우는 너무 복잡한 handcraft한 로직이 많아야 실제 그래도 볼만한 결과를 가져올 수 있었다.
- 또한 단순한 번역모델은 현재 transformer / bert 등 어마어마하게 성능 좋은 pre-trained 모델이 있어서 남들이 해 놓은 것을 또 하는게 가치가 있을 까 싶었다.
- 그래서 생각해낸 것이 바로 '성경 말투 따라잡기 모델'

성경은 우리나라에 들어오며 수차례 번역되었다. => 그 중 현대어 버전과 꽤나 오래 전에 쓰여진 고어(옛 화법)로 쓰여진 버전이 있다.

이것을 현대어 버전을 input data로 하고, 고어 버전을 output data로 하는 sequential 한 model을 학습시킨다면, OOV 같은 경우만 잘 처리해줘도 '말투'를 따라할 수 있는 모델을 만들 수 있을 것이라는 생각에서 시작하였다.

일단 시작은 '재미있을 것이다!'라는 생각에서 비롯되었다.

**기본 로직** 

- Keras, tensorflow를 활용한 sequential 모델이다. 현재까지 v2까지 제작되었고, v3는 현재 개발 중이다. 
- v1 : keras 기반의 naive seq2seq 모델 : 이전에 영어 - 프랑스어 번역기 모델을 제작하면서 만들어 놓은 모델 아키텍쳐가 있어서 빠르게 테스팅 하기 위해 사용하였다. 하지만 그렇게 결과가 좋지는 않았다.
- v2 : attention 메커니즘을 활용한 seq2seq 모델 : 기계 번역 큰 반향을 주었던 attention 메커니즘을 활용하면 더 잘 말투를 따라할 수 있을 것이라 생각해서 구현했다. 결과가 더 자연스러워졌다.
- v3 : transformer 모델을 사용하여 구축할 예정이다. 사실 그냥 빠르게 남들이 만들어놓은 코드를 짜집기 해서 만들 수는 있을 것 같지만 중요 개념들을 대충 이해한 상태로 구현하는 것이 의미가 없을 것 같아 잠시 미뤄두었다. 이번 바쁜 시즌이 끝나면 다시 시도할 예정이다.

**위 프로젝트를 통해 내가 얻은 것**

역시나 자연어 처리 관련 프로젝트는 **전처리의 상태**가 모델의 성능에 아주 큰 영향을 미치는 것을 알게 되었던 시간이었다.

맨 처음에는 단순히 영어 - 프랑스어 번역 모델을 만들었던 기억이 나서 keras의 기본 tokenizer를 사용하여 한글을 분류하였더니, 당연하지만 말도 안되는 결과들만 나왔던 경험이 있다.

그래서 konlpy 패키지의 okt 형태소분석기를 활용하고, stopword들을 제거한 후에야 비로소 의미 있는 결과를 얻을 수 있었다. 현재는 khaiii라는 카카오 형태소 분석기를 알게 되었고, 이를 활용하여 v3에는 적용할 예정이다. 또한, OOV의 처리가 매끄럽지 않아 모델의 성능을 떨어뜨리는 상황이 있음을 발견하였고, 이를 추후 개선할 예정이다.

**실제로 얻을 수 있던 결과**

적당한 수준의 말투를 따라할 수 있는 모델이 생성되었고, 이를 현재 배우고 있는 Django 프레임워크 기반 웹에 간단히 올려볼까 생각중이다. GCP 크레딧이 남아서 저렴한 VM으로 서버 만들고!

혹자는 물론 아주 간단한 토이 프로젝트다! 그냥 장난이다! 할 수도 있겠지만, 예전에 '나의 전생은..?'이라는 웹 기반 서비스를 기억하는 분이 있으실 것이라 생각한다. '신이 당신을 만들었을 때... 재능 한 스푼, 뭔 스푼' 등등. 사실 이러한 서비스에 큰 기술력은 들어가지 않았다. 간단한 무작위 추출 랜덤 알고리즘과 웹 기술 뿐이었다. 하지만, '재미있게 상품화'하는데 성공했고, 결국 SNS에서 유명세를 타며 이익 창출에 성공했다.

이처럼 성경말투 따라잡기 모델도 실제로 잘 포장하여 서비스를 진행한다면 이익을 창출 할 수 있는 결과를 얻을 수 있지 않을까 기대한다.

**현존 프로그램**

v3로 업그레이드 하려고 계획하고 있던 도중, tf-kr에 나와 똑같은 생각을 하신 분의 게시물이 올라왔다. 역시 사람 생각은 거기서 거기인가... 하는 생각도 들고, 아! 내가 먼저 말했으면 내꺼였는데 하는 아쉬움도 들었다. 하지만 가서 오픈소스 코드를 확인하니 transformer 기반으로 제작된 나보다 더 정교하게 잘 짜진 프로그램이란 것을 확인하였다. 앞으로 참고해서 공부할 예정이고, 남이 했던 것은 사실 재미가 그렇게 있지는 않아서 다른 pair data가 있는 소설이 있다면 참고해서 학습해볼까 계획중이다.



### R 아마존 감성 댓글

affin : 긍정 부정 -5~+5

bing : 그냥 positive negative

댓글 크롤러를 작성하여 각 댓글을 단어별로 전처리 한 다음 각 댓글의 감성을 분석했고, 그 내용을 휴대폰 제품별로 통계내어서 보여주었음.

판매량과 비슷한 추이의 그래프를 내서 꽤나 흥미로웠고, 폭발했던 갤럭시 노트 7 기종이 의외로 호평을 받아서 의외였다.

# 인성 면접

안녕하세요, 저는 분석하고 공부할때 즐거운 예비 신입사원 강태형입니다. 저는 이자리까지 오기 위해서 다양한 방법으로 제 역량을 향상시켜 왔습니다.

첫번째, 공모전입니다. 빅 콘테스트, 앨리스 데이터 챌린지 등의 공모전에 참가하면서 실전 데이터 분석 역량을 기르고, AI 기술을 적용해보았습니다.

두번째, 프로젝트입니다. 저는 관련 교육과정에서 AI 기술과 프로그래밍 기술을 활용하여 관련 프로젝트들을 수행하였습니다. (PPT / 성경말투 따라잡기)

세번째, 스터디입니다. kaggle, AI 논문 읽기, spark, explainable AI 등 다양한 스터디에 참여하며 제 역량의 범위를 넓혔습니다.

이제, 대한민국 IT를 선도하는 sk cnc에서 일하며 회사에 발전에 기여하고, 저 또한 성장하는 기회를 가지고 싶습니다. 감사합니다.