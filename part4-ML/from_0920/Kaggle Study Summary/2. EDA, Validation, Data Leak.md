10/14

## Week 2

- EDA
  - what?
    - 문제 이해 / 데이터 이해 / comfortable
    - intuition= > hypothesis => insight : magic feature
    - 통찰 + EDA + 모델링 >>>>>>> just stacking and put them into the model
    - 모델링과 EDA는 물론 다름 / but EDA로 제대로 데이터를 파악해야 함!
    - 시각화 => 패턴을 볼 수 있다! : 패턴으로 어떤 모델이 좋을지 확인하라!
  - why?
    - 심지어 데이터를 아주 잘 이해하면 : 모델링을 할 필요가 없다!
  - Buliding intuition
    - 도메인 지식 : 문제 이해
    - 데이터가 intuitive 한지 확인 : 데이터가 적절? : Age column => 뭔가 다른 의미다
    - 어떻게 데이터가 생성되었는지 이해
      - **적절한 validation scheme 만들기 위해**
      - train set과 test set이 다른 알고리즘으로 생성되었을 수 있음 : 분포가 다르다!
    - 데이터 에러가 생긴거에는 나름의 logic이 있을 수 있다 => is_errored column 생성가능
  - Exploring anoymized data
    - what is anonymize data?
      -  보안 측면에서 column name 바꾸고, 암호화 함 => 제대로 된 분석 위해 의미 파악은 필수(in legal way)
      - Guess the meaning of the columns
      - Guess the types of the column : feature relation
    - what can we do
      - 숫자들 분석 => scaling parameter 찾아서 나눠본다 => denomalization : 제대로 된 분석 위해 원래 값으로 복원!
      - 복원된 값들을 쭉 - 나열해서 다른 단서를 찾아본다 : 더하다 빼다 하면 힌트가 보임
      - 근데 완전한 재복원은 말처럼 쉽지가 않다 : 일단 data type : numeric or categorical, text 등등 구분하는게 먼저임!
      - **Try to decode the features**
        - **Guess the true meaning of the features**
      - **Guess the feature types**
        - **Each type needs its own preprocessing**
    - Visualization
      - Never make a conclusion based on a single plot : 다른 여러개
      - Explore inidividual featrues
        - Histograms
          - logarithm 
        - Plots
        - Statistics
      - Expore feature relations
        - 많은 feature를 한꺼번에 보는게 힘들다 => pair로 봐라
          - 색깔은 class 
        - Scatter plots
          - train class 1, 2 / test를 찍어봤을 때, train과 멀리 떨어져 있는 test의 분포가 많다면 고민해 봐야 함
          - histogram보다 더 좋을 수도 : 관계적 측면 : 두 feature가 어떤 관계를 가지고 있는지 파악 : 어디에 뭉쳐있더라 / 관계식을 도출할 수 있다더라 => score에 도움이 되는 => new feature generation!
        - Correlation plots
          - 너무 복잡하다면 clustering : k-means 등 사용 가능
        - Plot (index vs featrue statistics)
          - 분석 후 grouping 가능
        - **Pair => scatter plot / matrix, corrplot**
        - **Groups => corrplot + clustering, Plot(index vs feature statistics)**
  - Things to explore
  - dataset cleansing
    - data는 언제나 fraction만 주거나 문제가 있을 수 있음
    - 전부 같은 값을 가진 column => remove
    - train에는 없고 test에만 있는 category : col 제거하던가 아니면 특별한 모델 따로제작
    - 동일한 내용 컬럼 => remove
    - 내용은 동일하나 기호가 다르게 표기된 컬럼들 => 나오는 순서대로 label encoding 한 다음에 분석하면 확인 가능
    - 데이터가 shuffle되어 있을 수 있다 : 시계열에서?
    - 중복된 row가 많을 때 ?
  - **EDA CHECK LIST**
    - **Get Domain Knowledge**
    - **Check if the data is intuitive** **=> 직관적으로 옳은가?**
    - **Understand how the data was generated**
    - **Explore individual features**
    - **Explore pairs and groups**
    - **Clean features up**
    - **Check for leaks!**
  - ETC
    - horizontal line이 보이면 discrete values : categorical / count values
    - data는 clear해야 좋은 모델! => jittering is bad for model
  - Kaggle competition EDA
    - Springleaf EDA
      - shape of train / test table
      - isnull().sum() : row / column
      - determine the type of data
      - remove the duplicated features
      - nunique() / dropna() => historgram
      - integer number => count or category
      - select_dtype(include=['object','integer']) : df의 원하는 타입 col만 뽑아줌
      - **heatmap을 찍었을 때 box의 모양이 다양하게 나타나는 이유..?**
      - feature의 value_count를 보고 시각화를 하여 패턴 파악! : 반복되는 숫자가 있지는 않은지?
      - 2 features 시각화 : 두 특징이 관련이 있다면 차나 합으로 새로운 feature를 만들 수 있다! (e.g. : one feature is always larger than the other)
    - Numerai EDA
      - data leak이 있었음 : 언급 안했지만 time-series였음 : reconstruct order
      - knn clustering : column이 time-series였지만 shuffled 되어있던거를 sort 했더니 훨씬 좋은 결과를 가져왔다! => corr matrix
        - feature를 그룹화하여 볼 수 있어서 훨씬 좋다!
        -  consecutiveness : 연속성 : 새로 등장하는 데이터가 완전 새로운 데이터가 아니라, 이전 데이터의 분포 + noise 인 데이터들일 뿐! => 본질적으로 데이터는 변하지 않았다



- **Validation**

  - leaderboard에서 갑자기 competition이 끝난 후 jump해서 들어오는 사람들이 있다

    - validation을 무시하고 public leaderboard만 보고 submission
  
  - train / test가 많이 다르거나, private에 data가 너무 적음
  
    
  
- **validation & overfitting**
  
  - validation : model we train should be applicable to the future data
  
  - 미래의 데이터에 적용 가능해야 함!
  
    - we need to correctly understand the quality of our model!
  
    - 모델의 품질을 측정/이해할 수 있어야 함
  
    - The model could just memorize all patients from the train data and be 
  
      completely useless on the test data because we don't want this to happen.
  
      - 모델이 패턴을 찾아내야지, 정보를 그냥 memorize하게 되면 안됨
      
  
      
  
    => Validation을 잘 하자
  
    
  
    - train / val 나눔
    - ![1571145937265](..\Kaggle Study Summary\val)
  
  
  
- number of splits to esatblish stable validation : 몇등분으로 쪼개야 안정적인 validation이 될까
  
  - train / test split methods : 어떻게 나눠야 좋을까
  
- 물론 public leaderboard의 점수를 보고 모델을 수정해나가면 점수는 올라갈 지 모르지만, private leaderboard에서는 떨어질 것! => overfitting
  
  - 즉, train과 test를 아우르는 generalized model을 만들어야 함 => general pattern을 잘 학습해서
  
    
  
  - underfitting : model is too simple => can't capture important relationship
  
  - ![1571146183906](..\Kaggle Study Summary\underfitting)
  
    
  
  - overfitting : too complicated model => describing noise! : X generalized data
  
  - ![1571146252595](..\Kaggle Study Summary\overfitting)
  
  - 일반적 overfitting과 competition의 overfitting의 의미는 조금 다름
  
    - 일반 : validation acc < train acc
    - competition  : validation score is okay => but low model's quality on test data
  
  - underfitting과 overfitting 사이에서 적절한 복잡도의 모델을 찾아야 함



- **Validation Strateies**

  - **holdout**

    - 여기서는 part A / B 두개로 나눠 B로 모델 성능 체크
    - train - val - test split 3개로 나눔 (겹치지 않게)
    - 데이터를 나눌 때 잘못 나누면 모델 성능에 치명적일 수 있음 : holdout set 만들 때 신경써야 함
    - dataset 별로 안크면 분할이 치명적일 수 있음

  - **k-fold**

    - want to use all sample data for validation only onces
    - 좀더 강인 : stratified하게 할 수도 있음
    - 시간이 오래 거림 : k times

  - **leave-one-out**

    - sample 수 만큼 모델 만들고 : 각 경우의 수마다 1개씩 셈플 제외하고, 그 sample로 test : test들의 평균으로 performance 측정
    - 결과 stable, but 시간 많이 걸리고 모델의 다양성은 포함하기 어려움

  - **stratification**

    - small dataset / unbalanced dataset 

    

- **Data Splitting Strategies**

  - 일단 test set을 쓸 수 있게 분리해야 함 : 훈련된 모델로 test 해야하니까

  - time series에서는 train / val 어떻게 분리?

  - Previous and next target values 

  - Time based trend

  - ![1571151439214](..\Kaggle Study Summary\data splitting strategies)

  -  If we carefully generate features that are drawing attention to time-based patterns, will we get a reliable validation with a random-based split? => No

  -  시간 기반 패턴에 집중해서 피쳐를 만들다보면 => random based split validation에서는 신뢰하기 힘들다.

  - splitting strategies 간의 차이점

    - in generated features (생성된 features) : 피쳐들이 다름
    - in a way the model will rely on that features : 모델이 피쳐를 사용하는 법이 다름
    - in some kind of target leak : leak? 
      - e.g. 같은 사람은 train or val 한쪽에 몰아넣어야 한다 => Group k-fold
      - 일반적으로는 domain knowledge로 분류하는 것이 일반적
      - 

  - splitting ways

    - Random / rowwise

      - row들이 independent할 때 : dependent하면 문제 있을 수 있음!

    - Timewise

      - 말그대로 시간순으로!
      - moving window로 할 수도 있음
      - ![1571152162262](..\Kaggle Study Summary\moving window)

    - By ID

      - split by unique ID : hidden ID column 찾아서 해야 할 때도 있음

    - Combination

      - 여러개 섞어서 쓰기 : user ID + search engine ID 등

    - 데이터에 따라서 잘 split 해야 함!

    - wrapping : 대회 주최자들에 의해 만들어진 train / test set을 무조건 모방해야 한다는 생각은 버리는게 낫다

      - 만약 test set 에서 train / valid 에서 보지 못한 데이터가 등장할 경우
      - **Set up your validation to mimic the train / test split of the competition => train / val의 분포를 train / test의 분포를 비슷하게 흉내내도록 split 해야 함!**

      

- Problems occuring during validation

  - Validation stage : inconsistency of the data : 데이터가 일관성이 없다
    - cause of different scores and optimal params
      - too little data / too diverse & inconsistent data
    - solution
      - 평균 score from different Kfold splits(5)
      - 하나의 model만 tuning, 나머지는 just for scoring
  - Submission stage : validation과 leaderboard score의 gap이 너무 크다!
    - EDA 잘 해라
    - LB score가 val score랑 uncorrelated
    - => 큰 문제이다! : train / test 를 잘 mimic해야 함 : But it is difficult!
    - => competition에 참여하자마자 빠르게 submission 해봐라 : 문제 파악을 빠르게 해야 함!
    - cause of problem
      - too little data in public LB / train, test data are from different distributions
      - ![1571154703289](..\Kaggle Study Summary\diff_dist)
      - man / woman이 각각 train / test라고 생각한다면 train으로 학습된 model은 좋은 결과를 가져오지 못할 가능성이 높음
    - How to solve it?
      - figure out the optimal constant prediction for train and test data => shift predictions by the difference
      - => 최적의 상수 찾아서, 예측치를 shift!
      - leaderboard probing : 그런데 이런 경우는 Rare함
      - ![1571155233382](..\Kaggle Study Summary\sub)
      - 위와 같은 경우가 훨씬 흔함
      - ![1571155298513](..\Kaggle Study Summary\train_test_val)
      - 이런식으로 validation의 분포를 맞춰주어라
      - 결국 이러한 좋은 CV setting을 만들기 위해서 => LB에 제출해서 비교!
      - 지진 데이터
  - leaderboard shuffle
    - randomness
    - little little data
    - different distribution : e.g. time-series
  - Validation을 잘 나눠서 훈련한 후, 믿어라!



- Summary
  - ![1571155804261](..\Kaggle Study Summary\summary_2)
  - ![1571155881337](..\Kaggle Study Summary\summary_2_2)



- Data leak : 캐글에서 통하는 꼼수의 예
  - time-series : 시간순 아닌 패턴을 보임 : 힌트일 수 있다!
  - meta data : file 저장한 시간 / 개 찍은 카메라 종류 / 고양이 찍은 카메라 종류
  - information in IDs : ID가 target에게 군집을 이룸
  - row order
  - LB probing
    - category ~ ID 관계가 있을 수 있다!



- Data Leakage Assignment
  
- References:
  -  https://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html  : demo of the spectiral biclustering algorithm
  -  http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/ 