# FINAL ESSENCE : FOR SK CNC

### 0-0. 1분 자기소개

안녕하세요, 저는 분석하고 공부할때 즐거운 예비 신입사원 강태형입니다. 저는 이자리까지 오기 위해서 다양한 방법으로 제 역량을 향상시켜 왔습니다.

첫번째, 공모전입니다. 빅 콘테스트, 앨리스 데이터 챌린지 등의 공모전에 참가하면서 실전 데이터 분석 역량을 기르고, AI 기술을 적용해보았습니다.

두번째, 프로젝트입니다. 저는 관련 교육과정에서 AI 기술과 프로그래밍 기술을 활용하여 관련 프로젝트들을 수행하였습니다. (PPT / 성경말투 따라잡기)

세번째, 스터디입니다. kaggle, AI 논문 읽기, spark, explainable AI 등 다양한 스터디에 참여하며 제 역량의 범위를 넓혔습니다.

이제, 대한민국 IT를 선도하는 sk cnc에서 일하며 회사에 발전에 기여하고, 저 또한 성장하는 기회를 가지고 싶습니다. 감사합니다.



### 0-1. 지원 동기

- **데이터를 분석하고, 그 안에서 인사이트를 찾아내는데서 짜릿함을 느끼기 때문입니다**. 지금까지는 경험적으로 얻어진 통찰로 의사 결정을 해왔다면, 세상이 진짜 복잡해진 지금, 이제는 **데이터 기반 의사 결정의 시대**가 왔다고 생각합니다.
- 물론 단박에 찾아지지 않는 경우도 참 많고, 무수한 반복작업에 시달려야 할 때도 있습니다. 하지만, 그 과정들이 소중하고 즐겁습니다. 또한 다양한 도메인들의 데이터를 공부할 때는 더욱 그렇습니다. **제가 아는 범위가 넓어지는 것이기 때문입니다.** 그리고 그것이 제가 SK C&C에 지원한 이유기도 합니다. 다양한 위치에서 다양한 데이터를 분석하며, 회사와 사회에 가치 있는 일을 하고, 저 자신도 더욱 성장하고 싶습니다



### 1. MNIST Fashion dataset

ResNet

- **skip connection**을 이용한 네트워크
- 네트워크가 깊어지면서 생겼던 vanishing / exploding gradient 문제를 해결한 네트워크.
  - 이전까지는 그냥 네트워크를 깊게 만들면 back-prop을 할 때 갱신되는 weight가 점점 작아져서 학습속도가 엄청나게 느려지거나 학습이 잘 되지 않음
- residual block의 입력에서 출력으로 바로 elementwise하게 합을 해주는 shortcut 연결을 해주게 됨으로서, **입력의 작은 움직임도 더 잘 파악해서 학습을 잘 되게 함**
- 중간의 3x3 conv를 1x1 3x3 1x1로 연산하여 연산량을 줄임
- act. fuc와 BN의 위치에 따라서 residual block의 성능이 크게 변화 => 수정
  - 하나의 residual unit 안에 BN-ReLU-conv-BN-ReLU-conv 의 과정이 숨어있고 이를 쌓음
- 연산량 보존을 위해 크기가 줄어들면 필터 수를 늘림
- skip connection을 통해 gradient가 보존되어 흐르기 때문에 vanishing이 획기적으로 막아짐!

GoogleNet

- Inception module을 사용한 네트워크
  - inception module : 다양한 feature를 추출하기 위해 여러개의 conv를 병렬적으로 활용 / 다양한 scale의 feature 추출 가능 : 연산량이 커 bottleneck 구조 사용

- 결국 conv network의 성능을 향상시키는 방법은 직접적으로 network의 크기를 늘리는 것
- 근데 그냥 늘리면 파라미터 수 너무 많아지고, 깊어져서 overfitting / vanishing gradient

- 맨 마지막 단에 그냥 average pooling

ResNet-GoogleNet

- ResNet의 skip connection을 GoogleNet의 앞 뒤에다가 달아줌

Yolo : 요즘에 관심이 있다

- 빠르지만, 정확도는 조금 낮음 : 특히 몰려있으면
- 그러나 class 대한 일반적 이해도는 높음 : 다른데 써도 잘 됨

- 앞쪽 layer에서 googleNet / conv / fc layer를 거쳐서 7 x 7 x 30 의 아웃풋을 얻게 됨
  - 30의 앞의 10개는 x,y,w,h,c 두 쌍이고 나머지 20개는 class 확률
- 이게 input image를 7 x 7 그리드로 나누고, 그 그리드마다 최대 2개의 물체를 검출하게 됨
- 그리고 IoU 계산으로 조건에 부합하지 못하는 bounding box는 드롭!
- grid의 갯수가 한정되어 있기 때문에, 한정된 갯수만 검출할 수 있고, 되게 작고 많고 그러면 검출 잘 못함
- 그러나 일반화한 검출은 잘함, 그리고 빠름 : 막 그림 안에 있는 물체도 검출하고 그럼



### 2. 기존의 틀 깨는 과감한 실행

- 일단 그냥 진짜 돈을 보내주거나, 식량을 보내주는 것이 아닌, '식량을 만드는 시설'을 우리가 직접 만들어서 보내줄 수 있었다는 점이 상당히 기존의 틀을 깨는, 새로운 도전이었다.
- 양배추, 양상추, 토마토의 수확을 올렸다
- 핸드펌프 사용
- 액체 비료를 사용함
- 솜을 사용
- 가장 중요했던 부분은 외국인 친구들과 영어로 커뮤니케이션하면서 타협점을 만들면서 좋은 결과를 냈다는 것이라고 생각합니다.
- 미국에서 기본적인 수경재배 방법은 전기를 이용하는 모터를 사용하는 것으로 고정관념이 잡혀 있었는데, 이를 간단히 포기하고 차선책을 빠르게 선택한 것이 참 좋았던 부분이라 생각합니다.



### 2-1. 창의적인 경험

- sequential data를 다루는 모델에 관심이 많아졌고, 이를 활용하여 실제 프로그램을 한번 만들고 싶었다.

- 그런데 챗봇같은 경우는 너무 복잡한 handcraft한 로직이 많아야 실제 그래도 볼만한 결과를 가져올 수 있었다.
- 그래서 생각해낸 것이 바로 '성경 말투 따라잡기 모델'

- seq2seq : 인코더 - 디코더 두 파트로 이루어져 있으며, 입력 시퀀스를 전달받아 context vector를 만들고, decoder에서 이를 auto-regressive하게 출력해내는 방식
- seq2seq with attention
  - 디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서 전체 입력 문장을 다시한번 참고!
  - 해당 시점에서 예측할 단어와 연관이 있는 입력 단어부분을 좀더 attention을 주어 보게 됨
  - 즉, 각 스텝마다 인코더의 히든 벡터를 얼만큼 이용할 지 어텐션 계수를 통해서 정하고, 이 어텐션 계수는 학습됩니다.

### 3. 빅 콘테스트

- 리니지 게임 데이터를 이용한 이탈 유저 예측 및 평균 결제금액 예측

- 내가 한것 : **만들고 / 합치고 / 채워넣고 / 분석**
  - **넓게 펴져있는 피쳐들을 pivot / crosstab 등을 통해서 합치고 새로 만들어냄**
  - **primary key 값이 user id여서 그것을 기준으로** **다양한 테이블 하나로 합침**
    - 고충이 조금 있었다
  - **null data implementation** : group / id 별 평균 / knn
    - KNN vs Kmeans
      - KNN은 지도, Kmeans는 비지도 학습'
      - knn : 현재 위치에서 가장 가까운 k개 찾고, 그놈에게 속해
      - kmeans : 군집화를 위해서 centroid를 뿌리고, 개별 점들은 그 가까운 centroid에 소속되고, centroid는 그 점들의 중간으로 이동 이동 ==> 이동 안되는 순간 멈춤
      - kmeans는 조금 더 이상치에 영향을 많이 받고 불안정하다고 알고 있음
  - **data의 feature를 하나하나 시각화 하여 보려 함 : 인사이트를 찾으려 함**
- 부족했던 점
  - 너무 모델링(LSTM)에만 치중하여 더 중요한 데이터를 등한시 / 부스팅,배깅 머신러닝 알고리즘이 더 잘될 때도 많다.
  - **내 작업에만 너무 집중하다보니 남이 이미 해놓은 것을 제대로 보지 못했다.**
- 내가 얻은 부분
  - 게임 데이터라서 꽤 재미있게 분석했다, 데이터가 나름대로 정규화 되어있었는데, 이를 역정규화 하는 과정에서 약 0.065%의 사람들이 전체 11%이상의 매출을 내고 있는 것을 보며 게임이 참 위험하다 : 그래서 이렇게 되어서 먼저 knn으로 군집화를 시킨 다음 이중으로 **메타 학습**을 시켜보려 했으나 잘 되지 않음
  - **데이터를 다루는 것에 익숙해짐** : 로드하고, 헤드 찍어보고, 머지하고, 아웃라이어 걸러내고, 피봇 크로스탭
  - **시각화**또한 익숙해짐 : 피쳐들의 분포 눈으로 확인하고 corrmat heatmap 찍어보고
  - 나름대로 최선을 다하고 굉장한 노력을 들였지만, 결과가 잘 안나와서 마음이 아팠다. **그러나 어떻게든 분석해보려고 다각도에서 접근했던 노력이 데이터 분석의 나의 시야를 넓혀주었고, 파이썬 언어와 각 라이브러리와 친숙**해진 계기가 되었다.
- 실패한 이유, 남들은 어떻게 했더라
  - 이 문제가 점수를 내는 방법이 그냥 정확도가 아니라 이탈 여부와 평균 결제 금액을 이용한 식으로 계산한 '잔존 가치' 계산이었다. 
  - 그러나 나는 너무 정확도에만 의존하였고, 실제 적용되는 **메트릭**에 대한 연구가 적었던 점이 아쉽다. 실제로 예측을 그냥 전부 이탈! 하고 결제 금액을 특정 금액으로 고정하면 점수가 잘 나온다고 하던데 : 이렇게하면 그냥 전부 이탈유저로 보고 전부 프로모션 때려버리면 일단 최소한의 기대이익은 어느정도 보장할 수 있는 메트릭인가 하는 생각이 들었다.
- 그래서 나의 태도
  - 캐글 스터디에 참여 시작 : 기본 competition에 참여
  - 그리고 좀더 갈고 닦아 Elice data challenge에서 10위를 달성
    - 가시거리 예측(시계열 분석) / 금융상품 가입(binary)
    - 구매자 결제 내역 기반으로 직업 예측 / 와인 평가점수 예측 :  와인 평가내역(문서) + 와인 특성 고려 
      - tf-idf 행렬 / 각 문서에서 중요한 단어를 점수화해주는 행렬 : 특정 각 문서에서'만' 자주 등장할 수록 점수가 높다 => 문서를 대표하는 단어!
    - xgboost / lightgbm / random forest의 앙상블 방법을 주로 사용
    - 시계열 데이터는 lstm 쓸 걸 하는 생각 듬
    - 입상 비결
      - **집요함** :  column들과 feature들을 분석했다. 히트맵을 찍어보면서 trial-error 방식의 테스트도 해보고, 피쳐를 새롭게 만들어보며 최선을 다했다.
      - **마지막까지 포기하지 않음**
      - **같이 공부하던 형과 인사이트를 나눔**



### 4. 챗봇 프로젝트 => 성경말투 따라잡기 모델

- sequential data를 다루는 모델에 관심이 많아졌고, 이를 활용하여 실제 프로그램을 한번 만들고 싶었다.

- 그런데 챗봇같은 경우는 너무 복잡한 handcraft한 로직이 많아야 실제 그래도 볼만한 결과를 가져올 수 있었다.
- 그래서 생각해낸 것이 바로 '성경 말투 따라잡기 모델'

- seq2seq : 인코더 - 디코더 두 파트로 이루어져 있으며, 입력 시퀀스를 전달받아 context vector를 만들고, decoder에서 이를 auto-regressive하게 출력해내는 방식
- seq2seq with attention : 디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서 전체 입력 문장을 다시한번 참고! : 해당 시점에서 예측할 단어와 연관이 있는 입력 단어부분을 좀더 attention을 주어 보게 됨
  - 즉, 각 스텝마다 인코더의 히든 벡터를 얼만큼 이용할 지 어텐션 계수를 통해서 정하고, 이 어텐션 계수는 학습됩니다.
- 내가 얻은 인사이트
  - 전처리의 상태가 모델에 큰 영향을 끼친다
  - stopword 제거는 필수, 형태소 분석기 사용 (konlpy okt) => khaiii 형태소 분석기 사용할 예정, OOV 처리
- 실제로 얻은 결과
  - 토이 프로젝트일 수도 있지만 : 예전에 '신이 당신을 만들 때...' 처럼 쉬운 알고리즘의 서비스도 재미있게 상품화 하는 게 중요하다고 생각 => 결국 SNS에서 유명세 타며 이익 창출
  - 위 프로젝트로 그러한 일이 가능하지 않을까 기대함
- 현존 프로그램
  - transformer 기반으로 잘 만들어서 올려놨다. 이걸로 공부해서 구현해보려 한다.
  - 데이터를 찾을수만 있으면 사투리 번역기로 만들어서 한번 해보고 싶은 생각도 있다.
    - 그런게 재밌지 않나?
- Transformer network에 대한 설명
  - 아까 설명했던 attention을 사용했던 RNN 네트워크가 여전히 벡터 차원도 그렇고 일단 auto-regressive하기 때문에 여러 문제가 많았다.
  - 그래서 그냥 Feed-forward 한 형태로 만들었다!
    - 셀프 어텐션을 이용하여 피드포워드에서도 잘 학습되게 만듬
      - 지금까지는 word2Vec 같은 걸 만들면 일단 좌표공간에 박긴 박는데 그게 문맥을 따라가지는 않았음!
      - 셀프 어텐션 : softmax(QK) * V : 이렇게 함으로써 각 문장과 문맥에 관련 있는 것으로 압축된 벡터가 되면서 더 성능을 높일 수 있게 됨!
    - auto-regressive 아니어서 굉장히 빠르다!



### 5. Spark Study

- 클러스터에서 셔플을 덜 일어나게 하는 쿼리 : 넓은 트랜스포메이션을 가능한 적게 하는 쿼리 : 즉, 넓게 뿌려져 있는 데이터를 다 가져와서 모으는 그런 것 : 물론 해야하긴 하지만 너무 많이하면 느려진다

- 기본 구조 : 클러스터 매니저에 익스큐터와 드라이버 프로세스가 연결되어 있고, 이 드라이버 프로세스에 명령을 내려 익스큐터들을 컨트롤

  보통 하둡 프레임워크와 결합될 때 더 성능이 좋다

  hadoop과의 가장 큰 차이점 : in memory 연산이 가능하기 때문에 훨씬 빠른 속도로 데이터를 읽어올 수 있다.

  기본적으로 연습은 pyspark를 사용했음 : 파이썬을 주 언어로 사용했기 때문에 편해서

  기본 구조 : 나는 고수준 API로 연습했다 :  Data source(Hive, hadoop)으로 데이터 호출 => 여러 트랜스포메이션을 거치면서 플랜을 짬 => 이를 action으로 한번에 처리 => spark job을 내부 익스큐터들의 상태에서 가장 이상적이게 배분해서 실행



### 6. 슬로우페이퍼 스터디

- 처음에는 영어와 수식으로 뒤범벅된 논문을 읽는다는 사실 자체가 큰 **두려움**이었다
- 그리고 실제로 읽었을 때 잘 이해하지 못하고, 자괴감이 생기기도 했다, 그러나 **포기하기는 싫었**다
- 배경 지식을 이해하기 위해 미리 선정된 논문의 배경 지식을 알려주는 논문을 읽고 갔으며, 필요하다면 논문을 보다 쉽게 풀이해놓은 **블로그 글을 참고한** 후 참가했다
- **질문**에 주저하지 않았다 : 모르는 것 그냥 넘어가면 그대로 끝까지 모른다!
- 현재의 작음에 실망하지 말고, 오늘 **'keep-going'하자는 마인드**로 도전했다
- 가장 최근에 읽었던 논문은 뭔가? : Weight Agnostic Neural Network



### 7. 아마존 댓글 감성 분석

affin : 긍정 부정 -5~+5

bing : 그냥 positive negative

댓글 크롤러를 작성하여 각 댓글을 단어별로 전처리 한 다음 각 댓글의 감성을 분석했고, 그 내용을 휴대폰 제품별로 통계내어서 보여주었음.

판매량과 비슷한 추이의 그래프를 내서 꽤나 흥미로웠고, 폭발했던 갤럭시 노트 7 기종이 의외로 호평을 받아서 의외였다.



### 8. 도전적인 패기

- 빅 콘테스트 참가
- 다양한 스터디에 누군가는 부족하다 말할 수 있지만 일단 참가했다
  - DeepLab 논문반 : 모두의 연구소
    - Bert
      - 사전학습과 transfer learning으로 크게 발전
    - ALBert
      - embedding을 factorization
      - layer parameter sharing
    - on-device neural net with moblie GPU
  - 슬로우페이퍼 : 논문 읽기 스터디
    - Weight agnostic NN
      - weight에 별로 상관 없는 인공신경망
      - 현재까지의 concept는 모델의 가중치를 학습시켜서 좋은 결과를 낳은 반면, 이제는 모델의 structure를 학습시켜서 만들면, 가중치에 큰 영향을 받지 않는 네트워크를 만들 수 있을 것이라는 내용
      - 작은 네트워크의 집합을 만들고 => 그것들을 특정한 알고리즘으로 랭크를 매긴 후, 좋은 애(로스가 낮은)들은 노드를 만들거나, 커넥션을 만들거나, activation function을 바꾸는 등의 처리를 통해 변형하여 이 작업을 반복하는 일종의 유전 알고리즘
      - 이렇게 했더니 이 작업을 그렇게 많이 안해도 Mnist 92% 찍더라
      - 모델 가중치를 학습 안해도 이정도 잘하는건 꽤나 좋지 않은가?
      - 그러나 이게 이런 모델을 풀로 학습시켰을 때 기존보다 좋다거나 훨씬 생산적이라는 보장은 아직 없으나, GAN과 같은 새로운 패러다임을 제시한 아주 도전적인 논문이기 때문에 이어지는 연구가 기대됨
  - AI college : Explainable AI (XAI)
    - 실제의 문제를 해결하자! 라는 컨셉을 가진 수업 : 논문 작성을 목표로 하고 있으며, 현업에서 이 주제에 대해 고민하고 있는 회사가 참가 : 우리는 SIA 가 참가 : 위성사진 detection 하는 스타트업인데, 이게 왜 적인지 아닌지 결정해야할 수도 있으므로 아주 중요한 문제
    - 모델이 내가 원하는대로 학습하는구나 검증 가능
    - 취약한 부분을 더 잘 학습시킬수 있게 모델 수정 가능
    - Attribution method : Heatmap을 찍어보면 어떤 부분이 어떤 결과에 영향을 줬는지 알 수 있음 : 그냥 saliency map은 상당히 부-옇다 : 그래서 CAM - grad-CAM 등등의 더 향상된 map들을 그릴 수 있다.
    - 혹은 backprop하면 됨 : 우리가 regularization을 주면 더 선명한 map을 얻을 수 있다고 제안함 : map이 부한것은 잘 못된것이 아니라 너무 잘 되서 그런것 : 오히려 : 너무 많은 비선형 함수를 지났기 때문에 원래로 돌아오면 구불구불해지듯이!



### 9. 팀워크 관련 질문

- 어떤 프로젝트를 하게 되면, **'giver'와 'taker' 두 종류로** 나누어 진다고 한다. 실제로 빅 콘테스트 공모전을 참가하며, 또한 PPT 팀 프로젝트를 수행하며 그렇게 자연스럽게 되었다. **하지만 나의 마인드는 언제나, 적어도 내가 참여한 프로젝트에서는 'giver'가 되자**는 주의였다. **'giver'로써 프로젝트에 기여할 때, 그 실제 기술들이 다시 나의 기술로 축적**될 것이라 생각했다. 또한 그렇게 'giver'로써 참여해야 내가 내 자신을 그 프로젝트에 '기여하고 참여했다'라고 당당히 말할 수 있다는 생각이 들었다. 실제로, 두 프로젝트를 같이 수행했던 팀원 중 나와 같은 성향을 띈 형과 두 프로젝트의 대부분을 수행했는데 결국 우리 둘이 가장 실력 향상이 컸고, 실제로 현재 가장 좋은 기회를 많이 얻고 있는 상황이다.
- 그럼에도 불구하고 내가 **'taker'들을 미워하거나 아니꼽게 생각하지 않았던 점이 오히려 'giver'로써 공헌하는 데 좋은 마음가짐을 갖게 했다**. 분명히 각자의 사정이 존재했었고, 그 점을 수용했고 억지로 닦달하지 않았다. 그 대신 그들이 더 잘 해낼 수 있는 일을 맡겼다. 특히, 아쉽게도 아무리 해도 코딩 능력은 잘 향상되지 않던 친구가 있었는데 정보 수집력은 좋았다. 그래서 두 프로젝트에서 도메인 지식을 잘 검색하고 추려내어서 공유하는 역할을 시켰더니 잘 해내었다. 각자의 위치에서 능력이 적절히 발휘되어 더 좋은 결과를 낳았다.



### 10. 실제 프로젝트 내용

#### **공동의 목표 달성을 위한 협업 : PPT**

**내가 실제로 한 것**

- 아이디어 제안 : 발표가 중요해지는 현대 사회, 그러나 현대인의 삶은 더 바빠지기 때문에 혼자서도 나의 발표를 간단하게 평가하고 피드백 받을 수 있으면 좋겠다!
- 기본 로직 제안
  - 동영상을 일정 텀을 두고 스냅샷을 찍은 다음, 각 스냅샷에 대해 Pose Estimation과 Face Emotion Recognition을 한다.
  - 각 snapshot에 대해 feedback을 제공한다
  - **전체 snapshot의 데이터들을 통계적으로 분석하여 피드백을 제공**
    - 각 랜드마크의 이동 분포가 너무 작다 : 각 부위의 움직임이 너무 적다 : 적극적인 제스처 제안
    - eye detection 횟수 / snapshot 수 : 얼마나 정면을 발표자가 바라보며 하였는가 등
- GCP에 개발 환경을 마련하여(GPU가 부착된 VM을 할당), 딥 러닝 알고리즘을 수행할 수 있는 환경을 구축하였음 (CUDA, cuDNN)
- Django 웹 서버와 연동하기 위해 venv를 활용한 가상 환경 구성 : keras / tensorflow inferrencing / Django Framework 개발 환경과의 conflict 방지
- OpenCV, OpenPose를 활용한 Human Pose Estimation landmark 좌표값 얻음
- Pose 데이터(좌표)를 자체 알고리즘을 활용하여 자세가 올바른 presentation과 적합하지 않다면 각 지표에 대해 조언을 주게 설계
  - 너무 눈을 스크린 쪽만 바라보고 있다 : 청중을 덜 응시하고 있다
  - 허리가 너무 굽어있다
  - 손을 주머니에 꽂고 발표하고 있다
- Kaggle Face Emotion Data를 활용하여 얼굴 표정을 분류하는 모델 학습
  - ResNet51 아키텍쳐로 학습, val acc 기준 55% 달성
  - positive / negative / neutral 3가지 분류로 나누어서 통계적으로 분석, 피드백 제공

**위 프로젝트를 통해 내가 얻은 것**

- Openpose / OpenCV 등의 오픈소스 사용에 익숙해짐
- 딥 러닝 학습을 위한 클라우드 환경 구축을 직접 다 해봄, 익숙해짐
- 이미지 분류 모델 구축 : 얼굴 감성 분석

**실제로 얻을 수 있었던 결과 & opensource 사용**

- Openpose를 이용해 얻은 face / body landmark 좌표를 활용해 발표시 자세 피드백
- 점수 및 세부 알고리즘은 원래 목표는 End - to - End 딥 러닝으로 구현해보려 했으나(Sequential), 데이터가 부족하고 오버피팅이 날 우려가 너무 커서(완전 이유를 잘 알 수 없는 블랙박스 모델) handcraft 알고리즘으로 작성함
- 그리고 이를 python Django framework를 이용해 웹에 게시

**실제로 적용할 수 있는 부분**

- 프레젠테이션을 준비함에 있어 본인은 나름대로 열심히 준비했다고 하지만 막상 발표를 보면 준비가 제대로 되어있지 않은 경우가 많다.
- 이 프로그램을 통해 중요한 발표가 있을 경우 시선처리, 제스쳐, 자세 등 다양한 비언어적 요소를 피드백 받고, '반복적으로' 연습할 수 있는 부분이 상당히 매력적인 부분이다.

**현존 프로그램**

- 비슷한 현존 프로그램으로는 'AI 면접' 소프트웨어를 들 수 있는데 그것은 채점, 즉 '시험'에만 한정적으로 쓰이는 제품임
- 그에 비해 PPT는 특정 시기에만 사용되는 것이 아니라 범용적으로 어디에서나, 다양한 목적을 위해 사용될 수 있으므로 타겟 고객층과 활용 범위가 더 넓다.



### 부록

### '행복' : 일과 행복을 연결시키는 나만의 생각?

- 일하면서 행복하게 일해야 결국 자발적으로 / 의욕적으로 할 수 있다!
- 일하면서 나의 행복을 어떻게 찾을 수 있을까?
  - 최신 기술이 끊임없이, 빠르게 범람하는 분야로 지원했다고 생각합니다. 누군가는 이러한 새로운 배움이 부담스럽고, 각자의 행복을 방해하는 것으로 생각할수도 있습니다. 그러나 **저는 이러한 새로운 배움이 즐겁고, 그로 인해 더 나은것을 만들어내는 것이 크게 즐겁습니다.** 그 결과 다양한 스터디에 참여하였고, **이러한 일은 제게 부담이 아닌 즐거움**으로 다가왔습니다.
  - 그리고 **행복은 결국 나 스스로 만족하고, 주어진 환경에 감사하며 살아가는데서 오는 것**이라고 생각합니다. 비록 삶이란게 완벽한 것 하나 없고 부족한 것이 많겠지만, 그 자체에 투정하는 것이 아니라 어떻게 하면 더 편리하게 바꿀까, 그렇게 **고민하며 해결해나가는 것이 삶의 또 하나의 재미**라고 생각합니다.
  - 그냥 주어진 일을 기계처럼 받아서 하는 게 아니라, **'이러한 업무를 이렇게 처리하는 것'이 내가 더 행복해지는 일이라는 것을 인지하는 것이 중요**하다고 생각합니다.



### 'Social Value' : 사회적 가치에 기여하라!

- 새로운 광고 캠페인, '업', '짝'만 보아도 숨은 의도를 잘 알수 있습니다.
  - 문제를 문제 그 자체, 스트레스로 받아들이는 것보다, 사회와 기업이 함께 해결해나가는 '업'
  - 사회와 기업이 함께 단짝으로 짝궁처럼 해결하는 '짝'
- **이러한 가치가 SK 주식회사에서는 '인공지능 문자통역' : 쉐어톡 으로 서비스!**
  - 내가 좋아하는 일을 하면서, 그 기술이 나와 회사, 그리고 사회를 행복하게 할 수 있다면 참 가치있는 일이고, 그러한 직장은 참 좋은 직장이라고 생각하고 지원했습니다.
- 강사 말이 문자로 (STT) / 일 대 다수의 문자 서비스 지원 : 접근성 크게 향상
- 저는 이러한 부분이 진정으로 기술이 나와 사회를 행복하게 하는 중요한 요소라고 생각하였고, 이러한 일을 저도 해 낼 기대를 하며 왔습니다. 
- 기술로 기업과 사회가 더 큰 행복을 만든다는 철학, 그 철학이 저를 SK에 지원하게 된 큰 계기 중 하나입니다. 