{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day26_0808.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn3PwQCsz6gm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "78104b95-93cd-4dc6-8609-fdb7e69d80c9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h16t-j8C3Vo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a76ef85-abc0-4fe2-d5d7-5f0e1553f0d0"
      },
      "source": [
        "# konlpy 설치\n",
        "!apt-get update\n",
        "\n",
        "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "\n",
        "!pip3 install JPype1-py3\n",
        "\n",
        "!pip3 install konlpy\n",
        "\n",
        "!JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.162)] [1 InRelease 2,587 B/88.7\r                                                                               \rGet:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB 16%] [Connecting to cloud\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease 66.3 kB/88.7 kB 75%] [Connecting to cloud.r-project.org] [Conne\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r0% [4 InRelease 15.6 kB/88.7 kB 18%] [1 InRelease 70.6 kB/88.7 kB 80%] [Connect\r0% [3 InRelease gpgv 242 kB] [4 InRelease 15.6 kB/88.7 kB 18%] [1 InRelease 75.\r0% [3 InRelease gpgv 242 kB] [4 InRelease 43.1 kB/88.7 kB 49%] [Connecting to c\r                                                                               \rHit:5 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "\r0% [3 InRelease gpgv 242 kB] [4 InRelease 46.0 kB/88.7 kB 52%] [Connected to cl\r0% [3 InRelease gpgv 242 kB] [Waiting for headers] [Connected to cloud.r-projec\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\r0% [3 InRelease gpgv 242 kB] [6 InRelease 9,844 B/74.6 kB 13%] [Connected to cl\r0% [3 InRelease gpgv 242 kB] [Connected to cloud.r-project.org (13.32.123.39)] \r                                                                               \rGet:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [3 InRelease gpgv 242 kB] [7 InRelease 3,626 B/3,626 B 100%] [Waiting for he\r                                                                               \r0% [3 InRelease gpgv 242 kB] [Waiting for headers]\r                                                  \rIgn:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r                                                  \r0% [3 InRelease gpgv 242 kB]\r                            \r0% [Waiting for headers]\r0% [2 InRelease gpgv 21.3 kB] [Waiting for headers]\r                                                   \rIgn:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r                                                   \r0% [2 InRelease gpgv 21.3 kB]\r                             \rHit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 21.3 kB]\r                             \rGet:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [29.0 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [731 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [597 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,257 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [10.8 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [906 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [14.2 kB]\n",
            "Get:21 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [12.3 kB]\n",
            "Fetched 3,836 kB in 2s (2,011 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "g++ set to manually installed.\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  gvfs openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libxxf86dga1 openjdk-8-jdk openjdk-8-jre x11-utils\n",
            "The following packages will be upgraded:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "2 upgraded, 13 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 42.8 MB of archives.\n",
            "After this operation, 20.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u222-b10-1ubuntu1~18.04.1 [8,267 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u222-b10-1ubuntu1~18.04.1 [27.4 MB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u222-b10-1ubuntu1~18.04.1 [69.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u222-b10-1ubuntu1~18.04.1 [1,756 kB]\n",
            "Fetched 42.8 MB in 1s (60.4 MB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 131289 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../01-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../02-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../03-x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../04-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../05-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../06-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../07-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../08-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../09-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../10-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Preparing to unpack .../11-openjdk-8-jdk-headless_8u222-b10-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) over (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "Preparing to unpack .../12-openjdk-8-jre-headless_8u222-b10-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) over (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../13-openjdk-8-jre_8u222-b10-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../14-openjdk-8-jdk_8u222-b10-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting JPype1-py3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/81/63f5e4202c598f362ee4684b41890f993d6e58309c5d90703f570ab85f62/JPype1-py3-0.5.5.4.tar.gz (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: JPype1-py3\n",
            "  Building wheel for JPype1-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for JPype1-py3: filename=JPype1_py3-0.5.5.4-cp36-cp36m-linux_x86_64.whl size=2675589 sha256=13f30c2554041cac17dc2e0151c6b6782b473b0bcb7d6eaa40423a7b8ec79c6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/37/1f/1015d908d12a0e9b239543d031fda0cded9823aa1306939541\n",
            "Successfully built JPype1-py3\n",
            "Installing collected packages: JPype1-py3\n",
            "Successfully installed JPype1-py3-0.5.5.4\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 6.3MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.5.7 (from konlpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 40.5MB/s \n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.7.0 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q4BMC433eLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96eef33a-8566-4edc-b29b-c311070bfa80"
      },
      "source": [
        "import jpype\n",
        "print(jpype.isJVMStarted())\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"all\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spPa1igFzrZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e697022-f3d2-4cdd-f8c8-7d8d9db625f7"
      },
      "source": [
        "from nltk.corpus import *\n",
        "from nltk.stem import *\n",
        "from konlpy.tag import Kkma\n",
        "from nltk.tokenize import *\n",
        "from nltk.tag import *\n",
        "import nltk\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras import *\n",
        "from keras.datasets import mnist\n",
        "from collections import Counter\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX0Km-hUzrK-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "8788f726-42a7-4e84-846f-e95d1148f198"
      },
      "source": [
        "text=\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.[27] Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is often described as a 'batteries included' language due to its comprehensive standard library.[28]\"\n",
        "text=sent_tokenize(text)\n",
        "text"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Python is an interpreted, high-level, general-purpose programming language.',\n",
              " \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\",\n",
              " 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.',\n",
              " '[27] Python is dynamically typed and garbage-collected.',\n",
              " 'It supports multiple programming paradigms, including procedural, object-oriented, and functional programming.',\n",
              " \"Python is often described as a 'batteries included' language due to its comprehensive standard library.\",\n",
              " '[28]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYZLSnKU5qUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "005c9d1c-7332-4e5d-b357-09c3078d75b9"
      },
      "source": [
        "# 아 counter그냥 dictionary처럼 쓰면 알아서 숫자 세줌 ㄱㅇㄷ\n",
        "# 그리고 생각해보니 원래 sentence 안에 딱 token화된 단어들만 있는게 주타\n",
        "\n",
        "voc=Counter()\n",
        "sentences = []\n",
        "stop_words = stopwords.words('english')\n",
        "for i in text:\n",
        "    sentence=word_tokenize(i)\n",
        "    res = []\n",
        "    for word in sentence: \n",
        "        word=word.lower()\n",
        "        if word not in stop_words: \n",
        "            if len(word) > 2:\n",
        "                res.append(word)\n",
        "                voc[word]=voc[word]+1\n",
        "    sentences.append(res) \n",
        "print(sentences)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['python', 'interpreted', 'high-level', 'general-purpose', 'programming', 'language'], ['created', 'guido', 'van', 'rossum', 'first', 'released', '1991', 'python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace'], ['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects'], ['python', 'dynamically', 'typed', 'garbage-collected'], ['supports', 'multiple', 'programming', 'paradigms', 'including', 'procedural', 'object-oriented', 'functional', 'programming'], ['python', 'often', 'described', \"'batteries\", 'included', 'language', 'due', 'comprehensive', 'standard', 'library'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ49cPUD5ujR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e352277f-2411-4890-89f1-f4acad07baa6"
      },
      "source": [
        "type(voc)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "collections.Counter"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxFEFvdz6Hxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "2fec2443-cae9-4a51-e20a-b006cb4c9543"
      },
      "source": [
        "voc"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({\"'batteries\": 1,\n",
              "         '1991': 1,\n",
              "         'aim': 1,\n",
              "         'approach': 1,\n",
              "         'clear': 1,\n",
              "         'code': 2,\n",
              "         'comprehensive': 1,\n",
              "         'constructs': 1,\n",
              "         'created': 1,\n",
              "         'described': 1,\n",
              "         'design': 1,\n",
              "         'due': 1,\n",
              "         'dynamically': 1,\n",
              "         'emphasizes': 1,\n",
              "         'first': 1,\n",
              "         'functional': 1,\n",
              "         'garbage-collected': 1,\n",
              "         'general-purpose': 1,\n",
              "         'guido': 1,\n",
              "         'help': 1,\n",
              "         'high-level': 1,\n",
              "         'included': 1,\n",
              "         'including': 1,\n",
              "         'interpreted': 1,\n",
              "         'language': 3,\n",
              "         'large-scale': 1,\n",
              "         'library': 1,\n",
              "         'logical': 1,\n",
              "         'multiple': 1,\n",
              "         'notable': 1,\n",
              "         'object-oriented': 2,\n",
              "         'often': 1,\n",
              "         'paradigms': 1,\n",
              "         'philosophy': 1,\n",
              "         'procedural': 1,\n",
              "         'programmers': 1,\n",
              "         'programming': 3,\n",
              "         'projects': 1,\n",
              "         'python': 4,\n",
              "         'readability': 1,\n",
              "         'released': 1,\n",
              "         'rossum': 1,\n",
              "         'significant': 1,\n",
              "         'small': 1,\n",
              "         'standard': 1,\n",
              "         'supports': 1,\n",
              "         'typed': 1,\n",
              "         'use': 1,\n",
              "         'van': 1,\n",
              "         'whitespace': 1,\n",
              "         'write': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlIoYdhQyr5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ohe(w,word2Idx):\n",
        "  ohv = [0]*len(word2Idx)\n",
        "  p = word2Idx[w]\n",
        "  ohv[p] = 1\n",
        "  return ohv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8U5i2yMzpth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"오늘 메뉴는 뼈다귀 해장국입니다. 맛있게 먹어요. 국산이래요. 뼈다귀 최고 뼈다귀 최고\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iStC5wYi0FOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74b07be7-ee97-44a4-ded9-a8c4f365c836"
      },
      "source": [
        "from keras_preprocessing.text import Tokenizer\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts([text])\n",
        "tok"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f4c7873d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuZlwRxi0RaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "8897b972-94ca-42b8-a3c8-d2471d09de21"
      },
      "source": [
        "tok.word_counts"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('오늘', 1),\n",
              "             ('메뉴는', 1),\n",
              "             ('뼈다귀', 3),\n",
              "             ('해장국입니다', 1),\n",
              "             ('맛있게', 1),\n",
              "             ('먹어요', 1),\n",
              "             ('국산이래요', 1),\n",
              "             ('최고', 2)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rpUcLwk1pTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "ad38e42e-47f3-4418-a33e-5c819c9c43c9"
      },
      "source": [
        "tok.word_index"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'국산이래요': 8,\n",
              " '맛있게': 6,\n",
              " '먹어요': 7,\n",
              " '메뉴는': 4,\n",
              " '뼈다귀': 1,\n",
              " '오늘': 3,\n",
              " '최고': 2,\n",
              " '해장국입니다': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_5jvqBh0Tz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text2 = \"뼈다귀 관련 음식을 가장 맛있게 먹어요\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZG0Ju5L1Ui-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "17244081-a83e-43e7-c179-2bd6179eda39"
      },
      "source": [
        "tok.texts_to_sequences([text2]) \n",
        "# 내가 만든 corpus에서 알맞은 index를 불러옴!"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 6, 7]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PySBg9vi1W7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = tok.texts_to_sequences([text2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxDQTbiJ147j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af3ff78c-fe3b-489d-ce9f-c0920cc8c922"
      },
      "source": [
        "res[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 6, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wUI8czZ15hb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc25997a-ffa6-4289-c317-59c9f4509367"
      },
      "source": [
        "res[0][1]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIaQlMft16fO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c15aaba2-1c47-48e7-d61d-ca9f3c8c16b4"
      },
      "source": [
        "vLen = len(tok.word_index)\n",
        "vLen"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC14O9m52PIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7330e041-926a-47f9-a249-e5be8975a799"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "to_categorical(res,num_classes=vLen+1)\n",
        "# 짜잔! ohe가 되었다!"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmn-4MK22lAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ohe의 한계점?\n",
        "\n",
        "# 1. 음식, 식사, 점심, 날씨 => 원핫인코딩 => 크기:4 => 단어들 간 유사한 정도를 나타내기 어렵다!\n",
        "# 2. 공간을 많이 차지함(sparse matrix => dense matirx로 해야하지 않나?)\n",
        "# 3. 연관검색어 표현이 어려움 ex) \"뼈다귀\" => \"뼈다귀 해장국\", \"뼈다귀 국물\", \"뼈다귀 감자탕\"\n",
        "# => 해결하기 위한 여러가지 노력 : LSA / Word2Vec\n",
        "# LSA : 카운트 기반으로 단어의 의미를 벡터화하는 알고리즘\n",
        "# Word2Vec : 단어를 벡터공간으로 표현 (거리계산 / 단어들 간 의미를 이용)\n",
        "# Seq2Seq, RNNLM, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPAwL0-H3RjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 언어 모델?\n",
        "# Vocabulary : 기계가 학습한 단어의 집합\n",
        "# out of Vocabulary : OOV : 기계가 학습안한 단어들\n",
        "# 학습된 단어와 학습안된 단어가 있어서 두 단어를 유사도를 보고싶다면?\n",
        "# => BPE : OOV!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvvzXa_DCNgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BPE 알고리즘 : OOV 등장했을때 문제 해결\n",
        "# KMP, 보이어무어, 레벤슈타인 거리 : 단어 검색 빠르게 해주는 알고리즘\n",
        "# 1. 모든 단어들을 글자단위로 분리\n",
        "# 훈련 데이터에 등장한 단어와 단어 빈도수!\n",
        "# vocab = {'l o w'  : 5, 'l o w e r' : 2,\n",
        "# 'n e w e s t':6, 'w i d e s t':3}\n",
        "\n",
        "# 연속으로 가장 많이 등장한 글자의 쌍 => 한 글자로 표현(변환)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT-m1QtTMHIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "ff4337b2-f6b8-4c11-cdaa-bcb1f36d27ae"
      },
      "source": [
        "import re, collections\n",
        "def get_stats(vocab):\n",
        "  pairs = collections.defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "    symbols = word.split()\n",
        "    for i in range(len(symbols)-1):\n",
        "      pairs[symbols[i],symbols[i+1]] += freq\n",
        "  return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "  v_out = {}\n",
        "  bigram = re.escape(' '.join(pair))\n",
        "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "  for word in v_in:\n",
        "    w_out = p.sub(''.join(pair), word)\n",
        "    v_out[w_out] = v_in[word]\n",
        "  return v_out\n",
        "\n",
        "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n",
        "'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "  pairs = get_stats(vocab)\n",
        "  best = max(pairs, key=pairs.get)\n",
        "  vocab = merge_vocab(best, vocab)\n",
        "  print(best)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('e', 's')\n",
            "('es', 't')\n",
            "('est', '</w>')\n",
            "('l', 'o')\n",
            "('lo', 'w')\n",
            "('n', 'e')\n",
            "('ne', 'w')\n",
            "('new', 'est</w>')\n",
            "('low', '</w>')\n",
            "('w', 'i')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpSW71sfMHWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "cac8a7bb-a84c-4b3a-e975-bfe2ee0173e8"
      },
      "source": [
        "pairs"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('d', 'est</w>'): 3,\n",
              "             ('e', 'r'): 2,\n",
              "             ('i', 'd'): 3,\n",
              "             ('low', 'e'): 2,\n",
              "             ('r', '</w>'): 2,\n",
              "             ('w', 'i'): 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgnE_IXMjKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. BPE 알고리즘 구현(연습문제)\n",
        "# 2-0. 상품분류(10가지)\n",
        "# 2. 미니프로젝트 : 개-고양이"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rUYYPKtfR_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "data = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = data.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8SvRzqxfjzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtpdQHN6RUaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9797a55f-d8fc-4ef0-a2d0-fc83877443dd"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQX3v8xjgPBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11cd2820-4d14-48d9-8917-c10c0208d8ec"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2zuy_drgRHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af40e6fe-33cf-46b4-e827-664aff472f09"
      },
      "source": [
        "test_labels.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEp4H2uMjFdH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61992cd1-7547-4075-f1ff-be8280f07a3b"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l92VdqYveFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = test_labels.reshape(-1,1)\n",
        "train_labels = train_labels.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH98nAdwRmaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255\n",
        "test_images = test_images /255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOUOn_HhvzJi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac01e409-df90-4d8d-ed4a-d6e1ac85a75f"
      },
      "source": [
        "test_labels.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-SOMsmgf4sU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "df896817-b4ee-4fb9-9339-08841d1c9461"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "type(test_images[0])\n",
        "tmp = test_images[29].reshape(28,-1)\n",
        "plt.imshow(tmp)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efbc055fd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEnhJREFUeJzt3WuMlOd1B/D/mcvegd0FvGCgYDBO\njWiNmw3GsRs7cWI5xCqOFKHQKiKqm82HWK2lqKrlfqg/pBKqG0f+ULnCNQpOU5I2iWU+WI1dVNV1\n0yKvLQzGxAbb3BeWsFyWy+7O5fTDvkSLve951jPvzDu75/+TELtz5p15mN0/cznv8zyiqiAifzJp\nD4CI0sHwEznF8BM5xfATOcXwEznF8BM5xfATOcXwEznF8BM5lavnnTVJs7agvZ53OT2I2PUqzsLU\nWW1mfWyefbyM2GOTtpJZzw8Yx1+5at95SA0ft+lqBJcxpqOBB2ZcVeEXkQcAPA0gC+CfVHWLdf0W\ntOMOua+au0xP6BetmpvO5c26Fgv2DRi/5MW1nzYP/fCbdkBa3m0x6/nPnDPrC/82/ldMX99nHotM\n1ixL1q5rYcw4eGb+x7Fbd035uhW/7BeRLIB/APBlAKsAbBKRVZXeHhHVVzXv+dcCOKSqH6jqGICf\nANiQzLCIqNaqCf8iAMcmfH88uuw6ItInIv0i0l/AaBV3R0RJqvmn/aq6VVV7VbU3j+Za3x0RTVE1\n4T8BYMmE7xdHlxHRNFBN+F8HsFJEbhKRJgBfB7AzmWERUa1V3OpT1aKIPALglxhv9W1T1f2JjazR\nWK2fUNtI7P9jzZbUFOQWf+yjlt/q+8d/M48dUbvNmL2jbNaXNw2a9X9+5rOxtXd7zUOBsn0OgQbq\nZKuqz6+qLwF4KaGxEFEd8fReIqcYfiKnGH4ipxh+IqcYfiKnGH4ip+o6n3/GCvTxQ/3qSxvXmfVF\nf37QrN/TvTe29sblZeaxIX/Stdus/+yCPWX4fKE1tvbZty6bx27/7z8067c+edKsF48ciy+GfmY6\n888h4DM/kVMMP5FTDD+RUww/kVMMP5FTDD+RU6J1XKV0tnTrdF29V/JNsbXQlNzR9Z8x6/dvedWs\nny3Yy50X1F7F1nJqZLZZP3O1w6zf2nnarDdlirG1Ytked3PGXrX4yJVus37pCxdja9VOo25Uu3UX\nLurQlJaa5jM/kVMMP5FTDD+RUww/kVMMP5FTDD+RUww/kVOc0jtF1fSF2/7S3ssk1Mc/M2b32rMS\nf65GTuylt29uP2PWjw13mvWhMXsL8Dn5+G24hwv2DsBjgfMAlrYNmfWXH42fKn3jk78yj/WAz/xE\nTjH8RE4x/EROMfxETjH8RE4x/EROMfxETlXV5xeRwwCGAZQAFFU1tOmyS/NaLpn1jNGnB4DZuVGz\nbs17v1RqNo8NrQUwp3nErLdm7Tn3V0vxW4B3N10xjw2dQzBatrcXb/+8sX34k+ahLiRxks/nVfU3\nCdwOEdURX/YTOVVt+BXAyyLyhoj0JTEgIqqPal/2362qJ0TkBgCviMivVfW6Bemi/xT6AKAF9ns4\nIqqfqp75VfVE9PcggBcArJ3kOltVtVdVe/OwP3wiovqpOPwi0i4is659DeB+AG8nNTAiqq1qXvb3\nAHhBRK7dzr+o6r8nMioiqrmKw6+qHwC4LcGxTFvZW1aY9c78UbPelbP73aFefMl4AWfN9QeA0bL9\nK3B+JH6LbQBY2mHPqb9cjH+rl8vY22CPlOyxhdb1X9dzOLZ2wDzSB7b6iJxi+ImcYviJnGL4iZxi\n+ImcYviJnOLS3Qm4dOtcs56XD816qJU3L29PCT45Gr+8dgZ2q6+s9v//oSm9oVaiNeX3QsFuI7Zk\n47f3BoCOwFTnkvlvq3xb85mCz/xETjH8RE4x/EROMfxETjH8RE4x/EROMfxETrHPn4CLS+2H0Zpy\nCwBliFnvzsZvcw0AJxHf5w/ddug8gBvbL5j1ktq3f7nYFFsLbcF9U/tZs96WsbdNHy7FbwGendtt\nHls6a09Vngn4zE/kFMNP5BTDT+QUw0/kFMNP5BTDT+QUw0/kFPv8Cbhyo90rLwb62ecK1W1jlpP4\nJbBDS3MHlwUP9PFD6wFYy3OHzkE4drXLrM/J2ec/tFhLe8+z+/xgn5+IZiqGn8gphp/IKYafyCmG\nn8gphp/IKYafyKlgn19EtgF4EMCgqq6OLusG8FMAywAcBrBRVc/VbpiNrTirXNXxBy4sMOtXCnmz\nfk/PodjacDF+Tjtgr6sPAMVAHz8kdB6AZd+phWb9rs73zbo1n784r8M8Vt41yzPCVH4yPwTwwEcu\newzALlVdCWBX9D0RTSPB8KvqqwA+errTBgDbo6+3A3go4XERUY1V+pqsR1UHoq9PAehJaDxEVCdV\nf+CnqgrELwQnIn0i0i8i/QXYe6sRUf1UGv7TIrIQAKK/B+OuqKpbVbVXVXvzaK7w7ogoaZWGfyeA\nzdHXmwG8mMxwiKheguEXkR0A/hfAp0TkuIg8DGALgC+JyEEAX4y+J6JpJNjnV9VNMaX7Eh7L9DXL\n7pWHhPr4V8fsejVC8/VrKSf2+RFjo/a/e3nzabP+P8O3xN92p33bHt6g8gw/IqcYfiKnGH4ipxh+\nIqcYfiKnGH4ip7h0dwLaOqo7bfnMuVn2FcReGjwTqFvygXZbOVAPTfktG63E0Ljz79hLmh+8LTAV\nuhy/PfjlG+xffbb6iGjGYviJnGL4iZxi+ImcYviJnGL4iZxi+ImcYp8/Aa1N9pRea5tqAJBDdj+7\nsGTMrOeNLbqrlQ+MfbRob/FdzW23nrHPA3jvit3nb83EP26FjvSmMjcKPvMTOcXwEznF8BM5xfAT\nOcXwEznF8BM5xfATOcU+fwJa83afPwt7TnymZPec2+dcNetlxB/flCmax4Zk4ndiG7/vKrbgDm0P\n3jJkP27/dXSFWf/azXtia2NzzENd4DM/kVMMP5FTDD+RUww/kVMMP5FTDD+RUww/kVPBPr+IbAPw\nIIBBVV0dXfYEgG8BOBNd7XFVfalWg2x0HU3Vrds/stier3/nDQNmvVCOn1MfXJffOEcAAHKBtQKs\ndfkBe23+0DkE52+21wq4esleXb9knINQbKt8r4OZYirP/D8E8MAkl/9AVddEf9wGn2i6CoZfVV8F\nMFSHsRBRHVXznv8REdkrIttEpCuxERFRXVQa/mcArACwBsAAgO/HXVFE+kSkX0T6C6juvTERJaei\n8KvqaVUtqWoZwLMA1hrX3aqqvaram3ex/SHR9FBR+EVk4YRvvwrg7WSGQ0T1MpVW3w4A9wKYJyLH\nAfwNgHtFZA0ABXAYwLdrOEYiqoFg+FV10yQXP1eDsUxbLYF56QW1+9W/t/K4Wb+hZdish/a5N48N\n9Nqr3ROgMx+/FkEhsBZA0zq7ySQl+3G1zkGoYhmCGYMPAZFTDD+RUww/kVMMP5FTDD+RUww/kVNc\nujsB50btLbZ/p/2cWV/X9aFZPznaadZbjKXDM6EpvYGeV3Ng6e9q2oyh276p66xZ/8r8fWb96Njc\n+CJ36OYzP5FXDD+RUww/kVMMP5FTDD+RUww/kVMMP5FT7PMnYFZg6e45OXuL7bOFdrMemvpqTcsN\n9/Ht6cjVss4zCPX5Q04X7H22renIgdMfXOAzP5FTDD+RUww/kVMMP5FTDD+RUww/kVMMP5FT7PMn\n4MCJBWZ9Wbs9Lz00J36sbP+Y2rLx5xmEtuAO3Xc+Yy/dPVKyx2adZ9CWtbcmb8ra931kpNus3zn7\n/dhaZsUl81gP+MxP5BTDT+QUw0/kFMNP5BTDT+QUw0/kFMNP5FSwzy8iSwA8D6AHgALYqqpPi0g3\ngJ8CWAbgMICNqmovUD9DLf/jPWb9l9+706z/2UMvm/XBkVlmfUHuQmxtf3mReWxHYC2Carfotubz\nt2XsPn+xbD83LWi+aNafevZrsbXl2w6Yx1b3r54epvLMXwTwXVVdBWAdgO+IyCoAjwHYpaorAeyK\nvieiaSIYflUdUNU3o6+HARwAsAjABgDbo6ttB/BQrQZJRMn7RO/5RWQZgNsB7AbQo6oDUekUxt8W\nENE0MeXwi0gHgJ8DeFRVr3uzpaoKTL6QnIj0iUi/iPQXYL+/JKL6mVL4RSSP8eD/WFV/EV18WkQW\nRvWFAAYnO1ZVt6pqr6r25tGcxJiJKAHB8IuIAHgOwAFVfWpCaSeAzdHXmwG8mPzwiKhWpjKl9y4A\n3wCwT0Su9bQeB7AFwL+KyMMAjgDYWJshTn+33fueWS8EpuyGpuV2Zq/E1kbLWfPYnnx8mxAAjo/Z\n02abAlN+LV25yxUfC4RbhcaK5iidc9mVvk4w/Kr6GuJ3M78v2eEQUb3wDD8ipxh+IqcYfiKnGH4i\npxh+IqcYfiKnuHR3AjJtbWb9C92/NuvD5Rb79q2GNYDubPwy1KEturtz9hLWR0fnmvVcoM8/XIz/\nt801xg0AbTm7jz84Zk91vnvTm7G195+KLbnBZ34ipxh+IqcYfiKnGH4ipxh+IqcYfiKnGH4ip9jn\nT8CFP/p9u146Y9ZD89qLgV59pzGv3Vo6GwAW5ex57a+VbjHrrdmCWb9aysfWQn3++U12vaD2WgVr\nOo7G1g7P/V3z2NLZIbM+E/CZn8gphp/IKYafyCmGn8gphp/IKYafyCmGn8gp9vkTMHCP3UsPac/Y\n25iFtqq2jAX2BJifvWrWQ+sBNGeKZn24ED+ff1ZmxDy2Kx+/HwEAnCvY6ygMFmbHH3u/ff7C7B3/\nZ9ZnAj7zEznF8BM5xfATOcXwEznF8BM5xfATOcXwEzkV7POLyBIAzwPowfiO51tV9WkReQLAtwBc\nm6z+uKq+VKuBNrJPr/7ArDdn7DnvI+X4Oe8AML8lMO89E7eDOlDW+BoAnC832fVCq1kPWdR6PrbW\nIvaa/6XAOQYh1vGDX7HPrZi9o6q7nhamcpJPEcB3VfVNEZkF4A0ReSWq/UBV/752wyOiWgmGX1UH\nAAxEXw+LyAEAi2o9MCKqrU/0ukpElgG4HcDu6KJHRGSviGwTka6YY/pEpF9E+guwX2oRUf1MOfwi\n0gHg5wAeVdWLAJ4BsALAGoy/Mvj+ZMep6lZV7VXV3jyaExgyESVhSuEXkTzGg/9jVf0FAKjqaVUt\nqWoZwLMA1tZumESUtGD4RUQAPAfggKo+NeHyhROu9lUAbyc/PCKqlal82n8XgG8A2Ccie6LLHgew\nSUTWYLz9dxjAt2sywgaRW3RjbO3B+f3msaGWVWfWnrp6u7EENQCcL8dPKV4/b5957Npmu824sn3Q\nrLcE2piWpTm7Dbm69bhZP1boNuvW0t6fu/mQeexJszozTOXT/tcATPZTctnTJ5opeIYfkVMMP5FT\nDD+RUww/kVMMP5FTDD+RU6Kqdbuz2dKtd8h9dbu/RpFbstisX1m1wKxnCvbPqOlX+2NrB7+3xjx2\nzkG71z681Cwj/6mLZn3Zoxdia8Vjdh9/6E/vNOtz37LvO3sqfvvx4omZ2cnfrbtwUYfsH2qEz/xE\nTjH8RE4x/EROMfxETjH8RE4x/EROMfxETtW1zy8iZwAcmXDRPAC/qdsAPplGHVujjgvg2CqV5NiW\nqur8qVyxruH/2J2L9Ktqb2oDMDTq2Bp1XADHVqm0xsaX/UROMfxETqUd/q0p37+lUcfWqOMCOLZK\npTK2VN/zE1F60n7mJ6KUpBJ+EXlARN4VkUMi8lgaY4gjIodFZJ+I7BERe03u2o9lm4gMisjbEy7r\nFpFXRORg9Pek26SlNLYnRORE9NjtEZH1KY1tiYj8p4i8IyL7ReQvostTfeyMcaXyuNX9Zb+IZAG8\nB+BLAI4DeB3AJlV9p64DiSEihwH0qmrqPWER+RyASwCeV9XV0WV/B2BIVbdE/3F2qepfNcjYngBw\nKe2dm6MNZRZO3FkawEMAvokUHztjXBuRwuOWxjP/WgCHVPUDVR0D8BMAG1IYR8NT1VcBDH3k4g0A\ntkdfb8f4L0/dxYytIajqgKq+GX09DODaztKpPnbGuFKRRvgXATg24fvjaKwtvxXAyyLyhoj0pT2Y\nSfRE26YDwCkAPWkOZhLBnZvr6SM7SzfMY1fJjtdJ4wd+H3e3qv4BgC8D+E708rYh6fh7tkZq10xp\n5+Z6mWRn6d9K87GrdMfrpKUR/hMAlkz4fnF0WUNQ1RPR34MAXkDj7T58+tomqdHf9mZ6ddRIOzdP\ntrM0GuCxa6Qdr9MI/+sAVorITSLSBODrAHamMI6PEZH26IMYiEg7gPvReLsP7wSwOfp6M4AXUxzL\ndRpl5+a4naWR8mPXcDteq2rd/wBYj/FP/N8H8NdpjCFmXMsBvBX92Z/22ADswPjLwALGPxt5GMBc\nALsAHATwHwC6G2hsPwKwD8BejAdtYUpjuxvjL+n3AtgT/Vmf9mNnjCuVx41n+BE5xQ/8iJxi+Imc\nYviJnGL4iZxi+ImcYviJnGL4iZxi+Imc+n8Kn6+8abEVCQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtRej8XuhNPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8985d670-98fd-4fb8-912b-8267d2b28232"
      },
      "source": [
        "test_labels"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxZbpl-nlyNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_conv_twice(input_layer, num_filter, keep_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.Variable(tf.random_normal([filter_size,filter_size,1,num_filter]), )\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding='SAME')\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  w1_1 = tf.Variable(tf.random_normal([filter_size,filter_size,num_filter,num_filter]))\n",
        "  L1_1 = tf.nn.conv2d(L1, w1_1, strides=[1,strides,strides,1], padding='SAME') \n",
        "  L1_1 = tf.nn.relu(L1_1)\n",
        "  L1_1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "  res = tf.nn.dropout(L1_1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHhiTuLB3fQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_conv_once(input_layer, num_filter, keep_prob, filter_size=3, strides=1):\n",
        "  w1 = tf.Variable(tf.random_normal([filter_size,filter_size,1,num_filter]))\n",
        "  L1 = tf.nn.conv2d(input_layer, w1, strides=[1,strides,strides,1], padding='SAME')\n",
        "  L1 = tf.nn.relu(L1)\n",
        "  L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "  res = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "  \n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0dUu4fNx3Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_inception_model(input_layer, keep_prob,filter_num_list):\n",
        "  w1 = tf.Variable(tf.random_normal([1,1,1,filter_num_list[0]]))\n",
        "  l1 = tf.nn.conv2d(input_layer, w1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  w2_1 = tf.Variable(tf.random_normal([1,1,1,filter_num_list[1]]))\n",
        "  l2_1 = tf.nn.conv2d(input_layer, w2_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w2_2 = tf.Variable(tf.random_normal([3, 3, filter_num_list[1], filter_num_list[2]]))\n",
        "  l2_2 = tf.nn.conv2d(l2_1, w2_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  w3_1 = tf.Variable(tf.random_normal([1,1,1,filter_num_list[3]]))\n",
        "  l3_1 = tf.nn.conv2d(input_layer, w3_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w3_2 = tf.Variable(tf.random_normal([5, 5, filter_num_list[3], filter_num_list[4]]))\n",
        "  l3_2 = tf.nn.conv2d(l3_1, w3_2, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  l4_1 = tf.nn.max_pool(input_layer, ksize=[1,3,3,1], strides=[1,1,1,1], padding=\"SAME\")\n",
        "  w4_1 = tf.Variable(tf.random_normal([1,1,1,filter_num_list[5]]))\n",
        "  l4_2 = tf.nn.conv2d(l4_1, w4_1, strides=[1,1,1,1], padding=\"SAME\")\n",
        "  \n",
        "  res = tf.concat([l1, l2_2, l3_2, l4_2], 3) # input의 shape가 [-1,28,28,1] 이라고 가정\n",
        "  \n",
        "  return res # 다시 이 return된 layer를 inception 모델에 돌리기!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGsuw9sD4IKp",
        "colab_type": "text"
      },
      "source": [
        "# conv를 열라 많이 때렸으나 정확도가 좋지 않은 비운의 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICI5l_d2kE8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "training_epochs = 25\n",
        "batch_size = 128\n",
        "\n",
        "save_file = './model_mnist_fashion_cnn.ckpt'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_twice(x_img, 32, keep_prob)\n",
        "h2 = make_conv_twice(h1, 64, keep_prob)\n",
        "h3 = make_conv_twice(h2, 128, keep_prob)\n",
        "\n",
        "h3_flat = tf.reshape(h3, [-1,4*4*128])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[4*4*128,625], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([625])\n",
        "\n",
        "h4 = tf.matmul(h3_flat,fcw1) + fcb1\n",
        "h4 = tf.nn.relu(h4)\n",
        "h4 = tf.nn.dropout(h4, keep_prob=keep_prob_flatten)\n",
        "\n",
        "fcw2 = tf.get_variable(\"fcw2\", shape=[625,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb2 = tf.zeros([10])\n",
        "logits = tf.matmul(h4, fcw2) + fcb2\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmfmx0-4rDqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "64a32e10-d527-48dc-b63a-0c54fe111fc8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = train_images[batch_size*i:batch_size*(i+1),:,:], train_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.6}\n",
        "      cv, accv, _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "    print(\"epoch: {}, acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "  \n",
        "  acu = sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})\n",
        "  print(\"final_acc: {}\".format(\\\n",
        "            sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})))\n",
        "  saver.save(sess, save_file)\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.5546875, avg_cost: 3.0594221047866026\n",
            "epoch: 1, acc: 0.53125, avg_cost: 1.2437512167753304\n",
            "epoch: 2, acc: 0.6015625, avg_cost: 1.1626125858125531\n",
            "epoch: 3, acc: 0.578125, avg_cost: 1.1341197553098705\n",
            "epoch: 4, acc: 0.6015625, avg_cost: 1.096178237571676\n",
            "epoch: 5, acc: 0.6171875, avg_cost: 1.0798191325022626\n",
            "epoch: 6, acc: 0.5390625, avg_cost: 1.0729044628703688\n",
            "epoch: 7, acc: 0.59375, avg_cost: 1.0633080405557256\n",
            "epoch: 8, acc: 0.5234375, avg_cost: 1.0690238815851698\n",
            "epoch: 9, acc: 0.7109375, avg_cost: 1.0581109702077685\n",
            "epoch: 10, acc: 0.6484375, avg_cost: 1.0537488116667824\n",
            "epoch: 11, acc: 0.6640625, avg_cost: 1.0587240085642553\n",
            "epoch: 12, acc: 0.5546875, avg_cost: 1.0485842376947399\n",
            "epoch: 13, acc: 0.6796875, avg_cost: 1.0538722423152025\n",
            "epoch: 14, acc: 0.6484375, avg_cost: 1.0544579096584232\n",
            "epoch: 15, acc: 0.6171875, avg_cost: 1.04891444513431\n",
            "epoch: 16, acc: 0.6171875, avg_cost: 1.0519323276403623\n",
            "epoch: 17, acc: 0.625, avg_cost: 1.0491461590823958\n",
            "epoch: 18, acc: 0.5390625, avg_cost: 1.0334290124667003\n",
            "epoch: 19, acc: 0.609375, avg_cost: 1.0374403299174757\n",
            "epoch: 20, acc: 0.59375, avg_cost: 1.0376176429100537\n",
            "epoch: 21, acc: 0.6015625, avg_cost: 1.0300933800191967\n",
            "epoch: 22, acc: 0.6328125, avg_cost: 1.032411707007987\n",
            "epoch: 23, acc: 0.578125, avg_cost: 1.0286225491864054\n",
            "epoch: 24, acc: 0.5703125, avg_cost: 1.0203815301259362\n",
            "final_acc: 0.8726999759674072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDkGEFR471_G",
        "colab_type": "text"
      },
      "source": [
        "# Learning Rate만 바꾼 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDjh0_R8742K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "training_epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "save_file = './model_mnist_fashion_cnn_mod_lr.ckpt'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_twice(x_img, 32, keep_prob)\n",
        "h2 = make_conv_twice(h1, 64, keep_prob)\n",
        "h3 = make_conv_twice(h2, 128, keep_prob)\n",
        "\n",
        "h3_flat = tf.reshape(h3, [-1,4*4*128])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[4*4*128,625], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([625])\n",
        "\n",
        "h4 = tf.matmul(h3_flat,fcw1) + fcb1\n",
        "h4 = tf.nn.relu(h4)\n",
        "h4 = tf.nn.dropout(h4, keep_prob=keep_prob_flatten)\n",
        "\n",
        "fcw2 = tf.get_variable(\"fcw2\", shape=[625,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb2 = tf.zeros([10])\n",
        "logits = tf.matmul(h4, fcw2) + fcb2\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeDrn8YA77r-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = train_images[batch_size*i:batch_size*(i+1),:,:], train_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.7}\n",
        "      cv, _ = sess.run([cost, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "    accv = sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})\n",
        "    print(\"epoch: {}, acc: {}, avg_cost: {}\".format(epoch,accv, avg_cost))\n",
        "  \n",
        "  acu = sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})\n",
        "  print(\"final_acc: {}\".format(\\\n",
        "            sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})))\n",
        "  saver.save(sess, save_file)\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85z_IFJ86qkY",
        "colab_type": "text"
      },
      "source": [
        "# 적당히 하자 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0D98ETC6tBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0025\n",
        "training_epochs = 30\n",
        "batch_size = 128\n",
        "\n",
        "save_file = './model_mnist_fashion_cnn_modified.ckpt'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once(x_img, 32, keep_prob)\n",
        "h2 = make_conv_once(h1, 64, keep_prob)\n",
        "h3 = make_conv_once(h2, 128, keep_prob)\n",
        "\n",
        "h3_flat = tf.reshape(h3, [-1,4*4*128])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[4*4*128,625], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([625])\n",
        "\n",
        "h4 = tf.matmul(h3_flat,fcw1) + fcb1\n",
        "h4 = tf.nn.relu(h4)\n",
        "h4 = tf.nn.dropout(h4, keep_prob=keep_prob_flatten)\n",
        "\n",
        "fcw2 = tf.get_variable(\"fcw2\", shape=[625,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb2 = tf.zeros([10])\n",
        "logits = tf.matmul(h4, fcw2) + fcb2\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu19tQdN6wrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "60941b26-0da6-4d89-d2da-629f030c8d9e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = train_images[batch_size*i:batch_size*(i+1),:,:], train_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.7}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "  \n",
        "  acu = sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})\n",
        "  print(\"final_acc: {}\".format(\\\n",
        "            sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})))\n",
        "  saver.save(sess, save_file)\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, train_acc: 0.7109375, avg_cost: 2.138404281220886\n",
            "epoch: 1, train_acc: 0.6875, avg_cost: 0.9358284763800786\n",
            "epoch: 2, train_acc: 0.6640625, avg_cost: 0.8808967789523615\n",
            "epoch: 3, train_acc: 0.6953125, avg_cost: 0.8534735243799336\n",
            "epoch: 4, train_acc: 0.7578125, avg_cost: 0.8344270827678532\n",
            "epoch: 5, train_acc: 0.6875, avg_cost: 0.8301192546247423\n",
            "epoch: 6, train_acc: 0.6484375, avg_cost: 0.8256801431759814\n",
            "epoch: 7, train_acc: 0.7109375, avg_cost: 0.8298501284458696\n",
            "epoch: 8, train_acc: 0.6328125, avg_cost: 0.8208086289401745\n",
            "epoch: 9, train_acc: 0.6875, avg_cost: 0.8056632049827499\n",
            "epoch: 10, train_acc: 0.7265625, avg_cost: 0.8018204117687351\n",
            "epoch: 11, train_acc: 0.6875, avg_cost: 0.7936000848173077\n",
            "epoch: 12, train_acc: 0.6328125, avg_cost: 0.7918314486105215\n",
            "epoch: 13, train_acc: 0.6875, avg_cost: 0.7921625755281534\n",
            "epoch: 14, train_acc: 0.6796875, avg_cost: 0.7873407641791891\n",
            "epoch: 15, train_acc: 0.7109375, avg_cost: 0.7834360244182443\n",
            "epoch: 16, train_acc: 0.6875, avg_cost: 0.7814969014153517\n",
            "epoch: 17, train_acc: 0.6796875, avg_cost: 0.7794674377665561\n",
            "epoch: 18, train_acc: 0.703125, avg_cost: 0.7689683433526611\n",
            "epoch: 19, train_acc: 0.7578125, avg_cost: 0.7698383368870141\n",
            "epoch: 20, train_acc: 0.7578125, avg_cost: 0.7699530831514262\n",
            "epoch: 21, train_acc: 0.6875, avg_cost: 0.764259502864801\n",
            "epoch: 22, train_acc: 0.703125, avg_cost: 0.7702187709828731\n",
            "epoch: 23, train_acc: 0.640625, avg_cost: 0.7673480873688675\n",
            "epoch: 24, train_acc: 0.59375, avg_cost: 0.7663398186365751\n",
            "epoch: 25, train_acc: 0.640625, avg_cost: 0.7603608711153017\n",
            "epoch: 26, train_acc: 0.7421875, avg_cost: 0.7616238606791206\n",
            "epoch: 27, train_acc: 0.6796875, avg_cost: 0.759876185757482\n",
            "epoch: 28, train_acc: 0.75, avg_cost: 0.758378287411144\n",
            "epoch: 29, train_acc: 0.703125, avg_cost: 0.7519140508439796\n",
            "final_acc: 0.8870000243186951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ghGSTWmUa8W",
        "colab_type": "text"
      },
      "source": [
        "# 좀 더 적당히 하자 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5zkJysUUcaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.0025\n",
        "training_epochs = 30\n",
        "batch_size = 100\n",
        "\n",
        "save_file = './model_mnist_fashion_cnn_modified.ckpt'\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None,28,28])\n",
        "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None,1])\n",
        "y_ohe = tf.reshape(tf.one_hot(y,10),[-1,10])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_flatten = tf.placeholder(tf.float32)\n",
        "\n",
        "h1 = make_conv_once(x_img, 32, keep_prob)\n",
        "h2 = make_conv_once(h1, 64, keep_prob)\n",
        "\n",
        "h3_flat = tf.reshape(h2, [-1,7*7*64])\n",
        "\n",
        "fcw1 = tf.get_variable(\"fcw1\", shape=[7*7*64,625], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb1 = tf.zeros([625])\n",
        "\n",
        "h4 = tf.matmul(h3_flat,fcw1) + fcb1\n",
        "h4 = tf.nn.relu(h4)\n",
        "h4 = tf.nn.dropout(h4, keep_prob=keep_prob_flatten)\n",
        "\n",
        "fcw2 = tf.get_variable(\"fcw2\", shape=[625,10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "fcb2 = tf.zeros([10])\n",
        "logits = tf.matmul(h4, fcw2) + fcb2\n",
        "logits = tf.nn.dropout(logits, keep_prob=keep_prob_flatten)\n",
        "prediction = tf.argmax(logits, 1)\n",
        "\n",
        "c_pre = tf.equal(tf.argmax(logits,1), tf.argmax(y_ohe,1))\n",
        "acc = tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ohe, logits=logits))\n",
        "train = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egR2nZeoUase",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "29af810b-03aa-4657-d111-907895f3bbda"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "my_data = np.zeros([1,28*28])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_images.shape[0]/batch_size)\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = train_images[batch_size*i:batch_size*(i+1),:,:], train_labels[batch_size*i:batch_size*(i+1)]\n",
        "      myfeed = {x:batch_xs, y:batch_ys, keep_prob:0.7, keep_prob_flatten:0.7}\n",
        "      cv, accv,  _ = sess.run([cost, acc, train], feed_dict=myfeed)\n",
        "      avg_cost += cv/total_batch\n",
        "    print(\"epoch: {}, train_acc: {}, avg_cost: {}\".format(epoch,accv,avg_cost))\n",
        "  \n",
        "  acu = sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})\n",
        "  print(\"final_acc: {}\".format(\\\n",
        "            sess.run(acc, feed_dict={x:test_images,y:test_labels,keep_prob:1,keep_prob_flatten:1})))\n",
        "  saver.save(sess, save_file)\n",
        "#   predict = sess.run(prediction, feed_dict={x:my_data, keep_prob: 1, keep_prob_flatten: 1})\n",
        "#   print(\"my_data shape: {}\".format(my_data.shape))\n",
        "#   print(predict[0])\n",
        "#   print(type(acu))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, train_acc: 0.7099999785423279, avg_cost: 1.522604408264158\n",
            "epoch: 1, train_acc: 0.7400000095367432, avg_cost: 0.8096714786688479\n",
            "epoch: 2, train_acc: 0.75, avg_cost: 0.7813786839445435\n",
            "epoch: 3, train_acc: 0.7699999809265137, avg_cost: 0.7733025095363459\n",
            "epoch: 4, train_acc: 0.7599999904632568, avg_cost: 0.7740225375692054\n",
            "epoch: 5, train_acc: 0.6700000166893005, avg_cost: 0.777297642727693\n",
            "epoch: 6, train_acc: 0.7699999809265137, avg_cost: 0.7684856707354376\n",
            "epoch: 7, train_acc: 0.7699999809265137, avg_cost: 0.7669178994496663\n",
            "epoch: 8, train_acc: 0.7400000095367432, avg_cost: 0.7638794798652329\n",
            "epoch: 9, train_acc: 0.75, avg_cost: 0.7554289832711218\n",
            "epoch: 10, train_acc: 0.7200000286102295, avg_cost: 0.7483770808080829\n",
            "epoch: 11, train_acc: 0.7200000286102295, avg_cost: 0.7409332417945069\n",
            "epoch: 12, train_acc: 0.7599999904632568, avg_cost: 0.7370366824169958\n",
            "epoch: 13, train_acc: 0.7599999904632568, avg_cost: 0.7331250117222464\n",
            "epoch: 14, train_acc: 0.6899999976158142, avg_cost: 0.7232382764418916\n",
            "epoch: 15, train_acc: 0.6800000071525574, avg_cost: 0.7283467640976117\n",
            "epoch: 16, train_acc: 0.75, avg_cost: 0.7253241525590415\n",
            "epoch: 17, train_acc: 0.7200000286102295, avg_cost: 0.7203023866315683\n",
            "epoch: 18, train_acc: 0.75, avg_cost: 0.7167743293941018\n",
            "epoch: 19, train_acc: 0.7799999713897705, avg_cost: 0.7124912407994276\n",
            "epoch: 20, train_acc: 0.7400000095367432, avg_cost: 0.7154459852973624\n",
            "epoch: 21, train_acc: 0.7799999713897705, avg_cost: 0.7153296302755674\n",
            "epoch: 22, train_acc: 0.7900000214576721, avg_cost: 0.7158553929626943\n",
            "epoch: 23, train_acc: 0.7300000190734863, avg_cost: 0.712050913224618\n",
            "epoch: 24, train_acc: 0.8100000023841858, avg_cost: 0.7153649983306722\n",
            "epoch: 25, train_acc: 0.699999988079071, avg_cost: 0.7167992518345517\n",
            "epoch: 26, train_acc: 0.7300000190734863, avg_cost: 0.7073676914970077\n",
            "epoch: 27, train_acc: 0.6899999976158142, avg_cost: 0.7106710650026797\n",
            "epoch: 28, train_acc: 0.6899999976158142, avg_cost: 0.7133251618842279\n",
            "epoch: 29, train_acc: 0.800000011920929, avg_cost: 0.7107315896948175\n",
            "final_acc: 0.9014999866485596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nFAyl9QtO5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01e3133e-b3a6-4f94-c62c-aa1fd4f871c5"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfsRGKTptQjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3a78cfc-721f-489c-b65a-e6c22aa58e8d"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNSP74DXtxl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "2fd1229b-b33e-410d-dc93-fcda7ee02840"
      },
      "source": [
        "# You can change the directory name\n",
        "\n",
        "LOG_DIR = 'tb_logs'\n",
        "\n",
        "\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(LOG_DIR):\n",
        "\n",
        "  os.makedirs(LOG_DIR)\n",
        "\n",
        "  \n",
        "\n",
        "get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
        "\n",
        "\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-08 08:01:05--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.72.245.79, 3.214.163.243, 3.219.64.173, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.72.245.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13607069 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  12.98M  14.2MB/s    in 0.9s    \n",
            "\n",
            "2019-08-08 08:01:07 (14.2 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13607069/13607069]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://0dacd9e6.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOdVSlmFuy8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}